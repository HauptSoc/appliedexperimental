{
  "hash": "47b01bd0001af0604f3a308b575a6574",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ðŸ”¬ Simulation-Based Methods versus Theory-Based Methods\"\nformat: \n  revealjs:\n    theme: dark\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.4\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(infer)\nlibrary(moderndive)\nlibrary(lterdatasampler)\n\nmy_theme <- theme(axis.title.x = element_text(size = 18), \n                  axis.title.y = element_text(size = 18), \n                  axis.text.x = element_text(size = 12), \n                  axis.text.y = element_text(size = 12))\n\nobs_slope <- evals %>% \n  specify(response = score, \n          explanatory = bty_avg) %>% \n  calculate(stat = \"slope\")\n```\n:::\n\n\n\n# Lab 7 Recap\n\n## Common Mistakes\n\n2) Question 8 (Interpret Confidence Interval): A lot of groups either did not include the population or they stated the population was the 13 marshes and not all marshes on the East coast. For this question, only one or two groups said the population was about crabs!\n\n3) Question 11 (Bootstrap Assumptions): A lot of groups did not talk about marshes and instead mentioned resampling. Some also mentioned that the population was crabs :(\n\n\n# Thinking about the Final Project...\n\n\n## \n\n::: {style=\"font-size: 4em; color: #FFFFFF;\"}\nWhat did we do on Tuesday?\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nWe carried out a hypothesis test!\n:::\n\n::: columns\n::: {.column width=\"35%\"}\n$$H_0: \\beta_1 = 0$$\n\n$$H_A: \\beta_1 \\neq 0$$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = evals, \n       mapping = aes(x = bty_avg, y = score)) +\n  geom_jitter() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Average Beauty Score\", \n       y = \"Course Evaluation Score\") +\n  my_theme\n```\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/evals-slr-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n:::\n\n::: {style=\"font-size: 1em; color: #ed8402;\"}\nWhat do these hypotheses mean *in words*?\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nBy creating a permutation distribution!\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist <- evals %>% \n  specify(response = score, \n          explanatory = bty_avg) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"slope\")\n```\n:::\n\n\n\n. . .\n\n</br>\n\n::: {style=\"font-size: 1.5em; color: #B6CADA;\"}\nWhat is happening in the `generate()` step?\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nAnd visualizing where our observed statistic fell on the distribution\n:::\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualise(null_dist) +\n  shade_p_value(obs_stat = obs_slope, direction = \"two-sided\") +\n  labs(x = \"Permuted Slope Statistic\")\n```\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"35%\"}\n::: {style=\"font-size: 1.5em; color: #B6CADA;\"}\nWhat would you estimate the p-value to be?\n:::\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nAnd calculated the p-value\n:::\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  p_value\n    <dbl>\n1       0\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"35%\"}\n::: {style=\"font-size: 1.5em; color: #B6CADA;\"}\n</br> </br> What would you decide for your hypothesis test?\n:::\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2.5em; color: #FFFFFF;\"}\nHow would this process have changed if we used theory-based methods instead?\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nApproximating the permutation distribution\n:::\n\n::: columns\n::: {.column width=\"40%\"}\nA $t$-distribution can be a reasonable approximation for the permutation distribution if certain conditions are not violated.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(c(-5, 5),\n     c(0, dnorm(0)),\n      type = \"n\", \n     ylab = \"\", \n     xlab = \"\",\n     axes = FALSE\n     )\n\nat <- seq(-10, 10, 2)\naxis(1, at)\naxis(1, at - 1, rep(\"\", length(at)), tcl = -0.1)\nabline(h = 0)\nCOL. <- openintro::fadeColor(openintro::IMSCOL[\"blue\", \"full\"], c(\"FF\", \"89\", \"68\", \"4C\", \"33\"))\nCOLt <- openintro::fadeColor(openintro::IMSCOL[\"blue\", \"full\"], c(\"FF\", \"AA\", \"85\", \"60\", \"45\"))\nDF <- c(\"normal\", 8, 4, 2, 1)\nX <- seq(-10, 10, 0.02)\nY <- dnorm(X)\nlines(X, Y, col = COL.[1])\nfor (i in 2:5) {\n  Y <- dt(X, as.numeric(DF[i]))\n  lines(X, Y, col = COL.[i], lwd = 1.5)\n}\nlegend(2.5, 0.4,\n  legend = c(\n    DF[1],\n    paste(\"t, df = \", DF[2:5], sep = \"\")\n  ),\n  col = COL.,\n  text.col = COLt,\n  lty = rep(1, 5),\n  lwd = 1.5\n)\n```\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/t-distribution-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nWhat about the observed statistic?\n:::\n\n::: panel-tabset\n## Before\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_slope <- evals %>% \n  specify(response = score, \n          explanatory = bty_avg) %>% \n  calculate(stat = \"slope\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_slope\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: score (numeric)\nExplanatory: bty_avg (numeric)\n# A tibble: 1 Ã— 1\n    stat\n   <dbl>\n1 0.0666\n```\n\n\n:::\n:::\n\n\n\n## Now\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevals_lm <- lm(score ~ bty_avg,\n               data = evals)\n\nget_regression_table(evals_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept    3.88      0.076     51.0        0    3.73     4.03 \n2 bty_avg      0.067     0.016      4.09       0    0.035    0.099\n```\n\n\n:::\n:::\n\n\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nHow did R calculate the $t$-statistic?\n:::\n\n::: panel-tabset\n## Step 1: SE\n\n::: columns\n::: {.column width=\"40%\"}\n$SE_{b_1} = \\frac{\\frac{s_y}{s_x} \\cdot \\sqrt{1 - r^2}}{\\sqrt{n - 2}}$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_num <- ( sd(evals$score) / sd(evals$bty_avg) ) * \n  sqrt(\n    1 - cor(evals$score, evals$bty_avg)\n    ) \n\nse_denom <- sqrt(nrow(evals) - 2)\n\nse <- se_num / se_denom\n\nse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01495204\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Step 2: t-statistic\n\n::: columns\n::: {.column width=\"40%\"}\n$t = \\frac{b_1}{SE_{b_1}}$\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- pull(obs_slope) / se\n\nt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbty_avg \n4.45672 \n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Proof!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_regression_table(evals_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  <chr>        <dbl>     <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1 intercept    3.88      0.076     51.0        0    3.73     4.03 \n2 bty_avg      0.067     0.016      4.09       0    0.035    0.099\n```\n\n\n:::\n:::\n\n\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nHow does R calculate the p-value?\n:::\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = tibble(x = c(-5, 5)), \n       mapping = aes(x)) +\n  stat_function(fun = dt, \n                args = list(df = nrow(evals) - 2), \n                color = \"blue\", \n                linewidth = 1.5) +\n  ylab(\"\") +\n  scale_y_continuous(breaks = NULL) +\n  geom_vline(xintercept = t, color = \"red\", linewidth = 1.5)\n```\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n. . .\n\n**How many degrees of freedom does this** $t$-distribution have?\n\n## \n\n::: {style=\"font-size: 3.5em; color: #B6CADA;\"}\nDid we get similar results between these methods?\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nWhy not always use theoretical methods?\n:::\n\n. . .\n\n::: columns\n::: {.column width=\"45%\"}\nTheory-based methods only hold if the sampling distribution is normally shaped.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\nThe normality of a sampling distribution depends **heavily** on model conditions.\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nWhat are these \"conditions\"?\n:::\n\n. . .\n\n::: columns\n::: {.column width=\"40%\"}\nFor linear regression we are assuming...\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n**L**inear relationship between $x$ and $y$\n\n</br>\n\n**I**ndepdent observations\n\n</br>\n\n**N**ormality of residuals\n\n</br>\n\n**E**qual variance of residuals\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\n**L**inear relationship between $x$ and $y$\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nand_vertebrates %>%\n  filter(species != \"Cascade torrent salamander\") %>% \n  ggplot(mapping = aes(x = length_1_mm, \n                       y = weight_g, \n                       color = species)) +\n  geom_point() + \n  labs(x = \"Length (mm)\", \n       y = \"Weight (g)\") +\n  my_theme\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 13270 rows containing missing values (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n. . .\n\n**What should we do?**\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nVariable transformation!\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nand_vertebrates %>%\n  filter(species != \"Cascade torrent salamander\") %>% \n  ggplot(mapping = aes(x = length_1_mm, \n                       y = weight_g, \n                       color = species)) +\n  geom_point() + \n  labs(x = \"Log Transformed Length (mm)\", \n       y = \"Log Transformed Weight (g)\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  my_theme\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 13270 rows containing missing values (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/transform-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\n**I**ndependence of observations\n:::\n\n::: columns\n::: {.column width=\"40%\"}\nThe `evals` dataset contains 463 observations on 94 professors. Meaning, professors have **multiple** observations.\n\n</br>\n\n*What can we do?*\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"55%\"}\n**Best** -- use a random effects model\n\n**Reasonable** -- collapse the multiple scores into a single score\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\n**N**ormality of residuals\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfish <- and_vertebrates %>%\n  filter(!species %in% c(\"Cascade torrent salamander\", \"Coastal giant salamander\")) \n\ncurved_lm <- lm(weight_g ~ length_1_mm, data = fish)\n\nget_regression_points(curved_lm) %>% \n  ggplot(mapping = aes(x = residual)) + \n  geom_histogram() + \n  labs(x = \"Residual\") +\n  my_theme\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/non-normal-1.png){width=672}\n:::\n:::\n\n\n\n. . .\n\n**What should we do?**\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nVariable transformation!\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfish <- and_vertebrates %>%\n  filter(!species %in% c(\"Cascade torrent salamander\", \"Coastal giant salamander\"))\n\nnot_curved_lm <- lm(log(weight_g) ~ log(length_1_mm), data = fish)\n\nbroom::augment(not_curved_lm) %>% \n  mutate(.resid = `log(weight_g)` - `.fitted`) %>% \n  ggplot(mapping = aes(x = .resid)) + \n  geom_histogram(binwidth = 0.1) + \n  labs(x = \"Residual\") +\n  xlim(c(-0.5, 0.5)) +\n  my_theme\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 73 rows containing non-finite values (`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/normal-1.png){width=672}\n:::\n:::\n\n\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\n**E**qual variance of residuals\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = hbr_maples, \n       mapping = aes(x = stem_length, y = stem_dry_mass)\n       ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Stem Dry Mass (g)\",\n    title = \"Sugar Maple Seedlings in Hubbard Brook LTER\"\n  ) +\n  theme_minimal() + \n  my_theme\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/non-constant-variance-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n. . .\n\n**What should we do?**\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nVariable transformation!\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = hbr_maples, \n       mapping = aes(x = stem_length, y = stem_dry_mass)\n       ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Log Transformed Stem Dry Mass (g)\",\n    title = \"Sugar Maple Seedlings in Hubbard Brook LTER\"\n  ) +\n  theme_minimal() + \n  scale_y_log10() +\n  my_theme\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week8-day2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nAre these conditions required for **both** methods?\n:::\n\n. . .\n\n::: columns\n::: {.column width=\"40%\"}\n::: {style=\"font-size: 1.5em; color: #B6CADA;\"}\nSimulation-based Methods\n:::\n\n::: {style=\"font-size: 0.75em; color: #FFFFFF;\"}\n-   Linearity of Relationship\n\n-   Independence of Observations\n\n-   Equal Variance of Residuals\n:::\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"45%\"}\n::: {style=\"font-size: 1.5em; color: #B6CADA;\"}\nTheory-based Methods\n:::\n\n::: {style=\"font-size: 0.75em; color: #FFFFFF;\"}\n-   Linearity of Relationship\n-   Independence of Observations\n:::\n\n::: {style=\"font-size: 0.75em; color: #B6CADA;\"}\n-   Normality of Residuals\n:::\n\n::: {style=\"font-size: 0.75em; color: #FFFFFF;\"}\n-   Equal Variance of Residuals\n:::\n:::\n:::\n\n## \n\n::: {style=\"font-size: 2em; color: #FFFFFF;\"}\nWhat happens if the conditions are violated?\n:::\n\n. . .\n\nIn general, when the conditions associated with these methods are violated, the permutation and $t$-distributions will underestimate the true standard error of the sampling distribution.\n",
    "supporting": [
      "week8-day2_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}