{
  "hash": "bb46d83fd4e59f382cd6382e94841031",
  "result": {
    "markdown": "---\ntitle: \"Predicting Professor Evaluation Scores\"\nauthor: \"Your group's names here!\"\ndate: \"May 12, 2023\"\nformat: \n  html:\n    embed-resources: true\n    standalone: true\neditor: visual\nexecute: \n  eval: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(moderndive)\n\n# This is needed to make sure everyone gets the SAME testing / training datasets!\nset.seed(1234)\n\nevals <- evals |> \n  mutate(large_class = if_else(cls_students > 100, \n                               \"large class\", \n                               \"regular class\"), \n         eval_completion = cls_did_eval / cls_students \n         ) |> \n  select(-cls_did_eval, \n         -cls_students)\n```\n:::\n\n\n## Your Challenge\n\nThis week you have learned about model selection. During class you worked on performing a backward selection process to determine the \"best\" model for penguin body mass.\n\nToday, you are going to use **forward selection** to determine the \"best\" model for professor's evaluation score. There is an extra layer to this challenge, you are going to test how well your model predicts the evaluation scores of professors **not** in your dataset. The group(s) with the lowest prediction error will win **two** extra tokens.\n\nThis task will require you to fit **tons** of linear regressions. **You must be able to show me exactly how you got to your top model.** Meaning, I need to see a record of **every** model you fit and compared along the way.\n\n### Your Datasets\n\nI've used the `slice_sample()` function to divide the `evals` dataset into two datasets:\n\n1.  `evals_train` contains 80% of the observations in the original `evals` dataset -- **this is the dataset you will use for your models**\n\n2.  `evals_test` contains the other 20% of observations -- **this is the dataset you will use for your predictions**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevals_train <- slice_sample(evals, prop = 0.8)\nevals_test <- anti_join(evals, evals_train, by = \"ID\")\n```\n:::\n\n\n## Forward Selection\n\nThe forward selection process starts with a model with **no** predictor variables. That means, this model predicts the *same* mean evaluation score for every professor. I've fit this model for you below!\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_mean <- lm(score ~ 1, data = evals_train)\n```\n:::\n\n\nYou can pull out the adjusted $R^2$ for this model using the `get_regression_summaries()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_regression_summaries(one_mean)\n```\n:::\n\n\nBased on this output, we are starting with a **really** low adjusted $R^2$. So, things can only get better from here!\n\n### Step 1\n\n**Rules: You can only add a variable to the model if it improves adjusted** $R^2$ by at least 1% (0.01)\n\nAlright, so now we get cooking. The next step is to fit **every** model with **one** explanatory variable. I've provided a list of every explanatory variable you are allowed to consider!\n\n-   `prof_ID` -- identification variable for each professor\n-   `age` -- age of the professor\n-   `bty_avg` -- average beauty rating of the professor\n-   `gender` -- gender of the professor\n-   `ethnicity` -- ethnicity of the professor\n-   `language` -- language of school where professor received education\n-   `rank` -- rank of professor\n-   `pic_outfit` -- outfit of professor in picture\n-   `pic_color` -- color of professor's picture\n-   `large_class` -- whether the class had over 100 students\n-   `eval_completion` -- proportion of students who completed the evaluation\n-   `cls_level` -- class level\n\nWoof, that's 12 different variables. That means, for this first round, you will need to compare the adjusted $R^2$ for 12 different models to decide what variable should be added.\n\nEvery model you fit will have the *same* format:\n\n```         \nname_of_model <- lm(score ~ <variable>, data = evals_train)\n```\n\nBut, the name of the model will need to change. I've started the process for you, using the naming style of `one_` followed by the variable name (e.g., `one_id`, `one_bty`, etc.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_id <- lm(score ~ prof_ID, data = evals_train)\none_age <- lm(score ~ age, data = evals_train)\none_bty <- lm(score ~ bty_avg, data = evals_train)\none_gender <- lm(score ~ gender, data = evals_train)\none_ethnicity <- lm(score ~ ethnicity, data = evals_train)\none_language <- lm(score ~ language, data = evals_train)\n\n## Now, you need to fit the other six models! \n```\n:::\n\n\nAlright, now that you've fit the models, you need to inspect the adjusted $R^2$ values to see which of these 12 models is the \"top\" model -- the model with the highest adjusted $R^2$! Similar to before, I've provided you with some code to get you started, but you need to write the remaining code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_regression_summaries(one_id)\nget_regression_summaries(one_age)\nget_regression_summaries(one_bty)\nget_regression_summaries(one_gender)\nget_regression_summaries(one_ethnicity)\nget_regression_summaries(one_language)\n\n## Now, you need to compare the other six models! \n```\n:::\n\n\n**1. What model was your top model? Specifically, what variable was selected to be included in your top model?**\n\n### Step 2\n\nAlright, you've added one variable, the next step is to decide if you should add a second variable. This process looks nearly identical to the previous step, with one major change: **every model you fit needs to contain the variable you decided to add**. So, if you decided to add the `bty_avg` variable, every model you fit would look like this:\n\n```         \nname_of_model <- lm(score ~ bty_avg + <variable>, data = evals_train)\n```\n\nAgain, the name of the model will need to change. This round, you are on your own -- I've provided you with no code. Here are my recommendations:\n\n-   name each model `two_` followed by the names of both variables (e.g., `two_bty_id`)\n-   go through each variable step-by-step just like you did before\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to fit all 11 models that add a second variable to your top model goes here!\n```\n:::\n\n\nAlright, now you should have 11 more models to compare! Like before, you need to inspect the adjusted $R^2$ values to see which of these 101models is the \"top\" model.\n\n**Rules: You can only add a variable to the model if it improves adjusted** $R^2$ by at least 1% (0.01) from the model you chose in Question 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to compare all 11 models you fit goes here!\n```\n:::\n\n\n**2. What model was your top model? Specifically, what variables are included in your top model?**\n\n### Step 3\n\nAs you might have expected, in this step we add a *third* variable to our top model from the previous step. This process should be getting familiar at this point!\n\nThis process of fitting 10-12 models at a time is getting rather tedious! So, I've written some code that will carry out this process for us in **one** pipeline! This is how what the code looks:\n\n```         \nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n\nWoah, that's a lot. The only thing you need to change is:\n\n-   add in the names of the variables you selected in Steps 1 & 2 in the `~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)` step\n\n-   add in the names of the variables you selected in Steps 1 & 2 in the `select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>)` step\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to fit all  models that add a third variable to your top model goes here!\n\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1 & 2\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1 & 2\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 3 and see if it improved adjusted\\*\\* $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 2 is the \"best\" model!\n\n**3. What model was your top model? Specifically, what variables are included in your top model?**\n\n### Step 4\n\nAs you might have expected, in this step we add a *fourth* variable to our top model from the previous step. We're again going to use the code that allows for us to fit **lots** of regressions without having to type them all out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to fit all  models that add a forth variable to your top model goes here!\n\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1, 2, & 3\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1, 2 & 3\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 3 and see if it improved adjusted\\*\\* $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the \"best\" model!\n\n**4. What model was your top model? Specifically, what variables are included in your top model?**\n\n### Step 5\n\nAs you might have expected, in this step we add a *fifth* variable to our top model from the previous step. We're again going to use the code that allows for us to fit **lots** of regressions without having to type them all out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to fit all  models that add a forth variable to your top model goes here!\n\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1, 2, 3 & 4\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1, 2, 3 & 4\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 4 and see if it improved adjusted\\*\\* $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the \"best\" model!\n\n**5. What model was your top model? Specifically, what variables are included in your top model?**\n\n### Step 6\n\nAs you might have expected, in this step we add a *sixth* variable to our top model from the previous step. We're again going to use the code that allows for us to fit **lots** of regressions without having to type them all out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to fit all  models that add a forth variable to your top model goes here!\n\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1, 2, 3, 4 & 5\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1, 2, 3, 4 & 5\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 5 and see if it improved adjusted\\*\\* $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the \"best\" model!\n\n**6. What model was your top model? Specifically, what variables are included in your top model?**\n\n## Testing Your Top Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_model <- lm(score ~ <ALL VARIABLES SELECTED>, data = evals_train)\n\nevals_test %>% \n  mutate(predictions = predict(top_model, newdata = evals_test), \n         residuals = score - predictions) %>%\n  summarize(RMSE = sd(residuals))\n```\n:::\n",
    "supporting": [
      "lab-6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}