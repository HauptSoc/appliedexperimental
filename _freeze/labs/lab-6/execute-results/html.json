{
  "hash": "409382ea0966f27e3e9f030a40998b46",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Professor Evaluation Scores\"\nauthor: \"Your group's names here!\"\ndate: \"May 11, 2023\"\nformat: html\neditor: visual\nexecute: \n  eval: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(moderndive)\nlibrary(openintro)\n\nevals <- evals |> \n  mutate(large_class = if_else(cls_students > 100, \n                               \"large class\", \n                               \"regular class\"), \n         eval_completion = cls_did_eval / cls_students \n         ) |> \n  select(-cls_did_eval, \n         -cls_students, \n         -prof_id,\n         -course_id)\n```\n:::\n\n\n## Your Challenge\n\nThis week you have learned about model selection. During class you worked on performing a backward selection process to determine the \"best\" model for penguin body mass.\n\nToday, you are going to use **forward selection** to determine the \"best\" model for professor's evaluation score. This task will require you to fit **tons** of linear regressions. **You must be able to show me exactly how you got to your top model.** Meaning, I need to see a record of **every** model you fit and compared along the way.\n\n## Forward Selection\n\nThe forward selection process starts with a model with **no** predictor variables. That means, this model predicts the *same* mean evaluation score for every professor. I've fit this model for you below!\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_mean <- lm(score ~ 1, data = evals_train)\n```\n:::\n\n\nYou can pull out the adjusted $R^2$ for this model using the `get_regression_summaries()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_regression_summaries(one_mean)\n```\n:::\n\n\nBased on this output, we are starting with a **really** low adjusted $R^2$. So, things can only get better from here!\n\n### Step 1\n\n**Rules: You can only add a variable to the model if it improves the adjusted** $R^2$ by at least 1% (0.01).\n\nAlright, so now we get cooking. The next step is to fit **every** model with **one** explanatory variable. I've provided a list of every explanatory variable you are allowed to consider!\n\n-   `prof_ID` -- identification variable for each professor\n-   `age` -- age of the professor\n-   `bty_avg` -- average beauty rating of the professor\n-   `gender` -- gender of the professor\n-   `ethnicity` -- ethnicity of the professor\n-   `language` -- language of school where professor received education\n-   `rank` -- rank of professor\n-   `pic_outfit` -- outfit of professor in picture\n-   `pic_color` -- color of professor's picture\n-   `large_class` -- whether the class had over 100 students\n-   `eval_completion` -- proportion of students who completed the evaluation\n-   `cls_level` -- class level\n\nWoof, that's 12 different variables. That means, for this first round, you will need to compare the adjusted $R^2$ for [**12**]{.underline} different models to decide what variable should be added.\n\nEvery model you fit will have the *same* format:\n\n```         \nname_of_model <- lm(score ~ <variable>, data = evals_train)\n```\n\nBut, the name of the model will need to change. I've started the process for you, using the naming style of `one_` followed by the variable name (e.g., `one_id`, `one_bty`, etc.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_id <- lm(score ~ prof_ID, data = evals_train)\none_age <- lm(score ~ age, data = evals_train)\none_bty <- lm(score ~ bty_avg, data = evals_train)\none_gender <- lm(score ~ gender, data = evals_train)\none_ethnicity <- lm(score ~ ethnicity, data = evals_train)\none_language <- lm(score ~ language, data = evals_train)\n\n## Now, you need to fit the other six models! \n```\n:::\n\n\nAlright, now that you've fit the models, you need to inspect the adjusted $R^2$ values to see which of these 12 models is the \"top\" model -- the model with the highest adjusted $R^2$! Similar to before, I've provided you with some code to get you started, but you need to write the remaining code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_regression_summaries(one_id)\nget_regression_summaries(one_age)\nget_regression_summaries(one_bty)\nget_regression_summaries(one_gender)\nget_regression_summaries(one_ethnicity)\nget_regression_summaries(one_language)\n\n## Now, you need to compare the other six models! \n```\n:::\n\n\n**1. What model was your top model? Specifically, which variable was selected to be included?**\n\n### Step 2 - Adding a Second Variable\n\nAlright, you've added one variable, the next step is to decide if you should add a second variable. This process looks nearly identical to the previous step, with one major change: **every model you fit needs to contain the variable you decided to add**. So, if you decided to add the `bty_avg` variable, every model you fit would look like this:\n\n```         \nname_of_model <- lm(score ~ bty_avg + <variable>, data = evals_train)\n```\n\nAgain, the name of the model will need to change. This round, you are on your own -- I've provided you with no code. Here are my recommendations:\n\n-   name each model `two_` followed by the names of both variables included in the model (e.g., `two_bty_id`)\n-   go through each variable step-by-step just like you did before\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to fit all 11 models that add a second variable to your top model goes here!\n```\n:::\n\n\nAlright, now you should have 11 more models to compare! Like before, you need to inspect the adjusted $R^2$ values to see which of these 11 models is the \"top\" model.\n\n**Rules: You can only add a variable to the model if it improves adjusted** $R^2$ by at least 1% (0.01) from the model you chose in Question 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Code to compare all 11 models you fit goes here!\n```\n:::\n\n\n**2. What model was your top model? State which variables are included in the model you chose!**\n\n### Step 3 - Adding a Third Variable\n\nAs you might have expected, in this step we add a *third* variable to our top model from the previous step. This process should be getting familiar at this point!\n\nThis process of fitting 10-12 models at a time is getting rather tedious! So, I've written some code that will carry out this process for us in **one** pipeline! This is how the code looks:\n\n```         \nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, \n         -score,\n         -<VARIABLE 1 SELECTED>,\n         -<VARIABLE 2 SELECTED>\n         ) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n\nWoah, that's a lot. The only thing you need to change is:\n\n-   add in the names of the variables you selected in Steps 1 & 2 in the `~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)` step\n\n-   add in the names of the variables you selected in Steps 1 & 2 in the `select(-ID, -score, -<VARIABLE 1 SELECTED>, -<VARIABLE 2 SELECTED>)` step\n\nFor example, if you chose `gender` and `age` in Steps 1 and 2, your code on the first line would look like:\n\n```         \nmap(.f = ~lm(score ~ .x + gender + age, data = evals_train)) %>% \n```\n\nand your code on the fourth line would look like:\n\n```         \n  select(-ID, \n         -score,\n         -gender,\n         -sex\n         ) %>% \n```\n\nYour turn!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1 & 2\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1 & 2\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, \n         -score, \n         -<VARIABLE 1 SELECTED>, \n         -<VARIABLE 2 SELECTED>\n           ) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 3 and see if it improved adjusted $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 2 is the \"best\" model!\n\n**3. What model was your top model? State which variables are included in the model you chose!**\n\n### Step 4 - Adding a Fourth Variable\n\nAs you might have expected, in this step we add a *fourth* variable to our top model from the previous step. We're again going to use the code that allows for us to fit **lots** of regressions without having to type them all out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1, 2, & 3\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1, 2 & 3\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, \n         -score, \n         -<VARIABLE 1 SELECTED>, \n         -<VARIABLE 2 SELECTED>, \n         -<VARIABLE 3 SELECTED>\n         ) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 3 and see if it improved adjusted $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the \"best\" model!\n\n**4. What model was your top model? You must state which variables are included in the model you chose!**\n\n### Step 5 - Adding a Fifth Variable\n\nAs you might have expected, in this step we add a *fifth* variable to our top model from the previous step. We're again going to use the code that allows for us to fit **lots** of regressions without having to type them all out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1, 2, 3 & 4\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1, 2, 3 & 4\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, \n         -score, \n         -<VARIABLE 1 SELECTED>, \n         -<VARIABLE 2 SELECTED>, \n         -<VARIABLE 3 SELECTED>,\n         -<VARIABLE 4 SELECTED>\n          ) %>%  \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 4 and see if it improved adjusted $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 4 is the \"best\" model!\n\n**5. What model was your top model? You must state which variables are included in the model you chose!**\n\n### Step 6 - Adding a Sixth Variable\n\nAs you might have expected, in this step we add a *sixth* variable to our top model from the previous step. We're again going to use the code that allows for us to fit **lots** of regressions without having to type them all out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Change the <VARIABLES SELECTED> in line 2 to the names of the variables you selected in Steps 1, 2, 3, 4 & 5\n## Change the <VARIABLE 1 SELECTED> and <VARIABLE 2 SELECTED> in line 4 to the names of the variables you selected in Steps 1, 2, 3, 4 & 5\n\nevals_train %>% \n  map(.f = ~lm(score ~ .x + <VARIABLES SELECTED>, data = evals_train)) %>% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %>% \n  select(-ID, \n         -score, \n         -<VARIABLE 1 SELECTED>, \n         -<VARIABLE 2 SELECTED>, \n         -<VARIABLE 3 SELECTED>,\n         -<VARIABLE 4 SELECTED>,\n         -<VARIABLE 5 SELECTED>\n          ) %>% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %>% \n  slice_max(adj_r_sq)\n```\n:::\n\n\nThe output of this code is the variable that has the highest adjusted $R^2$. Compare this value to the value of your \"top\" model from Step 5 and see if it improved adjusted $R^2$ by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 5 is the \"best\" model!\n\n**6. What model was your top model? State which variables are included in the model you chose!**\n\n## Comparing with the `step()` Function\n\nLet's check the forward selection model you found with what model the `step()` function decides is best. Run the code chunk below to obtain the \"best\" model chosen by this function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- lm(score ~ ., data = evals_train)\nstep(full_model, direction = \"forward\")\n```\n:::\n\n\n7.  **Did the `step()` function choose the same model as you? If your \"best\" models do not not agree, why do you think this might have happened?**\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}