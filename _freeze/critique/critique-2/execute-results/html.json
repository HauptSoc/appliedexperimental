{
  "hash": "bd02060f707411fe295e6d1008653e05",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Critique 2: Exploring p-values\"\nformat: \n  html:\n    table-of-contents: true\n    toc-depth: 2\n    number-sections: true\n    number-depth: 1\neditor: visual\n---\n\n\n\n![](images/significant.jpeg)\n\n## Assignment Details\n\nIn your second statistical critique, you will focus on critiquing another key \naspect of any statistical argument---statistical significance. No doubt you have\nseen $p$-values in a previous statistical course and / or disciplinary course,\nand this week you're adding to that knowledge. For this critique you will\ncompare the model you selected in your Midterm Project with what model you would\nhave chosen based on a statistical test.\n\nThis critique involves coding! You can find a template for critique posted in\nthe STAT 313 main workspace on [Posit Cloud](https://posit.cloud/).\n\n## Part Zero: p-values in Multiple Linear Regression\n\nFor the first step of this critique, you are required to read about how p-values\ncan be used in the context of multiple linear regression:\n[Extending to Multiple Linear Regression](../weeks/chapters/week-8-reading-mlr.qmd \"Extending to Multiple Linear Regression\")\n\n# Part One: Revisiting the Midterm Project\n\nFor the first part of this critique, you are going to revisit the model you\nselected for your Midterm Project. You need to copy-and-paste the code you wrote\nin your Midterm Project to create your two visualizations. After these\nvisualizations, you should include the 2-3 sentences you wrote regarding *why*\nyou chose the model you did in your Midterm Project. This should be copied from\nthe \"Statistical Model\" section of your Midterm Project!\n\n# Part Two: Using p-values Instead\n\nFor this second part, you are tasked with testing what regression model you\nwould have chosen if you had used p-values to make your decision. Regardless of\nthe model you chose for your Midterm Project, you will fit the **most complex**\nregression model. In the context of a multiple linear regression with one \nnumerical and one categorical explanatory variables, the most complex model is\nthe different slopes (interaction) model.\n\n**Step 1:** Fit a different slopes multiple linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model <- lm(bill_length_mm ~ flipper_length_mm * species, \n               data = penguins)\n```\n:::\n\n\n\n**Step 2:** Run an ANOVA to test if the groups have different slopes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(my_model) |> \n  tidy()\n```\n:::\n\n\n\n# Part Three: Learning More about Misuses of $p$-values\n\n> \"The p-value was never intended to be a substitute for scientific reasoning.\" Ron Wasserstein, Executive Director of the American Statistical Association\n\nIssues with the use of $p$-values had gotten so problematic that the American \nStatistical Association (ASA)[^1] put out a statement in 2016 titled, [\"The ASA Statement on Statistical Significance and $p$-Values\"](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf). \nThis statement includes six principles which address misconceptions and misuse\nof the $p$-value.\n\n[^1]: This is my professional organization.\n\n<!-- > \"Over time it appears the p-value has become a gatekeeper for whether work is publishable. This apparent editorial bias leads to the 'file-drawer effect,' in which research with statistically significant outcomes are much more likely to get published, while other work that might well be just as important scientifically is never seen in print. It also leads to practices called by such names as 'p-hacking' and 'data dredging' that emphasize the search for small p-values over other statistical and scientific reasoning.\" -->\n<!-- > -->\n<!-- > Jessica Utts, President of the American Statistical Association -->\n\n<!-- In March of 2019, Valentin Amrhein, Sander Greenland, Blake McShane and more -->\n<!-- than 800 signatories published an article in Nature [calling for an end to \"statistical significance\"](https://www.nature.com/articles/d41586-019-00857-9). The article -->\n<!-- details how, on top of the many common misunderstandings about hypothesis -->\n<!-- testing and $p$-values, there is an incentive for researchers to \"cherry pick\" -->\n<!-- only the results that are \"statistically significant\" while dismissing those -->\n<!-- that aren't. There are two problems with this system: -->\n\n<!-- 1.  It incentivizes researchers to do whatever it takes to obtain \"significant\" -->\n<!-- p-values, even through dishonest means. -->\n<!-- 2.  It dismisses the importance of results where no \"significant\" effects are -->\n<!-- found. -->\n\nFor this section, you are required to:\n\n1.  read the American Statistical Association's statement on $p$-values and\nstatistical significance\n\n2.  note what misinterpretations you believe apply to the statistical findings\npresented in the **Results** section of the [Tuan et al. paper](https://cpslo-my.sharepoint.com/:b:/g/personal/atheobol_calpoly_edu/EXujE_m4Z4ZAhAVhPxIAmZoB2F4e-nTl4Cfruxjs1IU6HA).\n\n::: callout-tip\n# Justification\n\nNote, you are required to *justify* why you believe the selected\nmisinterpretations apply to the findings presented in Tuan et al.'s article.\n:::\n\n# Part Four: Lessons Learned\n\nNow that you have explored the use of p-values for model selection and\npublication criteria, write down **two** things you have learned that you will\ntake with you in your future courses / research.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}