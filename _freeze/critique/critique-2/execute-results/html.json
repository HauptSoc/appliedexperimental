{
  "hash": "36322fc728e264c51766e56e0321770c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Critique 2: Exploring p-values\"\nformat: \n  html:\n    table-of-contents: true\n    toc-depth: 2\n    number-sections: true\n    number-depth: 1\neditor: source\n---\n\n\n\n\n![](images/significant.jpeg)\n\n## Assignment Details\n\nIn your second statistical critique, you will focus on critiquing another key aspect of any statistical argument---statistical significance. No doubt you have seen $p$-values in a previous statistical course and / or disciplinary course, and this week you're adding to that knowledge. For this critique you will compare the model you selected in your Midterm Project with what model you would have chosen based on a statistical test.\n\nThis critique involves coding! You can find a template for critique posted in the STAT 313 main workspace on [Posit Cloud](https://posit.cloud/).\n\n## Part Zero: p-values in Multiple Linear Regression\n\nFor the first step of this critique, you are required to read about how p-values can be used in the context of multiple linear regression: [Extending to Multiple Linear Regression](../weeks/chapters/week-8-reading-mlr.qmd \"Extending to Multiple Linear Regression\")\n\n# Part One: Revisiting the Midterm Project\n\nFor the first part of this critique, you are going to revisit the model you selected for your Midterm Project. You need to copy-and-paste the code you wrote in your Midterm Project to create your two visualizations. After these visualizations, you should include the 2-3 sentences you wrote regarding *why* you chose the model you did in your Midterm Project. This should be copied from the \"Statistical Model\" section of your Midterm Project!\n\n# Part Two: Using p-values Instead\n\nFor this second part, you are tasked with testing what regression model you would have chosen if you had used p-values to make your decision. Regardless of the model you chose for your Midterm Project, you will fit the **most complex** regression model. In the context of a multiple linear regression with one numerical and one categorical explanatory variables, the most complex model is the different slopes (interaction) model.\n\n**Step 1:** Fit a different slopes multiple linear regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model <- lm(bill_length_mm ~ flipper_length_mm * species, \n               data = penguins)\n```\n:::\n\n\n\n\n**Step 2:** Run an ANOVA to test if the groups have different slopes\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(my_model) |> \n  tidy()\n```\n:::\n\n\n\n\n# Part Three: Learning More about Misuses of $p$-values\n\n> \"The p-value was never intended to be a substitute for scientific reasoning.\" Ron Wasserstein, Executive Director of the American Statistical Association\n\nIssues with the use of $p$-values had gotten so problematic that the American Statistical Association (ASA)[^1] put out a statement in 2016 titled, [\"The ASA Statement on Statistical Significance and $p$-Values\"](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf). This statement includes six principles which address misconceptions and misuse of the $p$-value.\n\n[^1]: This is my professional organization.\n\n<!-- > \"Over time it appears the p-value has become a gatekeeper for whether work is publishable. This apparent editorial bias leads to the 'file-drawer effect,' in which research with statistically significant outcomes are much more likely to get published, while other work that might well be just as important scientifically is never seen in print. It also leads to practices called by such names as 'p-hacking' and 'data dredging' that emphasize the search for small p-values over other statistical and scientific reasoning.\" -->\n\n<!-- > -->\n\n<!-- > Jessica Utts, President of the American Statistical Association -->\n\n<!-- In March of 2019, Valentin Amrhein, Sander Greenland, Blake McShane and more -->\n\n<!-- than 800 signatories published an article in Nature [calling for an end to \"statistical significance\"](https://www.nature.com/articles/d41586-019-00857-9). The article -->\n\n<!-- details how, on top of the many common misunderstandings about hypothesis -->\n\n<!-- testing and $p$-values, there is an incentive for researchers to \"cherry pick\" -->\n\n<!-- only the results that are \"statistically significant\" while dismissing those -->\n\n<!-- that aren't. There are two problems with this system: -->\n\n<!-- 1.  It incentivizes researchers to do whatever it takes to obtain \"significant\" -->\n\n<!-- p-values, even through dishonest means. -->\n\n<!-- 2.  It dismisses the importance of results where no \"significant\" effects are -->\n\n<!-- found. -->\n\nFor this section, you are required to:\n\n1.  read the American Statistical Association's statement on $p$-values and statistical significance\n\n2.  note what misinterpretations you believe apply to the statistical findings presented in the **Results** section of the [Tuan et al. paper](https://cpslo-my.sharepoint.com/:b:/g/personal/atheobol_calpoly_edu/EXujE_m4Z4ZAhAVhPxIAmZoB2F4e-nTl4Cfruxjs1IU6HA).\n\n::: callout-tip\n# Justification\n\nNote, you are required to *justify* why you believe the selected misinterpretations apply to the findings presented in Tuan et al.'s article.\n:::\n\n# Part Four: Lessons Learned\n\nNow that you have explored the use of p-values for model selection and publication criteria, write down **two** things you have learned that you will take with you in your future courses / research.\n",
    "supporting": [
      "critique-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}