---
title: "ðŸ”¬ Simulation-Based Methods versus Theory-Based Methods"
format: 
  revealjs:
    theme: style.scss
editor: source
---

```{r set-up}
library(tidyverse)
library(infer)
library(moderndive)
library(lterdatasampler)
library(palmerpenguins)

my_theme <- theme(axis.title.x = element_text(size = 18), 
                  axis.title.y = element_text(size = 18), 
                  axis.text.x = element_text(size = 12), 
                  axis.text.y = element_text(size = 12), 
                  legend.text = element_text(size = 12), 
                  legend.title = element_text(size = 18))
```

# Lab 6 Recap

## Common Mistakes

</br>




# The conclusions we reach depend on our p-value and confidence intervale being reliable.

# The conclusions we reach depend on our p-value and confidence intervale being reliable.

How can we know if our p-value is reliable?

## Model Conditions

For our p-value to be trustworthy, we need to know that the conditions of our model are not violated.

. . .

:::::: columns
::: {.column width="40%"}
For linear regression we are assuming...
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
**L**inear relationship between $x$ and $y$

</br>

**I**ndependent observations

</br>

**N**ormality of residuals

</br>

**E**qual variance of residuals
:::
::::::

## What happens if the conditions are violated?

In general, when the conditions associated with these methods are violated, we will [underestimate]{.underline} the true standard error (spread) of the sampling distribution.

. . .

-   Our p-values will be **too** small!
-   Our confidence intervals will be **too** narrow!
-   We will make more Type I errors than we expect!

## 

::: {style="font-size: 2em; color: #000000;"}
**L**inear relationship between $x$ and $y$
:::

```{r}
#| fig-align: center

and_vertebrates %>%
  filter(species != "Cascade torrent salamander") %>% 
  ggplot(mapping = aes(x = length_1_mm, 
                       y = weight_g, 
                       color = species)) +
  geom_point() + 
  labs(x = "Length (mm)", 
       y = "Weight (g)", 
       color = "Species") +
  my_theme
```

. . .

***What should we do?***

## 

::: {style="font-size: 2em; color: #000000;"}
Variable transformation!
:::

```{r transform}
#| fig-align: center

and_vertebrates %>%
  filter(species != "Cascade torrent salamander") %>% 
  ggplot(mapping = aes(x = length_1_mm, 
                       y = weight_g, 
                       color = species)) +
  geom_jitter(alpha = 0.5) + 
  labs(x = "Log Transformed Length (mm)", 
       y = "Log Transformed Weight (g)", 
       color = "Species") +
  scale_x_log10() +
  scale_y_log10() +
  my_theme
```

## 

::: {style="font-size: 2em; color: #000000;"}
**I**ndependence of observations
:::

::::::: columns
::: {.column width="40%"}
The `evals` dataset contains `r nrow(evals)` observations on `r distinct(evals, prof_ID) %>% nrow()` professors. Meaning, professors have **multiple** observations.

</br>

***What should we do?***
:::

::: {.column width="5%"}
:::

:::: {.column width="55%"}
::: fragment
**Best** -- use a random effects model

**Reasonable** -- collapse the multiple scores into a single score
:::
::::
:::::::

## 

::: {style="font-size: 2em; color: #000000;"}
**E**qual variance of residuals
:::

```{r non-constant-variance}
#| fig-align: center

ggplot(data = hbr_maples, 
       mapping = aes(x = stem_length, y = stem_dry_mass)
       ) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(
    x = "Stem Length (mm)",
    y = "Stem Dry Mass (g)"
    ) +
  theme_minimal() + 
  my_theme
```

. . .

***What should we do?***

## 

::: {style="font-size: 2em; color: #000000;"}
Variable transformation!
:::

```{r}
ggplot(data = hbr_maples, 
       mapping = aes(x = stem_length, y = stem_dry_mass)
       ) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(
    x = "Stem Length (mm)",
    y = "Log Transformed Stem Dry Mass (g)"
  ) +
  theme_minimal() + 
  scale_y_log10() +
  my_theme
```

## 

::: {style="font-size: 2em; color: #000000;"}
**N**ormality of residuals
:::

```{r non-normal}

fish <- and_vertebrates %>%
  filter(!species %in% c("Cascade torrent salamander", "Coastal giant salamander")) 

curved_lm <- lm(weight_g ~ length_1_mm, data = fish)

get_regression_points(curved_lm) %>% 
  ggplot(mapping = aes(x = residual)) + 
  geom_histogram() + 
  labs(x = "Residual") +
  my_theme
```

. . .

***What should we do?***

## 

::: {style="font-size: 2em; color: #000000;"}
Variable transformation!
:::

```{r normal}

fish <- and_vertebrates %>%
  filter(!species %in% c("Cascade torrent salamander", "Coastal giant salamander"))

not_curved_lm <- lm(log(weight_g) ~ log(length_1_mm), data = fish)

broom::augment(not_curved_lm) %>% 
  mutate(.resid = `log(weight_g)` - `.fitted`) %>% 
  ggplot(mapping = aes(x = .resid)) + 
  geom_histogram(binwidth = 0.1) + 
  labs(x = "Residual") +
  xlim(c(-0.5, 0.5)) +
  my_theme
```

## What if we can't fix a condition being violated?

. . .

::::::: columns
::: {.column width="45%"}
For the **L**, **I**, and **E** conditions...

we need to ask for help!
:::

::: {.column width="5%"}
:::

:::: {.column width="45%"}
::: fragment
For the **N** condition...

we need to use simulation instead of mathematical formulas to get our p-value and confidence interval.
:::
::::
:::::::

# Simulation-Based Methods versus Theory-Based Methods

## Mathematical / Theory-based Methods

::: incremental
-   Are a "simpler" approach
-   Use formulas instead of simulation to obtain standard error
-   Use $t$-distribution to get p-value and confidence interval
-   Require that the residuals are normally distributed
:::

## How does this look?

:::: panel-tabset
## Before

```{r}
#| echo: true
#| label: obs-slope-code
#| code-line-numbers: false

obs_slope <- evals %>% 
  specify(response = score, 
          explanatory = bty_avg) %>% 
  calculate(stat = "slope")
```

```{r}
#| label: obs-slope
pull(obs_slope)
```

## Now

::: smaller
```{r}
#| echo: true
#| label: obs-t-code
#| code-line-numbers: false

evals_lm <- lm(score ~ bty_avg, data = evals)

get_regression_table(evals_lm)
```
:::

```{r}
#| label: obs-t
#| include: false

evals_lm <- lm(score ~ bty_avg,
               data = evals)

coef_table <- get_regression_table(evals_lm)

t_stat <- filter(coef_table, term == "bty_avg") %>% 
  select(statistic)

slope_stat <- filter(coef_table, term == "bty_avg") %>% 
  select(estimate)

t_stat
```
::::

## 

::: {style="font-size: 2em; color: #000000;"}
How did R calculate the $t$-statistic?
:::

::::::: panel-tabset
## Step 1: SE

:::::: columns
::: {.column width="40%"}
$SE_{b_1} = \frac{\frac{s_y}{s_x} \cdot \sqrt{1 - r^2}}{\sqrt{n - 2}}$
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
```{r}
se_num <- ( sd(evals$score) / sd(evals$bty_avg) ) * 
  sqrt(
    1 - cor(evals$score, evals$bty_avg)
    ) 

se_denom <- sqrt(nrow(evals) - 2)

se <- se_num / se_denom

se
```
:::
::::::

## Step 2: t-statistic

$t = \frac{b_1}{SE_{b_1}} = \frac{`r slope_stat`}{`r se`} = `r slope_stat / se`$

## Proof!

```{r}
get_regression_table(evals_lm) 
```
:::::::

## How does R calculate the p-value?

. . .

```{r}
#| fig-align: center
#| out-width: 50%

ggplot(data = tibble(x = c(-5, 5)), 
       mapping = aes(x)) +
  stat_function(fun = dt, 
                args = list(df = nrow(evals) - 2), 
                color = "blue", 
                linewidth = 1.5) +
  labs(y = "", 
       x = "t-statistic") +
  scale_y_continuous(breaks = NULL) +
  geom_vline(xintercept = pull(t_stat), color = "red", linewidth = 1.5) +
  my_theme

```

. . .

::: callout-note
# Different $t$-distributions

A $t$-distribution has a *slightly* different shape depending on the sample size. Technically, we are using a $t$-distribution with $n - 2$ degrees of freedom to get our p-value.
:::

# Did we get similar results between these methods?

# Did we get similar results between these methods?

Why do you think that is?

## Approximating the permutation distribution

:::::: columns
::: {.column width="40%"}
A $t$-distribution can be a reasonable approximation for the permutation distribution [if the normality condition is not violated]{.underline}.
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
```{r t-distribution}
plot(c(-5, 5),
     c(0, dnorm(0)),
      type = "n", 
     ylab = "", 
     xlab = "",
     axes = FALSE
     )

at <- seq(-10, 10, 2)
axis(1, at)
axis(1, at - 1, rep("", length(at)), tcl = -0.1)
abline(h = 0)
COL. <- openintro::fadeColor(openintro::IMSCOL["blue", "full"], c("FF", "89", "68", "4C", "33"))
COLt <- openintro::fadeColor(openintro::IMSCOL["blue", "full"], c("FF", "AA", "85", "60", "45"))
DF <- c("normal", 8, 4, 2, 1)
X <- seq(-10, 10, 0.02)
Y <- dnorm(X)
lines(X, Y, col = COL.[1])

for (i in 2:5) {
  Y <- dt(X, as.numeric(DF[i]))
  lines(X, Y, col = COL.[i], lwd = 1.5)
}
legend(2.5, 0.4,
  legend = c(
    DF[1],
    paste("t, df = ", DF[2:5], sep = "")
  ),
  col = COL.,
  text.col = COLt,
  lty = rep(1, 5),
  lwd = 1.5
)

```
:::
::::::

