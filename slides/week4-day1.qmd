---
title: "Introduction to Linear Regression"
format: 
  revealjs:
    theme: style.scss
embed-resources: true
editor: visual
---

```{r packages}
library(tidyverse)
library(openintro)
library(moderndive)

```

```{r ggplot-theme}
#| echo: false

my_theme <- theme_bw() + 
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20), 
        axis.text.x = element_text(size = 14), 
        axis.text.y = element_text(size = 14)
        )
```


# Lab 2

## A Grading Reminder

<br>

:::::::: columns
:::: {.column width="47%"}
::: {style="font-size: 1.25em;"}
"Complete" = Satisfactory
:::

<br>

Your group obtained a "Success" on **every** question
::::

::: {.column width="5%"}
:::

:::: {.column width="47%"}
::: {style="font-size: 1.25em;"}
"Incomplete" = Growing
:::

<br>

Your group received a "Growing" on *at least one* question
::::
::::::::

## Common Mistakes

::: small
-   Units in axis labels (Q2 & Q9)
    -   What unit were the departure / arrival delays measured in?
-   Justifying *why* I should expect to be early / late (Q8 & Q10)
    -   Why is the mean / median a reasonable estimate of the "typical" delay?
    -   What aspect(s) of the distribution did you use to decide what a "typical" delay is?
:::

<!-- ## Making a Duplicate of Your Group's Lab 2 Project  -->
<!-- Insert new instructions on how to make a copy of the group project for  -->
<!-- individual revisions.  -->


## Completing Revisions

Lab 2 revisions are due by Friday, April 24.

::: small
1.  Read comments on Canvas
2.  Copy your group's lab assignment
3.  Complete your revisions
4.  Render your revised Lab 2
5.  Download your revised HTML
6.  Submit your revisions to the [original Lab 2]{.underline} assignment
:::

:::: callout-warning
# Reflections

::: small
Revisions are required to be accompanied with reflections on what you learned while completing your revisions. These can be written in your Lab 2 Quarto file (next to the problems you revised), in a Word document, in the comment box on Canvas.
:::
::::

```{r post-26-births-table}
births_post28 <- ncbirths %>% 
  drop_na(weight, weeks) %>% 
  filter(weeks > 28)

weeks_lm <- lm(weight ~ weeks, data = births_post28)

births_table <- get_regression_table(weeks_lm)
```

::: {style="font-size: 2em; color: #000000;"}
Relationships Between Variables
:::

. . .

-   In a statistical model, we generally have one variable that is the output and one or more variables that are the inputs.

:::::::::: columns
::::: {.column width="40%"}
:::: {.fragment .fade-in-then-semi-out}
::: {style="font-size: 0.65em;"}
-   Response variable
    -   a.k.a. $y$, dependent
    -   The quantity you want to understand
    -   In this class -- always numerical
:::
::::
:::::

::: {.column width="5%"}
:::

::::: {.column width="55%"}
:::: {style="font-size: 0.65em;"}
::: fragment
-   Explanatory variable
    -   a.k.a. $x$, independent, explanatory, predictor
    -   Something you think might be related to the response
    -   Either numerical or categorical
:::
::::
:::::
::::::::::

## 

::: {style="font-size: 2em; color: #000000;"}
Visualizing Linear Regression
:::

```{r viz}
#| echo: false
#| eval: true
#| fig-align: center

ggplot(data = bdims, aes(y = wgt, x = hgt)) + 
  geom_point() +
  scale_x_continuous("Explanatory Variable", labels = NULL) + 
  scale_y_continuous("Response Variable", labels = NULL) + 
  theme_bw() + 
  theme(axis.title.x = element_text(size = 36),
        axis.title.y = element_text(size = 36)
        )
```

. . .

</br>

:::::: columns
::: {.column width="40%"}
-   The scatterplot has been called the most "generally useful invention in the history of statistical graphics."
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
-   It is a simple two-dimensional plot in which the two coordinates of each dot represent the values of two variables measured on a single observation.
:::
::::::

## 

::: {style="font-size: 2em; color: #000000;"}
Characterizing Relationships
:::

</br>

::: incremental
-   Form (e.g. linear, quadratic, non-linear)

-   Direction (e.g. positive, negative)

-   Strength (how much scatter/noise?)

-   Unusual observations (do points not fit the overall pattern?)
:::

## 

::: {style="font-size: 2em; color: #000000;"}
Data for Today
:::

The [ncbirths]{style="color: #e28743;"} dataset is a random sample of 1,000 cases taken from a larger dataset collected in North Carolina in 2004.

::: {style="font-size: 0.75em;"}
> Each case describes the birth of a single child born in North Carolina, along with various characteristics of the child (e.g. birth weight, length of gestation, etc.), the child's mother (e.g. age, weight gained during pregnancy, smoking habits, etc.) and the child's father (e.g. age).
:::

## 

::: {style="font-size: 2em; color: #0F4C81;"}
Your Turn!
:::

:::::: columns
::: {.column width="65%"}
```{r}
#| echo: false
#| out-width: 100%

ncbirths %>% 
ggplot(aes(x = weeks, y = weight)) +
  geom_jitter() + 
  labs(x = "Length of pregnancy (in weeks)",
       y = "Birth weight of baby (in lbs)") +
  my_theme
```
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
How would your characterize this relationship?

-   form
-   direction
-   strength
-   unusual observations
:::
::::::

## Cleaning & Filtering the Data

It seems like pregnancies with a gestation less than 28 weeks have a non-linear relationship with a baby's birth weight, so we will filter these observations out of our dataset.

</br>

```{r}
#| echo: true
#| code-line-numbers: false
#| label: filter-clean-births-post-26-weeks

births_post28 <- ncbirths %>% 
  drop_na(weight, weeks) %>% 
  filter(weeks > 28)
```

. . .

:::: small
::: callout-caution
# Change in scope of inference

Removing these observations narrows the population of births we are able to make inferences onto! In this case, what population could we infer our findings onto?
:::
::::

# Summarizing a Linear Relationship

## 

::::: columns
::: {.column width="20%"}
</br> Correlation:
:::

::: {.column width="80%"}
> strength and direction of a *linear* relationship between two *quantitative* variables
:::
:::::

. . .

-   Correlation coefficient between -1 and 1
-   Sign of the correlations shows direction
-   Magnitude of the correlation shows strength

## 

::: {style="font-size: 2em; color: #000000;"}
Anscombe Correlations
:::

::::::: columns
::: {.column width="65%"}
```{r}
#| echo: false
#| out-width: 100%

anscombe <- anscombe %>%
  mutate(id = 1:nrow(.)) %>%
  pivot_longer(cols = -id, names_to = "key", values_to = "value") %>%
  separate(key, into = c("variable", "set"), sep = 1) %>%
  pivot_wider(names_from = variable, values_from = value)

ggplot(data = anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~set)
```
:::

::: {.column width="5%"}
:::

:::: {.column width="30%"}
::: {style="font-size: 0.75em;"}
Four datasets, very different graphical presentations

-   same mean and standard deviation in both $x$ and $y$
-   same correlation
-   same regression line
:::
::::
:::::::

. . .

<center>

[For which of these relationships is correlation a reasonable summary measure?]{style="font-size: 1.25em; color: #e28743;"}

## Calculating Correlation in R

```{r}
#| echo: true
#| code-line-numbers: false

get_correlation(births_post28, 
                weeks ~ weight)
```

. . .

</br>

What if I ran `get_correlation(births_post28, weight ~ weeks)` instead? Would I get the same value?

## 

::::: columns
::: {.column width="20%"}
</br> Linear regression:
:::

::: {.column width="80%"}
> we assume the the relationship between our response variable ($y$) and explanatory variable ($x$) can be modeled with a linear function, plus some random noise
:::
:::::

. . .

::: centered
$response = intercept + slope \cdot explanatory + noise$
:::

## Writing the Regression Equation

</br>

::::::::: columns
:::: {.column width="47%"}
::: fragment
**Population Model**

$y = \beta_0 + \beta_1 \cdot x + \epsilon$

</br>

$y$ = response

$\beta_0$ = population intercept

$\beta_1$ = population slope

$\epsilon$ = errors / residuals
:::
::::

::: {.column width="3%"}
:::

::::: {.column width="50%"}
:::: fragment
**Sample Model**

$\widehat{y} = b_0 + b_1 \cdot x$

</br>

$b_0$ = sample intercept

$b_1$ = sample slope

::: fragment
[Why does this equation have a hat on $y$?]{style="color: #e28743;"}
:::
::::
:::::
:::::::::

# Linear Regression with One Numerical Explanatory Variable

## Step 1: Fit a linear regression

```{r}
#| echo: true
#| code-line-numbers: false
weeks_lm <- lm(weight ~ weeks, 
               data = births_post28)
```

## Step 2: Obtain coefficient table

```{r}
#| echo: true
#| code-line-numbers: false
#| eval: false
get_regression_table(weeks_lm)
```

```{r}
get_regression_table(weeks_lm) %>% 
  gt::gt()
```

. . .

</br>

:::: small
::: callout-note
# `get_regression_table()`

This function lives in the **moderndive** package, so we will need to load in this package (e.g., `library(moderndive`) if we want to use the `get_regression_table()` function.
:::
::::

## 

::: {style="font-size: 2em; color: #000000;"}
Our focus (for now...)
:::

![](images/table-focus-estimate-sd.jpeg)

## 

::: {style="font-size: 2em; color: #000000;"}
Estimated regression equation
:::

$$\widehat{y} = b_0 + b_1 \cdot x$$

. . .

::::::: columns
:::: {.column width="48%"}
::: small
```{r}
#| echo: true
#| code-line-numbers: false
#| eval: false
weeks_lm <- lm(weight ~ weeks, 
               data = births_post28)
```
:::
::::

::: {.column width="2%"}
:::

::: {.column width="50%"}
```{r}
#| code-line-numbers: false
get_regression_table(weeks_lm) %>% 
  gt::gt()
```
:::
:::::::

. . .

</br>

<center>

[Write out the estimated regression equation!]{style="color: #e28743;"}

##  {background-color="#b76352"}

::::::::: columns
:::: {.column width="45%"}
::: {style="font-size: 1.75em; color: #000000;"}
How do you interpret the intercept value of `r filter(births_table, term == "intercept")$estimate`?
:::
::::

::: {.column width="5%"}
:::

::::: {.column width="45%"}
:::: fragment
::: {style="font-size: 1.75em; color: #000000;"}
How do you interpret the slope value of `r filter(births_table, term == "weeks")$estimate`?
:::
::::
:::::
:::::::::

## 

::: {style="font-size: 2em; color: #000000;"}
Obtaining Residuals
:::

</br>

$\widehat{weight} = `r filter(births_table, term == "intercept")$estimate`+`r filter(births_table, term == "weeks")$estimate` \cdot weeks$

</br>

. . .

[What would the residual be for a pregnancy that lasted 39 weeks and whose baby weighed 7.63 pounds?]{style="color: #e28743; font-size: 1.5em;"}

# Linear Regression with One Categorical Explanatory Variable

## Step 1: Finding `distinct` levels

```{r}
#| code-line-numbers: false
#| echo: true

distinct(births_post28, habit)
```

## 

::: {style="font-size: 1.75em; color: #000000;"}
Step 2: Fit a linear regression
:::

```{r}
#| code-line-numbers: false
#| echo: true
habit_lm <- lm(weight ~ habit,
               data = births_post28)

```

. . .

</br>

::: {style="font-size: 1.75em; color: #000000;"}
Step 3: Obtain coefficient table
:::

```{r}
#| code-line-numbers: false
#| echo: true
#| eval: false
get_regression_table(habit_lm)
```

```{r}
get_regression_table(habit_lm) %>% 
gt::gt()
```

. . .

:::: centered
::: {style="font-size: 2em;"}
🤔
:::
::::

## Step 4: Write Estimated Regression Equation

```{r}
get_regression_table(habit_lm) %>% 
gt::gt()
```

</br>

. . .

$$\widehat{weight} = 7.23 - 0.4 \cdot Smoker$$

</br>

. . .

[But what does $Smoker$ represent???]{style="color: #e28743; font-size: 1.25em;"}

## Categorical Explanatory Variables

<center>$$
  \widehat{y} = b_0 + b_1 \cdot x
$$</center>

. . .

:::::: columns
::: {.column width="40%"}
$x$ is a categorical variable with levels:

-   `"nonsmoker"`
-   `"smoker"`
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
We need to convert to:

-   a "baseline" group
-   "offsets" / adjustments to the baseline
:::
::::::

## Choosing a Baseline Group

```{r}
get_regression_table(habit_lm) %>% 
gt::gt()
```

</br>

[Based on the regression table, what `habit` group was chosen to be the baseline group?]{style="color: #e28743; font-size: 1.25em;"}

## Rewriting in Terms of Indicator Variables

$$\widehat{weight} = 7.23 - 0.4 \cdot 1_{smoker}(x)$$

where

<center>

$1_{smoker}(x) = 1$ if the mother was a `"smoker"`

$1_{smoker}(x) = 0$ if the mother was a `"nonsmoker"`

## Obtaining Group Means

$$\widehat{weight} = 7.23 - 0.4 \cdot 1_{Smoker}(x)$$ </br>

Given the equation, what is the estimated mean birth weight for nonsmoking mothers?

</br>

For smoking mothers?

## Causal Inference

We just concluded that babies born to a `"smoker"` weigh, on average, 0.4 pounds less than babies born to a `"nonsmoker"`.

</br> </br>

. . .

Can we conclude that smoking [caused]{style="color: #e28743;"} these babies to weigh less? Why or why not?

# Midterm Project Step 1

## [Project Proposal](../project/midterm-proposal-313.qmd)

::: small
1.  Choose a dataset

2.  Choose one numerical response variable

3.  Choose one numerical explanatory variable

4.  Choose a second explanatory variable, it can be either numerical or categorical
:::

:::: callout-tip
# Checking values of your numerical variable(s)

::: small
Your numerical variable **cannot** have a small number of values (e.g., 2 or 3). You can use the `distinct()` function to determine the unique values of your variable. For example, by running `distinct(hbr_maples, year)` I would discover that `year` only has two values (2003 and 2004), meaning `year` is not eligible to be a numerical response or explanatory variable. It could, however, be a categorical explanatory variable!
:::
::::

::: small
5.  Write your Introduction
:::
