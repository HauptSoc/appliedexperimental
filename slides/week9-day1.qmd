---
title: "**AN**alysis **O**f **VA**riance"
format: 
  revealjs:
    theme: style.scss
editor: source
---

```{r}
#| include: false
#| label: set-up

library(tidyverse)
library(palmerpenguins)
library(broom)
library(ggridges)
library(infer)
library(gapminder)
library(gt)

evals_small <- moderndive::evals %>% 
  group_by(prof_ID) %>% 
  mutate(min_eval = min(score), 
         age_cat = case_when(age < 37 ~ "young faculty",
                             age >= 37 & age < 45 ~ "middle age", 
                             age >= 45 & age < 60 ~ "older faculty", 
                             age >= 60 ~ "nearly retired"),
         age_cat = factor(age_cat, 
                          levels = c("young faculty", "middle age",
                                     "older faculty", "nearly retired")
                          )
         ) %>% 
  distinct(prof_ID, .keep_all = TRUE) %>% 
  ungroup()

my_theme <- theme_bw() + 
  theme(axis.text.y = element_text(size = 18), 
        axis.text.x = element_text(size = 16), 
        plot.title = element_text(size = 24), 
        axis.title.x = element_text(size = 18)
        ) 
```

# Lab 7 Recap

## Common Mistakes

**Question 8** (Hypothesis Test Conclusion)

::: {.midi}
*If you rejected the null hypothesis*, there is evidence of a linear
relationship between the size of a crab and the latitude in which it lives. 
:::

::: {.midi}
*If you failed to reject the null hypothesis*, there is insufficient evidence
of a linear relationship between the size of a crab and the latitude in which it
lives. 
:::

. . .

::: {.small}
::: {.callout-tip}
# Notice how both interpretations are in terms of the alternative hypothesis?

If you *always* write your conclusion in terms of $H_A$, then you will never 
accidentally "accept" the null hypothesis. 
:::
:::

## Common Mistakes

**Question 11** (Interpret Confidence Interval):

::: {.small}
> We need to be specific about the what parameter we believe is in our interval.
The slope statistic is measuring the relationship between which variables?
:::

::: {.midi}
We're analyzing the linear relationship between the size of a crab and the 
latitude in which it lives!
:::

. . .

::: {.small}
> What population does this interval apply to? Where were these crabs sampled
from? That is the population your interval applies to!
:::

::: {.midi}
The fiddler crabs were sampled from 13 salt marshes, but where were these salt
marshes located? What population might they belong to?
:::

## Common Mistakes

**Question 12** (Apply Confidence Interval)

</br>

Bergmann's Rule suggests that organisms are larger in larger latitudes (further
from the equator). 

> Does your interval suggest this is true for fiddler crabs?

# Week 9

## Final Project First Draft

**Step 1** - Due by tonight! 

:::{.small}
- Introduction (Data description, research questions)
:::
. . .

**Step 2** - Due by Thursday

:::{.small}
- Methods (Data visualizations)
- Results (Data analysis)
:::

. . .

**Step 3** - Due by Sunday

:::{.small}
- Discussion
- Conclusion
:::

# One-Way ANOVA

. . .

Compares the means of three of more groups to detect if the means of the groups
are different.

## Two "Flavors" of ANOVA

In my mind, there are two different options for incorporating an ANOVA into 
your analysis. 

. . .

::: columns
::: {.column width="40%"}
**Option 1:**

::: {.midi}
There is a *categorical* variable with 3+ groups that you would like to use for
your regression. 
:::
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
::: {.fragment}
**Option 2:**

::: {.midi}
There is a *numerical* variable that you would like to use to create 3+ groups
to incorporate into your regression. 
:::
:::
:::
:::

. . .

</br> 

Let's check out what I mean! 

<!-- change context to the regions in the gapminder dataset which we previously  -->
<!-- did a regression on! -->

## Categorical Variable 

::: {.midi}
```{r}
#| echo: true
#| label: gapminder-filter
#| code-line-numbers: false

gapminder2007 <- gapminder %>%
  filter(year == 2007) %>%
  select(country, lifeExp, continent, gdpPercap)
```
:::

::: {.small}
::: {.callout-tip}
# Independence Violations

In Lab 8 you should have found that each country has *multiple* observations, 
which [are not]{.underline} independent. One way to get around this is to 
`filter()` the data to only use *one* year. 
:::
:::

. . .

::: {.midi}
```{r}
#| echo: true
#| label: gapminder-countries
#| code-line-numbers: false
#| output-location: column

distinct(gapminder2007, 
         continent)
```
:::

## Discretizing a Continuous Variable


```{r}
#| echo: false
#| label: slr
#| fig-align: center

ggplot(data = evals_small, 
       mapping = aes(x = age, y = min_eval)) + 
  geom_jitter() + 
  geom_smooth(method = "lm") +
  labs(x = "Age of Faculty Member", 
       y = "", 
       title = "Minimum Evaluation Scores Decrease as Faculty Age") +
  my_theme +
  scale_y_continuous(breaks = seq(from = 2, to = 5, by = 0.5))
```

. . .

::: {.small}
::: {.callout-tip}
# Independence Violations

Last week we noticed that there are *multiple* observations for each faculty
member which [are not]{.underline} independent. One way to get around this is to
*collapse* these multiple observations into a *single* number. 
:::
:::

## Now...

```{r}
#| echo: false
#| label: age-cat

ggplot(data = evals_small, 
       mapping = aes(x = age_cat, y = min_eval)) + 
  geom_jitter(width = 0.1) + 
  labs(x = "", 
       y = "", 
       title = "Minimum Evaluation Scores Across Faculty Ages") +
  my_theme +
  scale_y_continuous(breaks = seq(from = 1.5, to = 5, by = 0.5))
```

# Carrying out an ANOVA

## Steps for an ANOVA

-   Compare how different a group of means are
-   Scale the differences relative to the variability of the groups
-   Summarize the differences with one number (an F-statistic)

# Visualizations for an ANOVA

# Visualizations for an ANOVA

We want visualizations that allow for us to easily compare:

-   the center (mean) of the groups
-   the spread (variability) of the groups

## Step 1: Compare your groups

```{r}
#| echo: false
#| out-width: 70%
#| label: age-cat-density
#| fig-align: center

ggplot(data = evals_small, 
       mapping = aes(y = age_cat, x = min_eval)) + 
  geom_density_ridges() + 
  labs(title = "Minimum Evaluation Scores Across Faculty Ages", 
       y = "",
       x = "") +
  my_theme +
  scale_x_continuous(breaks = seq(from = 2, to = 5, by = 0.5))
```

. . .

::: {style="color: #34605f;"}
::: columns
::: {.column width="47%"}
What can you say about the differences *between* the age groups?
:::

::: {.column width="3%"}
:::

::: {.column width="47%"}
What can you say about the variability *within* the age groups?
:::
:::
:::

## Step 2: Find the overall mean

```{r}
#| echo: false
#| out-width: 70%
#| label: overall-mean
#| fig-align: center

overall_mean <- summarise(evals_small, 
                          mean = mean(min_eval)
                          ) %>% 
  pull()

ggplot(data = evals_small, 
       mapping = aes(y = min_eval, x = "c")) + 
  geom_jitter(width = 0.2) + 
  geom_hline(yintercept = overall_mean, color = "red", lwd = 2) +
  labs(title = "Minimum Evaluation Scores Across Faculty Ages", 
       y = "",
       x = "") +
  my_theme +
  theme(axis.text.x = element_blank(), 
        axis.ticks.x = element_blank()) +
  scale_y_continuous(breaks = seq(from = 2, to = 5, by = 0.5))
```

. . .

::: {style="font-size: 1.25em; color: #ed8402;"}
This ignores the groups and finds [one]{.underline} mean for every observation!
:::

## Step 3: Find the group means

```{r}
#| echo: false
#| out-width: 70%
#| label: group-means
#| fig-align: center

group_means <- evals_small %>% 
  group_by(age_cat) %>% 
  summarise(mean = mean(min_eval)
                          ) %>% 
  mutate(
    position = c(1, 2, 3, 4)
         )

ggplot(data = evals_small, 
       mapping = aes(x = age_cat, y = min_eval)) + 
  geom_jitter(width = 0.1) + 
  labs(title = "Minimum Evaluation Scores Across Faculty Ages", 
       y = "",
       x = "") +
  my_theme +
  geom_segment(data = group_means, 
               mapping = aes(y = mean,
                             x = position - 0.25, 
                             xend = position + 0.25), 
               color = "purple", 
               lwd = 1.5
               ) +
  scale_y_continuous(breaks = seq(from = 2, to = 5, by = 0.5))
```

## Step 4: Calculate the sum of squares *between* groups

```{r}
#| echo: false
#| out-width: 70%
#| label: sum-of-squares
#| fig-align: center

ggplot(data = evals_small, 
       mapping = aes(x = age_cat, y = min_eval)) + 
  geom_jitter(width = 0.1) + 
  labs(title = "Minimum Evaluation Scores Across Faculty Ages", 
       y = "",
       x = "") +
  my_theme +
  geom_segment(data = group_means, 
               mapping = aes(y = mean,
                             x = position - 0.25, 
                             xend = position + 0.25), 
               color = "purple", 
               lwd = 1.5
               ) +
  geom_hline(yintercept = overall_mean, 
             color = "red", 
             lwd = 1.5, 
             lty = "dashed") +
  scale_y_continuous(breaks = seq(from = 2, to = 5, by = 0.5))
```

## 

::: {style="font-size: 2em; color: #000000;"}
Step 5: Calculate the sum of squares *within* groups
:::

```{r}
#| echo: false
#| out-width: 70%
#| label: sum-of-squares-2
#| fig-align: center

ggplot(data = evals_small, 
       mapping = aes(x = age_cat, y = min_eval)) + 
  geom_jitter(width = 0.1) + 
  labs(title = "Minimum Evaluation Scores Across Faculty Ages", 
       y = "",
       x = "") +
  my_theme +
  geom_segment(data = group_means, 
               mapping = aes(y = mean,
                             x = position - 0.25, 
                             xend = position + 0.25), 
               color = "purple", 
               lwd = 1.5
               ) +
  scale_y_continuous(breaks = seq(from = 2, to = 5, by = 0.5))
```

## 

::: {style="font-size: 2em; color: #000000;"}
Step 6: Calculate the F-statistic
:::



![](images/sum-of-squares.jpeg)

<!-- ::: {style="color: #B6CADA;"} -->
<!-- Can an F-statistic be negative? -->
<!-- ::: -->

## 

::: {style="font-size: 2em; color: #000000;"}
Step 7: Find the p-value
:::

![](images/f_dist.png)

## 

::: {style="font-size: 2em; color: #000000;"}
**F-distribution**
:::

An $F$-distribution is a variant of the $t$-distribution, and is also defined by
degrees of freedom.

. . .

</br>

This distribution is defined by **two** different degrees of freedom:

:::{.small}
1.  from the numerator (MSG) : $k$ (number of groups)  $- 1$
2.  from the denominator (MSE) : $n$ (number of observations)  $- k$ 
:::

## Exploring Different F-distributions

Let's play around and see how these two degrees of freedom change the shape of
the F-distribution:

</br> 

[Link to Applet on F-distributions](https://dcmpdatatools.utdanacenter.org/fdist/#:~:text=Explore%20how%20the%20shape%20of,Download%20Graph) 

##  

::: {style="font-size: 3em; color: #000000;"}
Do you always use an F-distribution to get the p-value?
:::

. . .

<center>

::: {style="font-size: 3em; color: #0F4C81;"}
NO!
:::

</center>

## Conditions of an ANOVA

-   Independence of observations

:::{.small}
> Observations are independent *within* groups **and** *between* groups
:::

. . .

-   Normality of the residuals

:::{.small}
> The distribution of residuals for each group is approximately normal
:::

. . .

-   Equal variability of the groups

:::{.small}
> The spread of the distributions are similar across groups
:::

## Choosing a Method

</br>

::: columns
::: {.column width="47%"}
::: {style="font-size: 1.25em; color: #000000;"}
Which condition(s) are required to use "theory-based" methods?
:::

</br>

:::{.fragment}
All three!
:::
:::

::: {.column width="3%"}
:::

::: {.column width="47%"}
:::{.fragment}
::: {style="font-size: 1.25em; color: #000000;"}
Which condition(s) are required to use "simulation-based" methods?
:::
:::

</br>

:::{.fragment}
All but normality!
:::
:::
:::

## 

```{r}
#| echo: false
#| label: conditions
#| fig-align: center

ggplot(data = evals_small, 
       mapping = aes(y = age_cat, x = min_eval)) + 
  geom_density_ridges() + 
  labs(title = "Minimum Evaluation Scores for Faculty of Different Age Groups", 
       y = "",
       x = "") +
  theme(axis.title = element_text(size = 72), 
        axis.text.y = element_text(size = 16), 
        axis.text.x = element_text(size = 14)
        )
```

. . .

::: {style="font-size: 1.5em; color: #34605f;"}
What do you think? Which method should we use?
:::

# Simulation-based Methods

## Step 1: Calculating the Observed F-statistic

```{r}
#| echo: true
#| label: obs-F

obs_F <- evals_small %>% 
  specify(response = min_eval, 
          explanatory = age_cat) %>% 
  calculate(stat = "F")
```

</br>


```{r}
#| label: obs-F-value

obs_F
```

## Step 2: Simulating what could have happened under $H_0$

. . .

</br>

::: {style="font-size: 1.25em; color: #34605f;"}
How could we use cards to simulate what minimum evaluation score a professor would have gotten, if their score was independent from their age?
:::

## Another Permutation Distribution

```{r null-dist}
#| echo: true
#| label: null-dist

null_dist <- evals_small %>% 
  specify(response = min_eval, 
          explanatory = age_cat) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "F")
```

## Another Permutation Distribution

```{r}
#| label: visualize-null-dist

visualise(null_dist) +
  labs(x = "F-statistic", 
       y = "",
       title = "Simulation-based Null Distribution") +
  theme(axis.title = element_text(size = 72), 
        axis.text.y = element_text(size = 16), 
        axis.text.x = element_text(size = 16), 
        axis.title.x = element_text(size = 18)
        )
  
```

. . .

::: {style="font-size: 1.25em; color: #ed8402;"}
Why doesn't the distribution have negative numbers?
:::

## Visualizing the p-value

```{r}
#| echo: true
#| eval: false
#| label: visualize-code

visualise(null_dist) + 
  shade_p_value(obs_stat = obs_F, 
                direction = "greater")
```

## Visualizing the p-value

```{r}
#| label: p-value-visualization-annotated
#| fig-align: center

p_val <- get_p_value(null_dist, obs_stat = obs_F, direction = "greater") %>% 
  pull()
  
null_dist %>% 
  visualise() + 
  shade_p_value(obs_stat = obs_F, direction = "greater") + 
  annotate(x = pull(obs_F) + 2.5,
           y = 100, 
           geom = "text", 
           label = stringr::str_c("p-value = ", p_val), 
           size = 6) +
  labs(x = "F-statistic", 
       y = "",
       title = "Simulation-based Null Distribution with 1000 Resamples") +
  theme(axis.title = element_text(size = 72), 
        axis.text.y = element_text(size = 16), 
        axis.text.x = element_text(size = 16), 
        axis.title.x = element_text(size = 18)
        )
```


## Making a Decision & Reaching a Conclusion

::: columns
::: {.column width="47%"}
For a p-value of `r p_val`, what decision would you reach regarding your hypothesis test?
:::

::: {.column width="3%"}
:::

::: {.column width="47%"}
:::{.fragment}
What would you conclude regarding the mean minimum evaluation score for different age groups of faculty?
:::
:::
:::

## Large p-values $\neq$ evidence for the null hypothesis! 

![](images/simpsons-p-value.jpeg){fig-alt="Four-panel meme from The Simpsons. In the first panel, a character says 'If p-value is greater than significance level, you can't reject the null hypothesis'. In the second panel, Principal Skinner replies 'yes'. In the third panel, the first character asks 'So you accept it'. In the fourth panel, Skinner responds with a firm 'no'. The meme highlights the common misunderstanding that failing to reject the null hypothesis means accepting it."}


# What if we didn't believe the normality condition was violated?

## Theory-based Methods

```{r}
#| echo: true
#| eval: false
#| label: anova-code

eval_lm <- lm(min_eval ~ age_cat, data = evals_small)

anova(eval_lm) |> 
  tidy()
```

```{r}
#| label: anova-output

eval_lm <- lm(min_eval ~ age_cat, data = evals_small)

anova(eval_lm) |> 
  tidy() |> 
  gt() |> 
  tab_style(
    style = cell_text(size = px(24)),
    locations = list(
      cells_body(),
      cells_column_labels()
    )
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  )
```

. . .

</br>

Is this the same `statistic` as before? 

. . .

What distribution was used to calculate the `p.value`?

. . .

## Making a Decision & Reaching a Conclusion

::: columns
::: {.column width="47%"}
For a p-value of 0.244, what decision would you reach regarding your hypothesis test?
:::

::: {.column width="3%"}
:::

::: {.column width="47%"}
:::{.fragment}
What would you conclude regarding the mean minimum evaluation score for different age groups of faculty?
:::
:::
:::

##  {background-color="#B6CADA"}

::: {style="font-size: 2.5em; color: #0F4C81;"}
Did the two methods yield different results?
:::

. . .

::: {style="font-size: 1.5em;"}
What does that imply about the normality condition?
:::

# Assessing Independence Activity

## With other students analyzing the same data...

- evaluate the *within* group independence condition

- evaluate the *between* group independence condition

</br> 

[Link to activity](../project/final-project/help/assess_independence.qmd)

# Final Project Work Session

## Introduction & Research Questions

Outline the [**two**]{.underline} questions your research seeks to address.

- You will have one question for each one-way ANOVA model
- Your question should be in terms of "differences in group means" not 
"relationships"
