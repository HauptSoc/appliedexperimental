[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "",
    "text": "Instructor: Dr. Allison Theobold – call me Dr. Theobold or Dr. T! I use they / she pronouns (that is my order of preference).",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "3.1 Textbooks",
    "text": "3.1 Textbooks\n\n\n\n\n\nÇetinkaya-Rundel and Hardin, Introduction to Modern Statistics. https://openintro-ims.netlify.app/\n\n\n\n\n\n\n\n\nIsmay & Kim, Modern Dive: Statistical Inference via Data Science. https://moderndive.com",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#required-technology",
    "href": "course-syllabus.html#required-technology",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "3.2 Required Technology",
    "text": "3.2 Required Technology\n\n\n\n\n\nR is the statistical software we will be using in this course (https://cran.r-project.org/)\n\n\n\n\n\n\n\n\nRStudio is the most popular way to interact with the R software. We will be interacting with RStudio through Posit Cloud (Posit is the company that owns RStudio). You will join the Stat 313 workspace, and then be able to access the course homework and lab assignments. We will be walking through this in the first week of lab!\n\n\n\n\n\n\n\n\nFor questions of general interest, such as course clarifications or conceptual questions, please use the Class Discord Server. Refer to the Day One Class Setup materials for more information on how to effectively use this server.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#my-course-goals",
    "href": "course-syllabus.html#my-course-goals",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "4.1 My course goals",
    "text": "4.1 My course goals\n\n\nData Visualization & Summarization\n\ncreate visualizations for one and two numerical variables\nuse facets and / or color to include additional variables into a visualization\ncalculate numerical summaries of variables\nfind summaries of variables across multiple groups\n\nWorking with Data & Reproducibility\n\nselect necessary columns from a dataset\nfilter rows from a dataset for numerical and categorical variables\nmodify existing numerical and categorical variables and / or create new variables\ncreate professional-looking, reproducible analyses using Posit projects, Quarto documents, and the here package\n\nLinear Models & Model Selection\n\nidentify which linear model is appropriate for a given research question\ndescribe the conditions required to obtain reliable estimates from linear models\nuse visualizations, summary statistics, and critical thinking to evaluate if linear model conditions are violated\nidentify methods to remedy condition violations\nfit additive and interactive linear models in R\ninterpret the coefficient estimates of a linear model\nuse visualizations and model selection techniques to determine if a specific variable should be included in a linear model\n\n\n\n\nStudy Design\n\ndistinguish between an experiment and an observational study\nidentify sources of variation and describe how to account for them\nargue what population a given sample is representative of\n\nFundamentals of Statistical Inference\n\nidentify the parameter of interest for a given linear model and associated research question\noutline the null (\\(H_0\\)) and alternative (\\(H_A\\)) hypotheses for a given research question\ndescribe what a null distribution is and how it is used to obtain a p-value\ninterpret a p-value in the context of a research question\nuse a p-value to make a decision about a hypothesis test and reach a conclusion about a research question\ndistinguish between Type I and Type II errors\ndescribe how sample size and significance level effect Type I and Type II errors\noutline the strengths and weaknesses of significance testing\ndescribe what a bootstrap distribution is and how it is used to obtain a confidence interval\ninterpret a confidence interval in the context of the parameter of interest\ndescribe the connection between confidence intervals and hypothesis testing",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-assessments-will-there-be",
    "href": "course-syllabus.html#what-assessments-will-there-be",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "5.1 What assessments will there be?",
    "text": "5.1 What assessments will there be?\nThe main idea of assessment in this course is communication and feedback. There are several different types of assessments or assignments in this class, but they will all allow you to check your own understanding and progress towards the learning goals, get in-depth feedback from me, and let me know where to spend more time or approach something differently.\nEach one is described briefly here, grouped into categories by course goal. See Section 5 for an explanation of how these contribute to your final grade. Our class schedule with topics is in Section 10.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#interpret-and-use-statistical-concepts",
    "href": "course-syllabus.html#interpret-and-use-statistical-concepts",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "5.2 Interpret and use statistical concepts",
    "text": "5.2 Interpret and use statistical concepts\n\nReadings and videos (every week)\nI favor a “flipped classroom,” as it gives you more time to clarify and solidify statistical concepts through hands-on exercises. Each week, you will read the required chapter(s), completing a required reading guide to walk you through the central concepts for each week. You are required to submit your completed reading guides by the start of class on Tuesday.\n\n\nConcept quizzes (every week)\nEach week there will be a short (~10 questions) quiz over the reading and videos from the week. These quizzes are intended to ensure that you grasped the key concepts from the week’s readings. The quizzes are not timed, so you can feel free to check your answers with the textbook and / or videos if you so wish. The quizzes are marked on completion as complete or incomplete. You are required to complete the concept quiz by the start of class on Tuesday.\n\n\nTutorials (every week)\nYou can think of the tutorials as an “interactive” textbook, as they interweave statistical ideas alongside examples of how to work with data in R and hands-on exercises writing the R code necessary to complete a given task. Each exercise has hints available if you get stuck!\nThe tutorials are work at your own pace, so you can complete them all at once or slowly throughout the week. The lab assignments will require for you to put the skills you learned in the tutorials to work, so you are required to complete each week’s tutorial before Thursday’s lab.\nThe tutorials are marked on completion as complete or incomplete. You will submit a screenshot of the completion page at the end of the tutorial to confirm that you completed the tutorial for the week. You are required to complete the tutorial by the start of class on Thursday.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#critically-evaluate-the-use-of-statistics",
    "href": "course-syllabus.html#critically-evaluate-the-use-of-statistics",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "5.3 Critically evaluate the use of Statistics",
    "text": "5.3 Critically evaluate the use of Statistics\n\nStatistical critiques (every 4-weeks)\nThese assignments are case studies in which you will evaluate a data visualization or statistical analysis, determining how well-performed and presented the analysis was and making recommendations for improving or using the analysis. Critiques are due roughly 1-week after they are assigned and should take 1-2 hours. You will receive feedback and a mark of Success or Growing (elaborated more on later), and you will be able to revise based on that feedback. There will be two total critiques.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#perform-statistical-analyses-to-answer-research-questions",
    "href": "course-syllabus.html#perform-statistical-analyses-to-answer-research-questions",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "5.4 Perform statistical analyses to answer research questions",
    "text": "5.4 Perform statistical analyses to answer research questions\n\nLab Assignments (every week)\nLabs will be assigned on Thursday every week, providing the opportunity to explore the course concepts in the context of real data. Lab assignments will require for you to work through the tutorial for the week, thus the tutorials are due before the start of class on Thursday.\nYou will complete the lab assignments in the same teams you collaborate with in class. You will access the lab assignment through Posit Cloud, which you will be walked through during the first lab. Your group will be expected to submit your completed lab on Canvas. You will need to submit only the HTML document (not the Quarto document). The individual assigned as the Report Manager (described below) will submit the team’s completed Lab assignment by Monday at 5pm.\n\nSuccess / Growing Grading\nI expect that you will approach each lab assignment seriously, investing the necessary time and energy to prepare your responses. Different from what you may have experienced, lab assignments are graded for “proficiency” of specific learning targets, which describe what you should be able to do after taking this course. You’ll receive a score for each problem on an assignment according to the Success / Not Yet rubric below, as well as feedback to help you improve.\n\n\n\n\n\n\n\nScore\nJustification\n\n\n\n\nSuccess\nThe solution to the problem is correct, legible, and easy to follow, with all reasoning provided. Any error does not bring into question your understanding of the topic.\n\n\nGrowing\nThe solution shows growth toward mastering the topic; however, elements of the solution bring into question your understanding of the topic, and thus further attention is needed.\n\n\n\n\n\nCompleting Revisions\nEvery week, your Lab assignment and / or Statistical Critique from the previous week will be graded and returned to you no later than Thursday at 9am. Meaning, Lab 2 (completed in Week 2) will be graded and returned to you by Thursday of Week 3. After the first submission of your Lab or Critique you will have the option to retry any problems for which you scored a Growing (G). A written reflection on how your understanding of the problem changed will accompany any revision.\nRevisions for a Lab or Critique are due on Thursdays at 5pm, one week from when they were returned. If you do not complete your revision within one week of when it was returned, you are no longer permitted to submit a revision. If you don’t earn a Success by your second revision, you are expected to make an appointment with me to meet during my office hours (or another agreed-upon time) to create a reassessment strategy.\n\n\nReflections on Your Learning\nRevisions must include a reflection describing how you revised your thinking when completing your revision. It’s not enough to say “[x] was wrong, so I fixed it”—you have to talk about why you got [x] wrong in the first place and what you learned that changed your mind. What do you know now that you didn’t know before? Who or what helped you learn?\n\nIf your revision does not include reflections, I’ll ask you to add them.\nSee some examples of really good reflections here – they’re (mostly) from an introductory statistics class, but I think you’ll get the idea.\n\nSubmit your revision to the same assignment box on Canvas as your original. This helps me keep track of who has outstanding revisions.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#synthesize-statistical-ideas",
    "href": "course-syllabus.html#synthesize-statistical-ideas",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "5.5 Synthesize statistical ideas",
    "text": "5.5 Synthesize statistical ideas\n\nMidterm & Final Projects\nThere will be two projects throughout the quarter, where you will be asked to synthesize the statistical concepts you have learned in a formal statistical report. Your critiques will help guide you toward how you do / don’t want your report to look. Each project will be done independently, and requires you to submit a project proposal and draft report before the final deadline. You are encouraged to use the feedback received on these assignments to improve your final report. The final reports will be graded as Excellent, Satisfactory, Progressing, or No Credit based on a rubric that will be shared with the initial assignment.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-have-accommodations-or-feel-that-accommodations-would-be-beneficial-to-my-learning",
    "href": "course-syllabus.html#what-if-i-have-accommodations-or-feel-that-accommodations-would-be-beneficial-to-my-learning",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "7.1 What if I have accommodations or feel that accommodations would be beneficial to my learning?",
    "text": "7.1 What if I have accommodations or feel that accommodations would be beneficial to my learning?\nI enthusiastically support the mission of Disability Resource Center to make education accessible to all. I design all my courses with accessibility at the forefront of my thinking, but if you have any suggestions for ways I can make things more accessible, please let me know. Come talk to me if you need accommodation for your disabilities. I honor self-diagnosis: let’s talk to each other about how we can make the course as accessible as possible. See also the standard syllabus statements, which include more information about formal processes.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#im-having-difficulty-paying-for-food-and-rent-what-can-i-do",
    "href": "course-syllabus.html#im-having-difficulty-paying-for-food-and-rent-what-can-i-do",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "7.2 I’m having difficulty paying for food and rent, what can I do?",
    "text": "7.2 I’m having difficulty paying for food and rent, what can I do?\nIf you have difficulty affording groceries or accessing sufficient food to eat every day, or if you lack a safe and stable place to live, and you believe this may affect your performance in the course, I urge you to contact the Dean of Students for support. Furthermore, please notify me if you are comfortable in doing so. This will enable me to advocate for you and to connect you with other campus resources.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#my-mental-health-is-impairing-my-ability-to-engage-in-my-classes-what-should-i-do",
    "href": "course-syllabus.html#my-mental-health-is-impairing-my-ability-to-engage-in-my-classes-what-should-i-do",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "7.3 My mental health is impairing my ability to engage in my classes, what should I do?",
    "text": "7.3 My mental health is impairing my ability to engage in my classes, what should I do?\nNational surveys of college students have consistently found that stress, sleep problems, anxiety, depression, interpersonal concerns, death of a significant other and alcohol use are among the top ten health impediments to academic performance. If you are experiencing any mental health issues, I and Cal Poly are here to help you. Cal Poly’s Counseling Services (805-756-2511) is a free and confidential resource for assistance, support and advocacy.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#someone-is-threatening-me-what-can-i-do",
    "href": "course-syllabus.html#someone-is-threatening-me-what-can-i-do",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "7.4 Someone is threatening me, what can I do?",
    "text": "7.4 Someone is threatening me, what can I do?\nI will listen and believe you if someone is threatening you. I will help you get the help you need. I commit to changing campus culture that responds poorly to dating violence and stalking.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-cant-arrange-for-childcare",
    "href": "course-syllabus.html#what-if-i-cant-arrange-for-childcare",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "7.5 What if I can’t arrange for childcare?",
    "text": "7.5 What if I can’t arrange for childcare?\nIf you are responsible for childcare on short notice, you are welcome to bring children to class with you. If you are a lactating parent, you many take breaks to feed your infant or express milk as needed. If I can support yo in navigating parenting, coursework, and other obligations in any way, please let me know.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-need-to-miss-class",
    "href": "course-syllabus.html#what-if-i-need-to-miss-class",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "8.1 What if I need to miss class?",
    "text": "8.1 What if I need to miss class?\nI encourage you to attend every class session, but policies are for narcs. I put a great deal of time into making each class session engaging and worth your time. Attendance in this course is not explicitly required, but it degrades your team’s trust in you when they don’t see you in class.\nHere’s what you should do if you do miss a class:\n\nTalk to a classmate to figure out what information you missed\nCheck Canvas for any necessary handouts or changes to assignments\nEmail me with any questions you have after reviewing notes and handouts\n\nIf you miss a bunch of classes, please come talk to me. I’m working from the assumption that you care and are trying, but something is getting in your way (health issues? depression / anxiety? college stress?); let’s figure out what that is and how I can help.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-need-to-turn-something-in-late",
    "href": "course-syllabus.html#what-if-i-need-to-turn-something-in-late",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "8.2 What if I need to turn something in late?",
    "text": "8.2 What if I need to turn something in late?\nAssignments are expected to be submitted on time. However, every student will be permitted to submit up to three individual assignments up to 3-days late, by completing a deadline extension form. Similar to the “real world,” deadline extensions must be requested before an assignment is due.\nWhen you complete the deadline extension form you will be required to state (1) what assignment you need an extension for, and (2) your proposed new deadline. Your new deadline must be within 3-days of the original deadline.\nAll deadline extensions must be done through the form, so I can keep track of who has used their allotment of extensions. If you are registered with DRC to have deadline extensions, you are required to complete a deadline extension request and make a note if your extension is related to a need related to DRC accommodations.\nAny late work is required to have a deadline extension request, meaning if you do not complete a deadline extension request for an assignment you are not permitted to turn it in late.\n\nSTAT 313 deadline extension form: https://forms.gle/HTL2DrwZwR29EwVT9\nSTAT 513 deadline extension form: https://forms.gle/2iw6mC2f2uvibUkb8",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#do-i-need-to-bring-a-computer-to-class",
    "href": "course-syllabus.html#do-i-need-to-bring-a-computer-to-class",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "8.3 Do I need to bring a computer to class?",
    "text": "8.3 Do I need to bring a computer to class?\nYou are allowed to use technology in the classroom! In fact, we will often do so as part of in-class activities. However, our class is held in a computer laboratory, so bringing a laptop is not required. You are permitted to use the lab computers, but if you would like to take notes on your computer / surface you are welcome to bring it to class.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#how-can-i-expect-to-be-treated-in-this-course",
    "href": "course-syllabus.html#how-can-i-expect-to-be-treated-in-this-course",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "9.1 How can I expect to be treated in this course?",
    "text": "9.1 How can I expect to be treated in this course?\nFollowing Ihab Hassan, I strive to teach statistics so that people will stop killing each other. In my classroom, diversity and individual differences are a sources of strength. One of the greatest failures of Statistics, historically and in the present, has been the exclusion of voices from the field. Everyone here can learn from each other, and doing so is vital to the structure of the course. Significant portions of this course involve group work and discussion in class. Some discussions will touch on sensitive topics. So that everyone feels comfortable participating in these activities, we must listen to each other and treat each other with respect. Any attitude or belief that espouses the superiority of one group of people over another is not welcome in my classroom. Such beliefs are directly destructive to the sense of community that we strive to create, and will sabotage our ability to learn from each other (and thus sabotage the entire structure of the course).\nIn summary: Be good to each other.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#working-in-teams",
    "href": "course-syllabus.html#working-in-teams",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "9.2 Working in teams",
    "text": "9.2 Working in teams\nWhen completing lab assignments, you will work in a team of three students. In Week 2, you will be assigned to a group and you will work in this same group for three total weeks (Weeks 2 - 4). Then, in Week 5 you will be assigned to a new group of three students, which you will collaborate with for the next three weeks (Weeks 5 - 7). Finally, in Week 8, you will be assigned to one final group which you will collaborate with for the final three weeks of the quarter (Weeks 8 - 10).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearching Group Collaborations\nFor quite some time I have been interested in what we, as your professors, can do to make group collaborations better for every student. This is a substantial area of my research, and, as such, I will be collecting data on your experiences collaborating with your peers in STAT 313. Every week, I will request for your to complete a pre-lab and a post-lab survey, detailing your feelings about your upcoming collaboration and your perceptions for how the collaboration went. Results from this study will provide our research team with a better understanding of (1) the impact of collaborative experiences on student self-perception, and (2) how to facilitate collaborative experiences that are beneficial for every student\nDuring Week 1, you will be asked to complete a form consenting to participate in this study. Your participation in this research will not directly affect your course grade. If you agree to participate, you will complete a weekly surveys regarding your experiences before and after collaborating in your group. Based on the results of your surveys from Weeks 2-8, you may be invited to participate in a 30-minute interview with an undergraduate student researcher, discussing your experiences collaborating with your peers.\nIf you have questions regarding this study or would like to be informed of the results when the study is completed, please contact me (Dr. Theobold) at atheobol@calpoly.edu. If you have concerns regarding the manner in which the study is conducted, you may contact Dr. Michael Black, Chair of the Cal Poly Institutional Review Board, at (805) 756-2894, mblack@calpoly.edu, or Trish Brock, Director of Research Compliance, at (805) 756-1450, pbrock@calpoly.edu.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-constitutes-plagiarism-in-a-statistics-class",
    "href": "course-syllabus.html#what-constitutes-plagiarism-in-a-statistics-class",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "9.3 What constitutes plagiarism in a statistics class?",
    "text": "9.3 What constitutes plagiarism in a statistics class?\nParaphrasing or quoting another’s work without citing the source is a form of academic misconduct. This included the R code produced by someone else! Writing code is like writing a paper, it is obvious if you copied-and-pasted a sentence from someone else into your paper because the way each person writes is different.\nEven inadvertent or unintentional misuse or appropriation of another’s work (such as relying heavily on source material that is not expressly acknowledged) is considered plagiarism. If you are struggling with writing the R code for an assignment, please reach out to me. I would prefer that I get to help you rather than you spending hours Googling things and get nowhere!\nAny incident of dishonesty, copying or plagiarism will be reported to the Office of Student Rights and Responsibilities. Cheating or plagiarism will earn you an “Incomplete” grade on the assignment and you will not be able to submit revisions for that assignment.\nIf you have any questions about using and citing sources, you are expected to ask for clarification.\nFor more information about what constitutes cheating and plagiarism, please see https://academicprograms.calpoly.edu/content/academicpolicies/Cheating.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-1-foundations-of-statistics-week-1",
    "href": "course-syllabus.html#unit-1-foundations-of-statistics-week-1",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "10.1 Unit 1: Foundations of Statistics (Week 1)",
    "text": "10.1 Unit 1: Foundations of Statistics (Week 1)\nThis introductory unit has three big tasks, (1) review statistical and data oriented concepts you have (likely) seen before, (2) think critically about why statistics is used in science, and (3) think about how (historically) statistics has been used for inference.\nReading: Chapters 1 and 2 in Introduction to Modern Statistics (IMS)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-2-exploratory-data-analysis-weeks-2-3",
    "href": "course-syllabus.html#unit-2-exploratory-data-analysis-weeks-2-3",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "10.2 Unit 2: Exploratory Data Analysis (Weeks 2 & 3)",
    "text": "10.2 Unit 2: Exploratory Data Analysis (Weeks 2 & 3)\nThis unit focuses on building skills for working with and visualizing different types of data. First, we will focus on numerical data–calculating summary statistics, histograms, scatterplots, and linegraphs. Next, we incorporate categorical variables into these summarizes (with groups) and visualizations (with colors and facets)!\nReading: Chapters 4 & 5 in IMS and Chapter 2 in Modern Dive",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-3-regression-modeling-weeks-4-5",
    "href": "course-syllabus.html#unit-3-regression-modeling-weeks-4-5",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "10.3 Unit 3: Regression Modeling (Weeks 4 & 5)",
    "text": "10.3 Unit 3: Regression Modeling (Weeks 4 & 5)\nIn this unit we begin exploring formal statistical methods. You will put the tools you learned for wrangling and visualizing to work in the context of linear regression. We will start in a familiar context—linear regression. Once we’ve explored the concepts of “simple” / basic regression we will turn up the heat and add some additional explanatory variables using multiple linear regression.\nReading: Chapters 5 & 6 in Modern Dive",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-4-model-selection-inference-for-regression-weeks-6-7",
    "href": "course-syllabus.html#unit-4-model-selection-inference-for-regression-weeks-6-7",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "10.4 Unit 4: Model Selection & Inference for Regression (Weeks 6 & 7)",
    "text": "10.4 Unit 4: Model Selection & Inference for Regression (Weeks 6 & 7)\nThis unit focuses on how we decide what variables should be included in our regression models and what we can say about the final models we obtain. We will explore these ideas using concepts you have seen before: hypothesis tests and confidence intervals. We will visit the ideas of p-values and significance testing, with a emphasis on making (and justifying) sound scientific decisions with the intention of obtaining the best regression model we can.\nReading: Supplementary resources written / compiled by Dr. Theobold",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-5-condition-violations-week-8",
    "href": "course-syllabus.html#unit-5-condition-violations-week-8",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "10.5 Unit 5: Condition Violations (Week 8)",
    "text": "10.5 Unit 5: Condition Violations (Week 8)\nThere are occasions where the conditions required for linear regression are violated. Rather than throwing up our hands and saying “Oh, well!”, we can use variable transformations to lessen condition violations. This unit will explore the use of log transformations to remedy non-linear relationships and non-constant variance.\nReading: Supplementary resources written / compiled by Dr. Theobold",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-6-anova-weeks-9-10",
    "href": "course-syllabus.html#unit-6-anova-weeks-9-10",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "10.6 Unit 6: ANOVA (Weeks 9 & 10)",
    "text": "10.6 Unit 6: ANOVA (Weeks 9 & 10)\nTo wrap up the quarter, we will look at a special case of linear regression–ANOVA. In this special case, our regression will include only categorical variables as explanatory variables. We will first review how we compare the means of two groups and then connect with what we learned about categorical variables in multiple linear to conceptualize how we can compare the means of three or more groups.\nThis unit will explore Chapter 22 in IMS, with supplementary materials created by Dr. Theobold.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "STAT 313 / 513: Applied Experimental Design and Regression Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Section 6.0.2 for information on what you can expect when you email me.↩︎\nA “Satisfactory” lab or critique occurs when every problem has been marked Satisfactory.↩︎",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "critique/critique-1.html",
    "href": "critique/critique-1.html",
    "title": "Statistical Critique 1",
    "section": "",
    "text": "In your first statistical critique will focus on critiquing a key aspect of any statistical argument—data visualization. You have explored data visualizations over the last two weeks, thinking about what makes a plot more or less clear. You will use your knowledge of data visualizations to provide a critique for two different styles of visualizations, (1) a “pop” visualization, and (2) a “scientific” visualization.\n\n\nYou are expected to include the two visualizations you decide to critique. These visualizations must be included in the paragraph surrounding your critique, not at the end of your document.\n\n\nFor the New York Times visualization, you can left click on the image and save it to your computer.\n\n\n\nFigure 1: Options when left clicking on NYT visualization – either copy the image or save it to your computer\n\n\nFor the visualization from your research article, you may need to take screenshot of the page and crop it to only include the image.\n\n\n\nFigure 2: Cropped image from scientific article\n\n\n\n\n\n\nYou are allowed to use any text editing software to make your critique (e.g., Word, Pages, Google Docs), but your submission must be a PDF. If you are unsure how to save your file as a PDF, I recommend using Google!"
  },
  {
    "objectID": "critique/critique-1.html#including-your-visualizations",
    "href": "critique/critique-1.html#including-your-visualizations",
    "title": "Statistical Critique 1",
    "section": "",
    "text": "You are expected to include the two visualizations you decide to critique. These visualizations must be included in the paragraph surrounding your critique, not at the end of your document.\n\n\nFor the New York Times visualization, you can left click on the image and save it to your computer.\n\n\n\nFigure 1: Options when left clicking on NYT visualization – either copy the image or save it to your computer\n\n\nFor the visualization from your research article, you may need to take screenshot of the page and crop it to only include the image.\n\n\n\nFigure 2: Cropped image from scientific article"
  },
  {
    "objectID": "critique/critique-1.html#submission",
    "href": "critique/critique-1.html#submission",
    "title": "Statistical Critique 1",
    "section": "",
    "text": "You are allowed to use any text editing software to make your critique (e.g., Word, Pages, Google Docs), but your submission must be a PDF. If you are unsure how to save your file as a PDF, I recommend using Google!"
  },
  {
    "objectID": "critique/critique-1.html#getting-started",
    "href": "critique/critique-1.html#getting-started",
    "title": "Statistical Critique 1",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\n\nGo to the What’s Going on in This Graph website\nScroll through the weekly graphs and click on a plot that you are interested in\nSave the image to your computer (as instructed above)"
  },
  {
    "objectID": "critique/critique-1.html#visualization-critique",
    "href": "critique/critique-1.html#visualization-critique",
    "title": "Statistical Critique 1",
    "section": "2.2 Visualization Critique",
    "text": "2.2 Visualization Critique\nYour critique of the visualization you selected needs to address the following questions:\n\nWhat aesthetics are being used in the plot?\n\n\n\n\n\n\n\nTip\n\n\n\nRemember: aesthetics map variables to aspects of the plot\n\n\n\nName at least two things the visualization does well — What makes the visualization clear to the reader?\nName at least two ways the visualization could be improved — What would the reader struggle to understand?"
  },
  {
    "objectID": "critique/critique-1.html#getting-started-1",
    "href": "critique/critique-1.html#getting-started-1",
    "title": "Statistical Critique 1",
    "section": "3.1 Getting Started",
    "text": "3.1 Getting Started\n\nAccess the research article you selected in Week 1. If you cannot find it on your computer, go to the Week 1: Statitics in Your Field assignment\nSave the image to your computer (as instructed above)"
  },
  {
    "objectID": "critique/critique-1.html#visualization-critique-1",
    "href": "critique/critique-1.html#visualization-critique-1",
    "title": "Statistical Critique 1",
    "section": "3.2 Visualization Critique",
    "text": "3.2 Visualization Critique\nYour critique of the visualization you selected needs to address the following questions:\n\nWhat aesthetics are being used in the plot?\n\n\n\n\n\n\n\nTip\n\n\n\nRemember: aesthetics map variables to aspects of the plot\n\n\n\nName at least two things the visualization does well — What makes the visualization clear to the reader?\nName at least two ways the visualization could be improved — What would the reader struggle to understand?\n\n\n\n\n\n\n\nCaution\n\n\n\nIf the article you selected does not have a visualization, you are permitted to substitute a table for a visualization. You will address the following questions:\n\nWhat aesthetics are being used in the table? Similar to a visualization, the aesthetics of a table are variables being mapped to aspects of the table.\nName at least two things the table does well — What makes the table clear to the reader?\nName at least two ways the table could be improved — What would the reader struggle to understand?"
  },
  {
    "objectID": "teams/team_formation.html",
    "href": "teams/team_formation.html",
    "title": "Team Creation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6           ✔ purrr   0.3.5      \n✔ tibble  3.1.8           ✔ dplyr   1.0.99.9000\n✔ tidyr   1.2.1           ✔ stringr 1.4.1      \n✔ readr   2.1.3           ✔ forcats 0.5.2      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(googlesheets4)\n\n\nReading in Survey Data\n\n## Smaller form with sections\nsurvey1 &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1-sW8UYoR0wjSvbXZVVCYuNF9eOqHDzY2D4kYP8gDf0w/edit?resourcekey#gid=1971034339\") %&gt;% \n  select(-1) %&gt;% \n  rename(section = `What section of STAT 313 are you in?`, \n         last = `Your last name (as seen in Canvas)`, \n         first = `Your preferred first name`, \n         group_preference = `What do you look for in group members? (select all that apply)`, \n         group_preference_other = `If you selected \\\"Other\\\" above, please elaborate on your selection!`, \n         gender = `How would you describe your gender identity? (select all that apply)`, \n         gender_other = `If you selected \\\"Prefer to self describe\\\" above, please elaborate on your selection!...8`, \n         hispanic = `Do you identify as Hispanic or Latina?`, \n         race = `Which of the following racial groups do you identify with? (select all that apply)`, \n         race_other = `If you selected \\\"Prefer to self describe\\\" above, please elaborate on your selection!...11`, \n         other_considerations = `Do you have any additional considerations you would like for me to know while I am forming teams?`) %&gt;% \n  distinct(last, .keep_all = TRUE)\n\n! Using an auto-discovered, cached token.\n\n\n  To suppress this message, modify your code or options to clearly consent to\n  the use of a cached token.\n\n\n  See gargle's \"Non-interactive auth\" vignette for more details:\n\n\n  &lt;\u001b]8;;https://gargle.r-lib.org/articles/non-interactive-auth.html\u0007https://gargle.r-lib.org/articles/non-interactive-auth.html\u001b]8;;\u0007&gt;\n\n\nℹ The googlesheets4 package is using a cached token for\n  '\u001b]8;;mailto:theobold.allison970@gmail.com\u0007theobold.allison970@gmail.com\u001b]8;;\u0007'.\n\n\nAuto-refreshing stale OAuth token.\n\n\n✔ Reading from \"Group Assignment Survey - STAT 313 Spring 2023 (Responses)\".\n\n\n✔ Range 'Form Responses 1'.\n\n\nNew names:\n• `If you selected \"Prefer to self describe\" above, please elaborate on your\n  selection!` -&gt; `If you selected \"Prefer to self describe\" above, please\n  elaborate on your selection!...8`\n• `If you selected \"Prefer to self describe\" above, please elaborate on your\n  selection!` -&gt; `If you selected \"Prefer to self describe\" above, please\n  elaborate on your selection!...11`\n\n## Larger form without sections\nsurvey2 &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1fGzz6yMcuJI-GpbUTO0Iz-CScg3Xv55lqGpgqzailDc/edit?resourcekey#gid=757833999\") %&gt;% \n  select(-1) %&gt;% \n  rename(last = `Your last name (as seen in Canvas)`, \n         first = `Your preferred first name`, \n         pronouns = `What pronouns would you like me to use in referring to you? Select all that apply!`, \n         gender = `How would you describe your gender identity? Select all that apply!`,\n         gender_other = `If you selected \\\"Prefer to self describe\\\" above, please elaborate on your selection!...6`, \n         race = `Which of the following racial groups do you identify with? Select all that apply!`, \n         race_other = `If you selected \\\"Prefer to self describe\\\" above, please elaborate on your selection!...8`, \n         hispanic = `Do you identify as Hispanic or Latinx?`, \n         group_preference = `What do you look for in group members? Select all that apply!`, \n         group_preference_other = `If you selected \\\"Other\\\" above, please elaborate on your selection!`, \n         other_considerations = `Do you have any additional considerations you would like for me to know when I am forming teams?`\n         ) %&gt;% \n  distinct(last, .keep_all = TRUE)\n\n✔ Reading from \"STAT 313 Team Scheduling Survey (Responses)\".\n✔ Range 'Form Responses 1'.\nNew names:\n\n\n\n\nJoining Surveys\nShould hopefully not need to do this next time…\n\nmatching_cols &lt;- intersect(colnames(survey1), \n                           colnames(survey2)\n                           )\n\nall_surveys &lt;- survey2 %&gt;% \n  select(all_of(matching_cols)) %&gt;% \n  full_join(survey1,\n            by = c(matching_cols)\n            )\n\n\n\nSurveys Completed\n\nsurvey1 %&gt;% \n  select(last) %&gt;% \n  arrange(last)\n\n# A tibble: 51 × 1\n   last        \n   &lt;chr&gt;       \n 1 Alarcon     \n 2 Betwos      \n 3 Bobbitt     \n 4 Caffo       \n 5 Chowaniec   \n 6 Chris Deuter\n 7 Cohn        \n 8 Colby       \n 9 Dohrmann    \n10 English     \n# … with 41 more rows\n\nsurvey2 %&gt;% \n  select(last) %&gt;% \n  arrange(last)\n\n# A tibble: 39 × 1\n   last        \n   &lt;chr&gt;       \n 1 Babb        \n 2 Bennitt     \n 3 Chmelka     \n 4 Coyne       \n 5 David       \n 6 Felices     \n 7 Franco      \n 8 Goebel      \n 9 Guglielmelli\n10 Hodges      \n# … with 29 more rows\n\n\n\n\nRemove Waitlist\n\nwaitlist &lt;- tibble(last = c(\"Lin\", \n                            \"Betwos\", \n                            \"Koo\", \n                            \"Chmelka\", \n                            \"Nayak\", \n                            \"Caffo\", \n                            \"Colby\", \n                            \"Franco\")\n                   ) %&gt;% \n  mutate(last = tolower(last)\n         )\n\nno_waitlist &lt;- all_surveys %&gt;% \n  mutate(last = tolower(last)) %&gt;% \n  anti_join(waitlist, by = \"last\")\n\n\n\nForm Groups\n\nnon_white_women &lt;- no_waitlist %&gt;% \n  filter(race != \"White\", \n         gender != \"Man / Male / Masculine\") %&gt;% \n  arrange(last)\n\nnon_white_women &lt;- no_waitlist %&gt;% \n  filter(race != \"White\", \n         gender != \"Man / Male / Masculine\") %&gt;% \n  arrange(last)"
  },
  {
    "objectID": "project/midterm-project-help/model-selection.html#one-categorical-one-numerical-explanatory-variable",
    "href": "project/midterm-project-help/model-selection.html#one-categorical-one-numerical-explanatory-variable",
    "title": "Model Selection Process",
    "section": "One Categorical & One Numerical Explanatory Variable",
    "text": "One Categorical & One Numerical Explanatory Variable"
  },
  {
    "objectID": "project/midterm-project-help/model-selection.html#two-numerical-explanatory-variables",
    "href": "project/midterm-project-help/model-selection.html#two-numerical-explanatory-variables",
    "title": "Model Selection Process",
    "section": "Two Numerical Explanatory Variables",
    "text": "Two Numerical Explanatory Variables"
  },
  {
    "objectID": "project/final_project_template.html#variables",
    "href": "project/final_project_template.html#variables",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Variables",
    "text": "Variables"
  },
  {
    "objectID": "project/final_project_template.html#data-visualizations",
    "href": "project/final_project_template.html#data-visualizations",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Data Visualizations",
    "text": "Data Visualizations"
  },
  {
    "objectID": "project/final_project_template.html#description-of-relationships",
    "href": "project/final_project_template.html#description-of-relationships",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Description of Relationships",
    "text": "Description of Relationships"
  },
  {
    "objectID": "project/final_project_template.html#model-conditions",
    "href": "project/final_project_template.html#model-conditions",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Model Conditions",
    "text": "Model Conditions"
  },
  {
    "objectID": "project/final_project_template.html#one-way-anova-of-and",
    "href": "project/final_project_template.html#one-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "One-Way ANOVA of  and ",
    "text": "One-Way ANOVA of  and"
  },
  {
    "objectID": "project/final_project_template.html#one-way-anova-of-and-1",
    "href": "project/final_project_template.html#one-way-anova-of-and-1",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "One-Way ANOVA of  and ",
    "text": "One-Way ANOVA of  and"
  },
  {
    "objectID": "project/final_project_template.html#additive-two-way-anova-of-and",
    "href": "project/final_project_template.html#additive-two-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Additive Two-Way ANOVA of , , AND ",
    "text": "Additive Two-Way ANOVA of , , AND"
  },
  {
    "objectID": "project/final_project_template.html#interaction-two-way-anova-of-and",
    "href": "project/final_project_template.html#interaction-two-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Interaction Two-Way ANOVA of , , AND ",
    "text": "Interaction Two-Way ANOVA of , , AND"
  },
  {
    "objectID": "project/final_project_template.html#model-conclusions",
    "href": "project/final_project_template.html#model-conclusions",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Model Conclusions",
    "text": "Model Conclusions"
  },
  {
    "objectID": "project/final-presentations/presentation-instructions.html",
    "href": "project/final-presentations/presentation-instructions.html",
    "title": "Final Project Presentations",
    "section": "",
    "text": "During finals week each of you will give a 3-minute presentation on one aspect of your final project you found the most interesting. Notice, you need to pick one aspect, since your presentation is so short.\nHere are some examples of what you could choose:\nOf course you could choose other topics, the only requirement is that your topic is related to what you did in your final project."
  },
  {
    "objectID": "project/final-presentations/presentation-instructions.html#slides",
    "href": "project/final-presentations/presentation-instructions.html#slides",
    "title": "Final Project Presentations",
    "section": "Slides",
    "text": "Slides\nFor your presentation you are allowed to make two slides:\n\nA title slide (make it fun!) with your name\nA content slide\n\nYou slides are due on Wednesday by 8am.\n\n\n\n\n\n\nCaution\n\n\n\nYour slides must be submitted as a PDF. Make sure they are saved in the correct orientation!"
  },
  {
    "objectID": "project/final-presentations/presentation-instructions.html#presentations",
    "href": "project/final-presentations/presentation-instructions.html#presentations",
    "title": "Final Project Presentations",
    "section": "Presentations",
    "text": "Presentations\nFor ease, I will randomly order the presentations. If you have a strong preference for when you would like to present, you can request a position. However, I cannot guarantee that I can accommodate every request.\nI will publish the order of presentations by Wednesday at 5pm."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html",
    "href": "project/final-project-help/twa-model-selection-process.html",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "",
    "text": "We will be using a “backward” selection process for deciding on the “best” ANOVA model. Meaning, we will start with a more complicated model and decide if it is “worth it.”"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#plot-the-model",
    "href": "project/final-project-help/twa-model-selection-process.html#plot-the-model",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Plot the model",
    "text": "Plot the model\n\n\n\n\n\n\n\n\n\nWhat are we looking for?\nWe are looking to see if relationship between the mean of \\(y\\) and explanatory variable 1 changes over the values of explanatory variable 2. This is akin to looking for evidence of different slopes in a multiple linear regression.\nFor these data we are looking to see if the relationship between the mean movie rating and the genre of the movie changes over the different eras."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#fit-the-model",
    "href": "project/final-project-help/twa-model-selection-process.html#fit-the-model",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Fit the model",
    "text": "Fit the model\nNotice I’m using a * for an interaction model!\n\naov(rating ~ era * genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 4 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.659   0.581\n2 genre         1   4.19   4.19     1.81    0.185\n3 era:genre     3   3.32   1.11     0.477   0.700\n4 Residuals    48 111.     2.32    NA      NA"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test",
    "href": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nThe era:genre line of the ANOVA table is testing if the mean movie rating and the genre of the movie changes over the different eras. It has the following hypotheses:\n\\(H_0\\): The relationship between the mean movie rating and the genre of the movie does not change over the eras\n\\(H_A\\): The relationship between the mean movie rating and the genre of the movie is different for at least one era\nWith a p-value of 0.7 (from an F-statistic of 0.477 with 3 and 48 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating and the genre of the movie is different for at least one era.\nSo, my next step is to fit an additive two-way ANOVA."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#fit-the-model-1",
    "href": "project/final-project-help/twa-model-selection-process.html#fit-the-model-1",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Fit the Model",
    "text": "Fit the Model\nNotice I’m using a + for an interaction model!\n\naov(rating ~ era + genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 3 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.680   0.568\n2 genre         1   4.19   4.19     1.86    0.178\n3 Residuals    51 115.     2.25    NA      NA"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-1",
    "href": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-1",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nFor an additive two-way ANOVA, we have two hypothesis tests, one test per explanatory variable.\n\nHypothesis Test for genre\nThe genre line of the ANOVA table is testing if the mean movie rating is the same for all genres. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every genre, given the era of the movie\n\\(H_A\\): The mean movie rating is different for at least one genre, given the era of the movie\nNotice the hypotheses are similar to a multiple linear regression with two explanatory variables, where the test for each variable is conditional on the other variable(s) included in the model.\nWith a p-value of 0.178 (from an F-statistic of 1.86 with 1 and 51 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that at least one genre has a different mean movie rating, given the era of the movie.\n\n\nHypothesis Test for era\nThe era line of the ANOVA table is testing if the mean movie rating is the same for all eras. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every era, given the genre of the movie\n\\(H_A\\): The mean movie rating is different for at least one era, given the genre of the movie\nWith a p-value of 0.568 (from an F-statistic of 0.68 with 3 and 51 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that at least one era has a different mean movie rating, given the genre of the movie."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#fit-the-model-2",
    "href": "project/final-project-help/twa-model-selection-process.html#fit-the-model-2",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Fit the Model",
    "text": "Fit the Model\n\naov(rating ~ genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 genre         1   4.68   4.68      2.13   0.150\n2 Residuals    54 119.     2.20     NA     NA"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-2",
    "href": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-2",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nThe genre line of the ANOVA table is testing if the mean movie rating is the same for all genres. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every genre\n\\(H_A\\): The mean movie rating is different for at least one genre\nNotice the hypotheses don’t include era this time, since that variable has been removed from the model!\nWith a p-value of 0.15 (from an F-statistic of 2.13 with 1 and 54 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that at least one genre has a different mean movie rating."
  },
  {
    "objectID": "project/final-project-directions.html",
    "href": "project/final-project-directions.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "For this project, you are expected to use a two-way ANOVA to investigate the relationship between one numerical response variable and two categorical explanatory variables. You are permitted to use the same dataset as your Midterm Project, so long as there are at least two categorical variables to choose from.\n\n\n\n\n\n\nNote\n\n\n\nIf you would like to analyze a discrete numerical variable (e.g., number of pets) as a categorical variable, you will need to convert that variable into a categorical variable in R as R assumes all variables with numbers should be numerical.\nYou will need Dr. Theobold’s help to perform this task. Dr. Theobold will work with you to convert your variable as long as you request help before Friday at 4pm.",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#your-task",
    "href": "project/final-project-directions.html#your-task",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "For this project, you are expected to use a two-way ANOVA to investigate the relationship between one numerical response variable and two categorical explanatory variables. You are permitted to use the same dataset as your Midterm Project, so long as there are at least two categorical variables to choose from.\n\n\n\n\n\n\nNote\n\n\n\nIf you would like to analyze a discrete numerical variable (e.g., number of pets) as a categorical variable, you will need to convert that variable into a categorical variable in R as R assumes all variables with numbers should be numerical.\nYou will need Dr. Theobold’s help to perform this task. Dr. Theobold will work with you to convert your variable as long as you request help before Friday at 4pm.",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#data-description",
    "href": "project/final-project-directions.html#data-description",
    "title": "Final Project Guidelines",
    "section": "1.1 Data Description",
    "text": "1.1 Data Description\nIn 4-6 sentences describe:\n\nhow the data were collected\nthe context of the data (e.g., are the data from from a published study?)\nthe background of the research problem (e.g., why were the data collected?)",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#questions-of-interest",
    "href": "project/final-project-directions.html#questions-of-interest",
    "title": "Final Project Guidelines",
    "section": "1.2 Questions of Interest",
    "text": "1.2 Questions of Interest\nState the question(s) of interest you will address with your statistical analysis. The more specific you define the question of interest here, the easier the rest of the analysis and report will be. The research questions should start with, “What is the relationship between…” and should be as specific as possible. Your Findings section should directly address the question(s) you pose here.\n\n\n\n\n\n\nMultiple research questions\n\n\n\nThis week you are starting with research questions are appropriate for the one-way ANOVA models you are fitting. However, you may find that you need to add research questions based on the models you fit in Week 10!",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#one-way-anova-model",
    "href": "project/final-project-directions.html#one-way-anova-model",
    "title": "Final Project Guidelines",
    "section": "3.1 One-Way ANOVA Model",
    "text": "3.1 One-Way ANOVA Model\nEveryone starts here! In this section, you need to do the following:\n\nFit two one-way ANOVA models – one model for each categorical explanatory variable\nObtain the ANOVA table for the model\nBased on the ANOVA table, state what decision was reached for the hypothesis in the one-way ANOVA model\nBased on the decision you made, state what you can conclude regarding the relationship between your variables",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#two-way-anova-additive-model",
    "href": "project/final-project-directions.html#two-way-anova-additive-model",
    "title": "Final Project Guidelines",
    "section": "3.2 Two-Way ANOVA Additive Model",
    "text": "3.2 Two-Way ANOVA Additive Model\n\n\n\n\n\n\nWhen to proceed\n\n\n\nIf you rejected the null hypothesis for both one-way ANOVA models, you can proceed to fitting a two-way ANOVA model\n\n\n\nFit a two-way ANOVA additive model\nObtain the tidy ANOVA table for the model\nBased on the ANOVA table, state what decision was reached for each hypothesis in the two-way ANOVA additive model.\nBased on the decision you made, state what you can conclude regarding the relationship between your variables.",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#conclusions",
    "href": "project/final-project-directions.html#conclusions",
    "title": "Final Project Guidelines",
    "section": "3.3 Conclusions",
    "text": "3.3 Conclusions\nBased on the results of your analysis what is your conclusion for the questions of interest? Connect your conclusion(s) to the relationships you saw in the visualizations you made.\nIn this section you should also describe whether you believe the tests you performed are “reliable”. Meaning, did you violate any of the conditions required of a one-way ANOVA model?",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "slides/week7-day1.html#section",
    "href": "slides/week7-day1.html#section",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Sampling Strategies"
  },
  {
    "objectID": "slides/week7-day1.html#section-1",
    "href": "slides/week7-day1.html#section-1",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "What types of samples could we collect? Are some methods “better” than other methods?"
  },
  {
    "objectID": "slides/week7-day1.html#section-2",
    "href": "slides/week7-day1.html#section-2",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "At your table…\n\n\n\n\nFirst\n\n\neach person samples 10 salaries\ncalculate the median\n\n\n\n\n\nThen\n\n\ncalculate the median of all 25 salaries"
  },
  {
    "objectID": "slides/week7-day1.html#section-3",
    "href": "slides/week7-day1.html#section-3",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Each table has a sample of 25 UC & CSU coach salaries.\n\n\n\nWould you feel comfortable inferring that the median salary of your sample is close to the median salary of all UC & CSU coaches?\n\n\nWhy or why not?"
  },
  {
    "objectID": "slides/week7-day1.html#section-4",
    "href": "slides/week7-day1.html#section-4",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Why sample more than once?\n\n\nVariability is a central focus of the discipline of Statistics!\n\n\nMaking decisions based on limited information is uncomfortable!\n\n\n\nYou likely weren’t willing to infer the population median salary from your sample!"
  },
  {
    "objectID": "slides/week7-day1.html#section-5",
    "href": "slides/week7-day1.html#section-5",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Sampling Framework\n\npopulation – collection of observations / individuals we are interested in\npopulation parameter – numerical summary about the population that is unknown but you wish you knew\n\n\nsample – a collection of observations from the population\nsample statistic – a summary statistic computed from a sample that estimates the unknown population parameter."
  },
  {
    "objectID": "slides/week7-day1.html#section-6",
    "href": "slides/week7-day1.html#section-6",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Statistical Inference\n\nThere were 252 “Head Coaches” at University of California and California State Universities in 2019 (that satisfied my search criteria)\n\n\n\nMedian salary for all 252 coaches\n$137,619\n\n\n\nInferring information from your sample onto the population is called statistical inference."
  },
  {
    "objectID": "slides/week7-day1.html#section-7",
    "href": "slides/week7-day1.html#section-7",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Statistical Inference Reasoning\n\n\n\nIf the sampling is done at random\nthe sample is representative of the population\nany result based on the sample can generalize to the population\nthe point estimate is a “good guess” of the unknown population parameter"
  },
  {
    "objectID": "slides/week7-day1.html#section-8",
    "href": "slides/week7-day1.html#section-8",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Shouldn’t one random sample be enough then? Isn’t that what we use to make confidence intervals and do hypothesis tests?"
  },
  {
    "objectID": "slides/week7-day1.html#section-9",
    "href": "slides/week7-day1.html#section-9",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Virtual Sampling\n\n\nrep_sample_n(coaches, \n             size = 25, \n             reps = 1, \n             replace = TRUE)\n\n\n\n\n\n\n\n\nEmployee Name\nJob Title\nTotal Pay & Benefits\n\n\n\n\nBeau Baldwin\nAsc Head Coach Crd 4\n708408\n\n\nStein Metzger\nIntercol Ath Head Coach Ex\n191728\n\n\nJordan Wolfrum\nIntercol Ath Head Coach Ex\n76597\n\n\nDavid Bradley Kreutzkamp\nHead Coach 5\n105683\n\n\nDaniel Dykes\nHead Coach 5\n540000\n\n\nDaniel Conners\nHead Coach 5\n156181\n\n\n\n\n\n\n\n\n\n\\(\\vdots\\)"
  },
  {
    "objectID": "slides/week7-day1.html#section-10",
    "href": "slides/week7-day1.html#section-10",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Distribution of 1000 medians from samples of 25 coaches"
  },
  {
    "objectID": "slides/week7-day1.html#section-11",
    "href": "slides/week7-day1.html#section-11",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Sampling Distributions\n\n\nVisualize the effect of sampling variation on the distribution of any point estimate\n\nIn this case, the sample median\n\nWe can use sampling distributions to make statements about what values we can typically expect.\n\n\n\nBe careful! A sampling distribution is different from a sample’s distribution!"
  },
  {
    "objectID": "slides/week7-day1.html#section-12",
    "href": "slides/week7-day1.html#section-12",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Distributions of 1000 medians from different sample sizes\n\n\n\nWhat differences do you see?"
  },
  {
    "objectID": "slides/week7-day1.html#section-13",
    "href": "slides/week7-day1.html#section-13",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Variability for Different Sample Sizes\n\n\n\n\n\n\n\nSample Size\nStandard Error of Median\n\n\n\n\n25\n19343.969\n\n\n50\n12459.358\n\n\n100\n8279.311\n\n\n\n\n\n\n\n\n\n\n\nStandard errors quantify the variability of point estimates\nAs a general rule, as sample size increases, the standard error decreases.\n\n\n\n\n\n\nCareful! There are important differences between standard errors and standard deviations."
  },
  {
    "objectID": "slides/week7-day1.html#section-14",
    "href": "slides/week7-day1.html#section-14",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "A good guess?"
  },
  {
    "objectID": "slides/week7-day1.html#section-15",
    "href": "slides/week7-day1.html#section-15",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Precision & Accuracy\n\n\n\n\n\n\n\n\nRandom sampling ensures our point estimates are accurate.\n\n\n\nLarger sample sizes ensure our point estimates are precise."
  },
  {
    "objectID": "slides/week7-day1.html#section-16",
    "href": "slides/week7-day1.html#section-16",
    "title": "Sampling Variability – The Heart of Inference",
    "section": "",
    "text": "Sampling Activity!"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section",
    "href": "slides/week3-day2-alt.html#section",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "What are the two data types R stores categorical variables as?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#dplyr-a-tool-bag-for-data-wrangling",
    "href": "slides/week3-day2-alt.html#dplyr-a-tool-bag-for-data-wrangling",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "dplyr – a tool bag for data wrangling",
    "text": "dplyr – a tool bag for data wrangling\n\n\n\n\n\n\nfilter()\nselect()\nmutate()\nsummarize()\narrange()\ngroup_by()"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-1",
    "href": "slides/week3-day2-alt.html#section-1",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "The Pipe %&gt;%"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-2",
    "href": "slides/week3-day2-alt.html#section-2",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "If you wanted means for each level of a categorical variable, what would you do?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#trout-size",
    "href": "slides/week3-day2-alt.html#trout-size",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "Trout Size",
    "text": "Trout Size\n\nThe HJ Andrews Experimental Forest houses one of the larges long-term ecological research stations, specifically researching cutthroat trout and salamanders in clear cut or old growth sections of Mack Creek.\n\n\n\ntrout %&gt;% \n  group_by(section) %&gt;% \n  summarize(mean_length = mean(length_1_mm, na.rm = TRUE)\n            )\n\n\n\n# A tibble: 2 × 2\n  section                               mean_length\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 clear cut forest                             85.3\n2 upstream old growth coniferous forest        81.4\n\n\n\n\n\n\nWhy na.rm = TRUE?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#classifying-channel-types",
    "href": "slides/week3-day2-alt.html#classifying-channel-types",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "Classifying Channel Types",
    "text": "Classifying Channel Types\n\nThe channels of the Mack Creek which were sampled were classified into the following groups:\n\n\n\n\"C\"\n\"I\"\n\"IP\"\n\"P\"\n\"R\"\n\"S\"\n\"SC\"\nNA\n\n\n\ncascade\nriffle\nisolated pool\npool\nrapid\nstep (small falls)\nside channel\nnot sampled by unit"
  },
  {
    "objectID": "slides/week3-day2-alt.html#filter-ing-specific-channel-types",
    "href": "slides/week3-day2-alt.html#filter-ing-specific-channel-types",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "filter()-ing Specific Channel Types",
    "text": "filter()-ing Specific Channel Types\n\nThe majority of the Cutthroat trout were captured in cascades (C), pools (P), and side channels (SC). Suppose we want to only retain these levels of the unittype variable.\n\n\n\n\ntrout %&gt;% \n  filter(unittype %in% c(\"C\", \"P\", \"SC\"))\n\n\n\n\n\n\n\n\n\n\n%in%\n\n\nIf you filter includes more than one value you must use %in% not ==!"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-3",
    "href": "slides/week3-day2-alt.html#section-3",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "Categorical Variables for Whom?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-4",
    "href": "slides/week3-day2-alt.html#section-4",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "Suppose Cal Poly is interested in summarizing the demographics of their undergraduate students. They have designed the following question asking about student’s gender identity:\n\n\n\nWhat is your gender identity?\nMale, Female, Other\n\n\n\nWho benefits from these options?\nWho suffers from these options?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-5",
    "href": "slides/week3-day2-alt.html#section-5",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "Data Feminism\n\n\n\n\n\n\n\n\n\nData science by whom?\nData science for whom?\nData sets about whom?\nData science with whose values?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-7",
    "href": "slides/week3-day2-alt.html#section-7",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "Rethink binaries\n\n\n \n\n\nHow would you redesign the survey question about student’s gender identity?"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-8",
    "href": "slides/week3-day2-alt.html#section-8",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "Challenge power"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-9",
    "href": "slides/week3-day2-alt.html#section-9",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "An aura objectivity\n\n\n\n\n\n\n\n\n\n\n\n\n“We focus on four conventions which imbue visualizations with a sense of objectivity, transparency and facticity. These include: (a) two-dimensional viewpoints, (b) clean layouts, (c) geometric shapes and lines, (d) the inclusion of data sources.”\nThe work that visualization communications do"
  },
  {
    "objectID": "slides/week3-day2-alt.html#section-10",
    "href": "slides/week3-day2-alt.html#section-10",
    "title": "Categorical Variables Recap and Some Ethics…",
    "section": "",
    "text": "Elevate emotion\n\nhttps://guns.periscopic.com/"
  },
  {
    "objectID": "slides/week7-day2.html#section",
    "href": "slides/week7-day2.html#section",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Approximate the variability you’d expect to see in other samples!\n\n\n\n\nBootstrapping!"
  },
  {
    "objectID": "slides/week7-day2.html#section-1",
    "href": "slides/week7-day2.html#section-1",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "A Bootstrap Resample\n\n\n\nAssumes the original sample is “representative” of observations in the population.\n\n\n\n\nUses the original sample to generate new samples that might have occurred with additional sampling.\n\n\n\n\n\nWe can use the statistics from these bootstrap samples to approximate the true sampling distribution!"
  },
  {
    "objectID": "slides/week7-day2.html#section-2",
    "href": "slides/week7-day2.html#section-2",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Why???"
  },
  {
    "objectID": "slides/week7-day2.html#section-3",
    "href": "slides/week7-day2.html#section-3",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Estimating a population parameter\n\n\n\nWe are interested in knowing how a statistic varies from sample to sample.\n\n\n\n\nKnowing a statistic’s behavior helps us make better / more informed decisions!\n\n\n\n\nThis helps us estimate what values are more or less likely for the population parameter to have."
  },
  {
    "objectID": "slides/week7-day2.html#section-4",
    "href": "slides/week7-day2.html#section-4",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Confidence Intervals\n\n\n\nCapture a range of plausible values for the population parameter.\n\n\n\n\n\nAre more likely to capture the population parameter than a point estimate."
  },
  {
    "objectID": "slides/week7-day2.html#section-5",
    "href": "slides/week7-day2.html#section-5",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Using bootstrap resamples to generate a confidence interval\n\n\nFrom your original sample, resample with replacement the same number of times as your original sample.\n\n\n\n\nThis is your bootstrap resample.\n\n\n\n\nRepeat this process many, many times.\n\n\nCalculate a numerical summary (e.g., mean, median) for each bootstrap resample.\n\n\nThese are your bootstrap statistics"
  },
  {
    "objectID": "slides/week7-day2.html#section-6",
    "href": "slides/week7-day2.html#section-6",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Bootstrap Distribution\n\n\n\ndefinition: a distribution of the bootstrap statistics from every bootstrap resample\n\n\n\n\nDisplays the variability in the statistic that could have happened with repeated sampling.\n\n\n\nApproximates the true sampling distribution!"
  },
  {
    "objectID": "slides/week7-day2.html#section-7",
    "href": "slides/week7-day2.html#section-7",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Confidence Interval\n\nGoal: Capture a range of plausible values for the population parameter."
  },
  {
    "objectID": "slides/week7-day2.html#section-8",
    "href": "slides/week7-day2.html#section-8",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "How do I get this plausible range of values?\n\n\n\n\n\nBootstrapping!"
  },
  {
    "objectID": "slides/week7-day2.html#section-9",
    "href": "slides/week7-day2.html#section-9",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Penguins!"
  },
  {
    "objectID": "slides/week7-day2.html#section-10",
    "href": "slides/week7-day2.html#section-10",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Statistic: \\(\\beta_1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe relationship between penguin’s bill length and body mass for all penguins in the Palmer Archipelago"
  },
  {
    "objectID": "slides/week7-day2.html#section-11",
    "href": "slides/week7-day2.html#section-11",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Generating a bootstrap resample\n\n\nStep 1: specify() your response and explanatory variables\n\n\nStep 2: generate() bootstrap resamples\n\n\nStep 3: calculate() the statistic of interest"
  },
  {
    "objectID": "slides/week7-day2.html#section-12",
    "href": "slides/week7-day2.html#section-12",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Declare your variables!\n\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, explanatory = body_mass_g)"
  },
  {
    "objectID": "slides/week7-day2.html#section-13",
    "href": "slides/week7-day2.html#section-13",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Generate your resamples!\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g) %&gt;% \n  generate(reps = 500, type = \"bootstrap\")\n\n\n\nreps – the number of resamples you want to generate\n\"bootstrap\" – the method that should be used to generate the new samples"
  },
  {
    "objectID": "slides/week7-day2.html#section-14",
    "href": "slides/week7-day2.html#section-14",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Your turn!\n\n\n\nWhy do we resample with replacement when creating a bootstrap distribution?\n\nWhen we resample with replacement from our original sample what are we assuming about our sample?"
  },
  {
    "objectID": "slides/week7-day2.html#section-15",
    "href": "slides/week7-day2.html#section-15",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Calculate your statistics!\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g) %&gt;% \n  generate(reps = 500, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n\"slope\" – the statistic of interest"
  },
  {
    "objectID": "slides/week7-day2.html#section-16",
    "href": "slides/week7-day2.html#section-16",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "The final product\n\n\nvisualize(boot1) + \n  labs(title = \"Bootstrap Distribution of 500 reps\", \n       x = \"Slope Statistic\")\n\n\n\nWhat does one dot / point on a bootstrap distribution represent?"
  },
  {
    "objectID": "slides/week7-day2.html#section-17",
    "href": "slides/week7-day2.html#section-17",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "A plausible range of values for: \\(\\beta_1\\)\n\n\nvisualise(boot1) +\n  shade_confidence_interval(endpoints = boot1_CI, \n                            color = \"red\", fill = \"pink\") +  \n  labs(title = \"Bootstrap Distribution of 500 reps\", \n       x = \"Slope Statistic\")"
  },
  {
    "objectID": "slides/week7-day2.html#section-18",
    "href": "slides/week7-day2.html#section-18",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "The 95% confidence interval is…\n\n\nget_confidence_interval(boot1, \n                        level = 0.95, \n                        type = \"percentile\")\n\n\n\n\n\n\n\nLower Bound\nUpper Bound\n\n\n\n\n0.00353\n0.00458\n\n\n\n\n\n\n\n\n\nWhat do we hope is captured by this interval?"
  },
  {
    "objectID": "slides/week7-day2.html#section-19",
    "href": "slides/week7-day2.html#section-19",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "How do we interpret this interval?\n\n\n\n“We are 95% confident the slope of the relationship between bill length and body mass for all penguins in the Palmer Archipelago is between 0.00355 and 0.00453\n\n\n\nWhat does it mean to be 95% confident?"
  },
  {
    "objectID": "slides/week7-day2.html#section-20",
    "href": "slides/week7-day2.html#section-20",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Classic interpretation mistakes\n\n\n\n\n“95% of the time the population parameter would fall between 0.00355 and 0.00453.”\n\n\n\n\n\n“We are 95% confident the sample statistic is in our interval.”"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section",
    "href": "slides/week7-day2-how-to-bootstrap.html#section",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Bootstrap Distribution\n\n\n\ndefinition: a distribution of the bootstrap statistics from every bootstrap resample\n\n\n\n\nDisplays the variability in the statistic that could have happened with repeated sampling.\n\n\n\nApproximates the true sampling distribution!"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-1",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-1",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Penguins!"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-2",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-2",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Statistic: \\(\\beta_1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe relationship between penguin’s bill length and body mass for all penguins in the Palmer Archipelago"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-3",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-3",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Generating a bootstrap resample\n\n\nStep 1: specify() your response and explanatory variables\n\n\nStep 2: generate() bootstrap resamples\n\n\nStep 3: calculate() the statistic of interest"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-4",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-4",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Declare your variables!\n\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, explanatory = body_mass_g)"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-5",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-5",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Generate your resamples!\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g) %&gt;% \n  generate(reps = 500, type = \"bootstrap\")\n\n\n\nreps – the number of resamples you want to generate\n\"bootstrap\" – the method that should be used to generate the new samples"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-6",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-6",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "Calculate your statistics!\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g) %&gt;% \n  generate(reps = 500, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n\"slope\" – the statistic of interest"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-7",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-7",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "The final product\n\n\nvisualize(bootstrap) + \n  labs(title = \"Bootstrap Distribution of 500 reps\", \n       x = \"Slope Statistic\")"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-8",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-8",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "A plausible range of values for: \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/week7-day2-how-to-bootstrap.html#section-9",
    "href": "slides/week7-day2-how-to-bootstrap.html#section-9",
    "title": "How do you bootstrap???",
    "section": "",
    "text": "The 95% confidence interval is…\n\n\nget_confidence_interval(bootstrap, \n                        level = 0.95, \n                        type = \"percentile\")\n\n\n\n\n\n\n\nLower Bound\nUpper Bound\n\n\n\n\n0.0035\n0.00452"
  },
  {
    "objectID": "slides/week1-day2.html#section",
    "href": "slides/week1-day2.html#section",
    "title": "Working with Data",
    "section": "",
    "text": "There’s Data\n\n\nGender stereotypes in 5-7 year old children\n\n\n\n\n\n\n\n\nsubject\nsex\nage\ntrait\ntarget\nstereotype\nhigh_achieve_caution\n\n\n\n\n29\nfemale\n5\nsmart\nchildren\n1.00\n0.75\n\n\n115\nmale\n5\nnice\nadults\n0.75\n1.00\n\n\n38\nfemale\n7\nsmart\nchildren\n0.75\n0.75\n\n\n40\nfemale\n6\nsmart\nadults\n0.50\n0.75\n\n\n32\nfemale\n6\nsmart\nadults\n0.75\n0.75\n\n\n91\nfemale\n5\nsmart\nchildren\n1.00\n0.75"
  },
  {
    "objectID": "slides/week1-day2.html#section-1",
    "href": "slides/week1-day2.html#section-1",
    "title": "Working with Data",
    "section": "",
    "text": "Lots of Data\n\nBody girth measurements and skeletal diameter measurements for 247 men and 260 women.\n\n\n\n\n\n\nage\nwgt\nhgt\nsex\nsho_gi\nwai_gi\nnav_gi\nhip_gi\n\n\n\n\n25\n64.1\n177.8\n1\n104.8\n70.8\n84.9\n89.4\n\n\n27\n60.2\n161.3\n0\n107.3\n72.7\n76.7\n95.9\n\n\n51\n72.2\n174.0\n1\n113.1\n82.9\n83.6\n95.8\n\n\n20\n54.6\n156.0\n0\n100.3\n66.0\n73.2\n90.2\n\n\n33\n66.4\n174.0\n0\n107.1\n72.2\n89.4\n98.6\n\n\n29\n91.8\n186.7\n1\n123.0\n87.6\n89.4\n106.7"
  },
  {
    "objectID": "slides/week1-day2.html#section-2",
    "href": "slides/week1-day2.html#section-2",
    "title": "Working with Data",
    "section": "",
    "text": "In Every Context\n\n\nNBA player of the week from 1985 to 2016\n\n\n\n\n\n\n\n\nAge\nDate\nDraft Year\nHeight\nPlayer\nPosition\n\n\n\n\n26\nJan 30, 2017\n2010\n211cm\nDeMarcus Cousins\nC\n\n\n32\nDec 12, 2016\n2003\n203cm\nLeBron James\nF\n\n\n28\nApr 10, 2017\n2008\n191cm\nRussell Westbrook\nG\n\n\n27\nMar 20, 2017\n2010\n213cm\nHassan Whiteside\nC\n\n\n22\nMar 17, 2002\n1997\n6-8\nTracy McGrady\nGF\n\n\n25\nMar 13, 1988\n1983\n6-7\nClyde Drexler\nSG"
  },
  {
    "objectID": "slides/week1-day2.html#section-3",
    "href": "slides/week1-day2.html#section-3",
    "title": "Working with Data",
    "section": "",
    "text": "You Can Imagine\n\n\nFish Sampled on Blackfoot River\n\n\n\n\n\n\n\n\ntrip\nmark\nlength\nweight\nyear\nsection\nspecies\n\n\n\n\n2\n0\n470\n790\n2002\nJohnsrud\nRBT\n\n\n2\n1\n210\nNA\n2004\nJohnsrud\nRBT\n\n\n2\n0\n186\n60\n1991\nScottyBrown\nWCT\n\n\n2\n0\n282\n290\n1998\nScottyBrown\nWCT\n\n\n1\n0\n182\n54\n1996\nJohnsrud\nRBT\n\n\n1\n0\n456\n940\n2004\nJohnsrud\nBrown"
  },
  {
    "objectID": "slides/week1-day2.html#section-4",
    "href": "slides/week1-day2.html#section-4",
    "title": "Working with Data",
    "section": "",
    "text": "Your Turn\n\n\nEvery year, the US releases to the public a large data set containing information on births recorded in the country.\nA total of 13 variables were collected on every birth, including information about:\n\nthe birth (baby weight, sex of baby, premie status)\nthe pregnancy (hospital visits, length of gestation, )\nthe mother’s attributes (age, smoking status, marital status, race)\nthe father’s age\n\n\n\n\nHow would you expect this dataframe to look?"
  },
  {
    "objectID": "slides/week1-day2.html#section-5",
    "href": "slides/week1-day2.html#section-5",
    "title": "Working with Data",
    "section": "",
    "text": "Types of Variables\n\n\n\nDiagram of types of variables we will analyze!"
  },
  {
    "objectID": "slides/week1-day2.html#section-6",
    "href": "slides/week1-day2.html#section-6",
    "title": "Working with Data",
    "section": "",
    "text": "Examples\n\n\nA person’s height (usually) would be a continuous, numerical variable\nThe number of classes someone takes would be a discrete, numerical variable\nA course letter grade would be a ordinal, categorical variable\nThe color of someone’s hair would be a regular, categorical variable"
  },
  {
    "objectID": "slides/week1-day2.html#section-7",
    "href": "slides/week1-day2.html#section-7",
    "title": "Working with Data",
    "section": "",
    "text": "Your Turn\n\n\nSuppose researchers have yearly data on Elephant Seal abundance on Pedras Blancas from 2010 - 2014.\n\n\n\nWhat type of variable would year be?"
  },
  {
    "objectID": "slides/week1-day2.html#section-8",
    "href": "slides/week1-day2.html#section-8",
    "title": "Working with Data",
    "section": "",
    "text": "Types of Studies\n\n\n\nExperiment\n\nrandomization\nreplication\ncontrolling\nblocking\n\n\n\n\nObservational Study\n\ncollect data in a way that does not directly interfere with how the data arise"
  },
  {
    "objectID": "slides/week1-day2.html#section-9",
    "href": "slides/week1-day2.html#section-9",
    "title": "Working with Data",
    "section": "",
    "text": "Relationships Between Variables\n\n\nexplanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable\n\n\nIf two variables are not associated, then they are said to be independent.\nIf two variables are associated, then they are said to be dependent."
  },
  {
    "objectID": "slides/week1-day2.html#section-10",
    "href": "slides/week1-day2.html#section-10",
    "title": "Working with Data",
    "section": "",
    "text": "Causal Inference\n\n\nassociation \\(\\neq\\) causation\n\n\nWhat do you need to say that the explanatory variable causes a change in the response variable?"
  },
  {
    "objectID": "slides/week1-day2.html#section-11",
    "href": "slides/week1-day2.html#section-11",
    "title": "Working with Data",
    "section": "",
    "text": "Lab Warm-up"
  },
  {
    "objectID": "slides/week1-day2.html#section-12",
    "href": "slides/week1-day2.html#section-12",
    "title": "Working with Data",
    "section": "",
    "text": "Data Types in R\n\n\n\nglimpse(births_small)\n\nRows: 1,000\nColumns: 10\n$ fage           &lt;int&gt; 34, 36, 37, NA, 32, 32, 37, 29, 30, 29, 30, 34, 28, 28,…\n$ mage           &lt;dbl&gt; 34, 31, 36, 16, 31, 26, 36, 24, 32, 26, 34, 27, 22, 31,…\n$ weeks          &lt;dbl&gt; 37, 41, 37, 38, 36, 39, 36, 40, 39, 39, 42, 40, 40, 39,…\n$ premie         &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"pr…\n$ gained         &lt;dbl&gt; 28, 41, 28, 29, 48, 45, 20, 65, 25, 22, 40, 30, 31, NA,…\n$ weight         &lt;dbl&gt; 6.96, 8.86, 7.51, 6.19, 6.75, 6.69, 6.13, 6.74, 8.94, 9…\n$ lowbirthweight &lt;chr&gt; \"not low\", \"not low\", \"not low\", \"not low\", \"not low\", …\n$ sex            &lt;fct&gt; male, female, female, male, female, female, female, mal…\n$ habit          &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"no…\n$ whitemom       &lt;chr&gt; \"white\", \"white\", \"not white\", \"white\", \"white\", \"white…\n\n\n\n\n\nWhat do you think dbl means?\nHow is that different from int?\n\n\n\nWhat does chr mean?\nHow might it differ from fct?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section",
    "href": "slides/week1-day1-alt.html#section",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "About Me…"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-1",
    "href": "slides/week1-day1-alt.html#section-1",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What is Statistics?\n\n\n\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data.\n\n\nStatistics is the study of how best to collect, analyze, and draw conclusions from data.\n\n\nIntroduction to Modern Statistics"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-2",
    "href": "slides/week1-day1-alt.html#section-2",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What Statistics Is To Me\n\n \n\n\n\n\nThe data science cycle – Wickham & Grolemund"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-3",
    "href": "slides/week1-day1-alt.html#section-3",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What you can expect in STAT 313\n\nThis course will teach you the fundamentals of linear models—simple linear regression, multiple linear regression, and analysis of variance—and experimental design. You will extend the concepts covered in your Stat I course, to:\n\n\nwork with data in a reproducible way (using R)\nvisualize and summarize a variety of datasets (in R)\ncritically evaluate the use of Statistics\nperform statistical analyses to answer research questions (using R)"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-4",
    "href": "slides/week1-day1-alt.html#section-4",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Coding 🙀\n\n\nCoding is a huge part of how doing statistics in the wild looks.\n\n\n\nEveryone is coming from a different background\nDifferent aspects of the course will be difficult to different people\nYou will be given coding resources each week\nUse your peers to support your learning"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-5",
    "href": "slides/week1-day1-alt.html#section-5",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Course Components\n\n\n\n\nBefore Class\n\n\nReading Guides\nConcept Quizzes\nR Tutorials\n\n\n\n\n\nDuring Class\n\n\nGroup Discussion\nHands-on Activities\nLab Assignments\n\n\n\n\n\nOutside of Class\n\n\nStatistical Critiques\nMidterm Project\nFinal Project"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-6",
    "href": "slides/week1-day1-alt.html#section-6",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Specifications Based Grading\n\n\n\n\nEveryone is capable of earning an A!"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-7",
    "href": "slides/week1-day1-alt.html#section-7",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "How Smart are You?\n\n\n\n(2 minutes)\n\nWrite two criteria would you use to rank yourself compared to everyone else in this class\n\n\n\n\n(3 minutes)\n\nTalk with the person on your right about the criteria you proposed\n\n\n\n\n(5 minutes)\n\nShare out\nPerson with most vowels in name should be prepared to share!"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-8",
    "href": "slides/week1-day1-alt.html#section-8",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Cooperative Learning\n\nis a structured form of small-group learning\n\n\n\n\nRoles relate to how the work should be done\n\nRoles are not about breaking up the work intellectually\n\nRoles allow each person to contribute to the group in significant ways\n\nEach person’s participation is necessary to complete the task"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-9",
    "href": "slides/week1-day1-alt.html#section-9",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Group Norms\n\n\n\n\nZero tolerance for: racism, sexism, homophobia, transphobia, ageism, ableism\nRespect one another\nIntent and impact both matter\n\n\n\n\n\nNon-judgmental\nTake space, make space\nEmbrace discomfort\nMake decisions by consensus"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-10",
    "href": "slides/week1-day1-alt.html#section-10",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "How We Learn Together\n\n\n\n\n\nNo one is done until everyone is done\nYou have the right to ask anyone in your group for help\nYou have the duty to help anyone in your group who asks for help\n\n\n\n\n\n\n\nHelping someone means explaining your thinking not giving answers or doing the work for them\nProvide a justification when you make a statement\nThink and work together – don’t divide up the work"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-11",
    "href": "slides/week1-day1-alt.html#section-11",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Break"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-13",
    "href": "slides/week1-day1-alt.html#section-13",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Tidy Data\n\n\nExpected layout of “tidy” datasets"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-14",
    "href": "slides/week1-day1-alt.html#section-14",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Gender stereotypes in 5-7 year old children\n\n\n\n\n\n\n\n\nsubject\nsex\nage\ntrait\ntarget\nstereotype\nhigh_achieve_caution\n\n\n\n\n140\nmale\n5\nnice\nchildren\n0.50\n0.25\n\n\n95\nfemale\n5\nsmart\nadults\n0.75\n1.00\n\n\n61\nfemale\n6\nnice\nadults\n1.00\n0.75\n\n\n144\nmale\n5\nsmart\nadults\n0.75\n0.50\n\n\n39\nfemale\n7\nsmart\nchildren\n0.50\n0.75\n\n\n124\nmale\n5\nnice\nadults\n0.50\n0.75"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-15",
    "href": "slides/week1-day1-alt.html#section-15",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Body girth and skeletal diameter measurements\n\n\n\n\n\n\n\nage\nwgt\nhgt\nsex\nsho_gi\nwai_gi\nnav_gi\nhip_gi\n\n\n\n\n40\n85.5\n180.3\n1\n111.7\n88.8\n90.8\n101.3\n\n\n36\n54.5\n167.6\n0\n96.4\n73.6\n86.9\n94.7\n\n\n23\n86.2\n174.0\n1\n126.3\n81.6\n86.5\n100.9\n\n\n28\n59.2\n162.1\n0\n101.5\n70.5\n78.5\n91.9\n\n\n26\n74.6\n176.0\n1\n113.0\n85.6\n89.2\n98.0\n\n\n24\n59.4\n162.9\n0\n107.1\n71.2\n85.7\n97.5"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-16",
    "href": "slides/week1-day1-alt.html#section-16",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "NBA player of the week\n\n\n\n\n\n\n\n\nAge\nDate\nDraft Year\nHeight\nPlayer\nPosition\n\n\n\n\n28\nMar 12, 2006\n1998\n6-7\nPaul Pierce\nSF\n\n\n25\nMar 24, 2002\n1997\n6-11\nTim Duncan\nFC\n\n\n28\nFeb 9, 2009\n2001\n7-0\nPau Gasol\nFC\n\n\n20\nFeb 6, 2006\n2005\n6-0\nChris Paul\nPG\n\n\n28\nMar 8, 2010\n2003\n6-4\nDwyane Wade\nSG\n\n\n25\nApr 25, 1999\n1993\n6-9\nChris Webber\nF-C"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-17",
    "href": "slides/week1-day1-alt.html#section-17",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Your Turn\n\n\nEvery year, the US releases to the public a large data set containing information on births recorded in the country.\nA total of 13 variables were collected on every birth, including information about:\n\nthe birth (baby weight, sex of baby, premie status)\nthe pregnancy (hospital visits, length of gestation, )\nthe birth parent’s attributes (age, smoking status, marital status, race)\nthe partner’s age\n\n\n\n\nHow would you expect this dataframe to look?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-18",
    "href": "slides/week1-day1-alt.html#section-18",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Military Spending\n\n\n\n\n\n\n\nCountry\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\nAfrica\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nUSA\n4.922642\n4.840174\n4.477401\n4.046679\n3.695891\n3.477846\n3.418941\n3.313385\n3.316244\n3.413107\n\n\nAustralia\n1.856791\n1.757078\n1.670963\n1.639861\n1.772211\n1.950601\n2.081512\n1.997974\n1.894180\n1.879802\n\n\nNorway\n1.515698\n1.451436\n1.402134\n1.413997\n1.472039\n1.507280\n1.626681\n1.622270\n1.628300\n1.684120\n\n\nSweden\n1.188288\n1.104288\n1.133304\n1.116715\n1.129776\n1.069579\n1.052833\n1.024075\n1.031100\n1.120599\n\n\n\n\n\n\n\n\n\n\n\nDo these data satisfy the “tidy” principles?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-19",
    "href": "slides/week1-day1-alt.html#section-19",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Vehicle Efficiency\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\nToyota Corona\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n\nDo these data satisfy the “tidy” principles?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-20",
    "href": "slides/week1-day1-alt.html#section-20",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-21",
    "href": "slides/week1-day1-alt.html#section-21",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Types of Variables\n\n\n\nDiagram of types of variables we will analyze!"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-24",
    "href": "slides/week1-day1-alt.html#section-24",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Your Turn (90-seconds)\n\n\nWrite down one example of:\n\na continuous, numerical variable\na discrete, numerical variable\nan ordinal, categorical variable\na regular, categorical variable\n\n\n\nShare out!"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-26",
    "href": "slides/week1-day1-alt.html#section-26",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Data Types in R\n\n\n\nglimpse(births_small)\n\nRows: 1,000\nColumns: 10\n$ fage           &lt;int&gt; 34, 36, 37, NA, 32, 32, 37, 29, 30, 29, 30, 34, 28, 28,…\n$ mage           &lt;dbl&gt; 34, 31, 36, 16, 31, 26, 36, 24, 32, 26, 34, 27, 22, 31,…\n$ weeks          &lt;dbl&gt; 37, 41, 37, 38, 36, 39, 36, 40, 39, 39, 42, 40, 40, 39,…\n$ premie         &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"pr…\n$ gained         &lt;dbl&gt; 28, 41, 28, 29, 48, 45, 20, 65, 25, 22, 40, 30, 31, NA,…\n$ weight         &lt;dbl&gt; 6.96, 8.86, 7.51, 6.19, 6.75, 6.69, 6.13, 6.74, 8.94, 9…\n$ lowbirthweight &lt;chr&gt; \"not low\", \"not low\", \"not low\", \"not low\", \"not low\", …\n$ sex            &lt;fct&gt; male, female, female, male, female, female, female, mal…\n$ habit          &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"no…\n$ whitemom       &lt;chr&gt; \"white\", \"white\", \"not white\", \"white\", \"white\", \"white…\n\n\n\n\n\nWhat do you think dbl means?\nHow is that different from int?\n\n\n\nWhat does chr mean?\nHow might it differ from fct?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-27",
    "href": "slides/week1-day1-alt.html#section-27",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Types of Studies\n\n\n\nExperiment\n\nrandomization\nreplication\ncontrolling\nblocking\n\n\n\n\nObservational Study\n\ncollect data in a way that does not directly interfere with how the data arise"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-28",
    "href": "slides/week1-day1-alt.html#section-28",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Relationships Between Variables\n\n\nexplanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable\n\n\nIf two variables are not associated, then they are said to be independent.\nIf two variables are associated, then they are said to be dependent."
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-29",
    "href": "slides/week1-day1-alt.html#section-29",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Causal Inference\n\n\nassociation \\(\\neq\\) causation\n\n\nWhat do you need to say that the explanatory variable causes a change in the response variable?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#lab-1",
    "href": "slides/week1-day1-alt.html#lab-1",
    "title": "Welcome to Stat 313!",
    "section": "Lab 1",
    "text": "Lab 1\n\nAccess Posit Cloud from link posted on Canvas\n\nCreate an Account\nJoin the STAT 313 / 513 workspace\nAccess the Lab 1 Project\nOpen the Project\n\nA video will be posted guiding you on how to work with Quarto\n\nAsk questions on Discord\nEmail / text / chat with your group members"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-30",
    "href": "slides/week1-day1-alt.html#section-30",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Find Your Group!"
  },
  {
    "objectID": "slides/week4-day2.html#section",
    "href": "slides/week4-day2.html#section",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "Least Squares\n\n\n\nPublished in 1805 by Legendre\n\n\n\n\n\n\n\n\n\n\n\n\nand Gauss in 1809\n\n\n\n\n\n\n\n\n\n\n\n\nUsed to determine, from astronomical observations, the orbits of bodies about the Sun."
  },
  {
    "objectID": "slides/week4-day2.html#section-1",
    "href": "slides/week4-day2.html#section-1",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "“regression”\n\n\n\n\n\n\n\n\nCoined by Francis Galton in the 19th century\nDescribed a biological phenomenon\n\nHeights of children of tall parents tend to be tall, but shorter than their parents"
  },
  {
    "objectID": "slides/week4-day2.html#section-2",
    "href": "slides/week4-day2.html#section-2",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "A “polymath”\n\n\n\n\n\n\n\n\nIn Statistics, Galton (1822–1911) is a towering figure.\nHe invented standard deviation, correlation, linear regression, ANOVA\nGalton’s developments and discoveries were fueled in large part by his fascination with the science of heredity."
  },
  {
    "objectID": "slides/week4-day2.html#section-3",
    "href": "slides/week4-day2.html#section-3",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "The Invention of Eugenics\n\n\n\n\nThe science of heredity could help humanity better itself through breeding.\nBased on Greek eugenes, meaning “well-born”\nGalton served as founding president of the British Eugenics Society\n\n\n\n\n\n“What nature does blindly, slowly and ruthlessly, man may do providently, quickly, and kindly. As it lies within his power, so it becomes his duty to work in that direction.”"
  },
  {
    "objectID": "slides/week4-day2.html#section-5",
    "href": "slides/week4-day2.html#section-5",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "Eugenics and the US\n\n\n\n\nIn 1896 Connecticut made it illegal for people with epilepsy or who were “feeble-minded” to marry.\nJohn Kellogg established a “pedigree registry”\nFrom 1909 to 1979 California sterilized nearly 20,000 residents of the state mental institutions\nIn 1927 the US Supreme Court ruled that sterilization of the handicapped did not violate the Constitution — “…three generations of imbeciles are enough.”\nBetween 1970 and 1976 between 25 and 50% of Native Americans were sterilized, many without consent"
  },
  {
    "objectID": "slides/week4-day2.html#section-6",
    "href": "slides/week4-day2.html#section-6",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "And then it spread…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMein Kampf references American eugenics\nDeclared non-Aryan races inferior\nBelieved Germans should do everything possible to make sure their gene pool stayed “pure”"
  },
  {
    "objectID": "slides/week4-day2.html#section-8",
    "href": "slides/week4-day2.html#section-8",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "Would you exist?\n\n\n\n\nIs your skin white?\nAre you blonde?\nDo you have blue eyes?\n\n\n\nWere your ancestors poor?\nAre you Muslim, Hindu, Buddhist, Sikh, Tao, or Jewish?\nAre you LGBTQIQ+?"
  },
  {
    "objectID": "slides/week4-day2.html#section-9",
    "href": "slides/week4-day2.html#section-9",
    "title": "The Ugly History of Linear Regression",
    "section": "",
    "text": "More Information\n\n\nRadiolab Presents: G\n\n“G” is a multi-episode exploration of one of the most dangerous ideas of the past century: the concept of intelligence.\nhttps://www.wnycstudios.org/podcasts/radiolab/projects/radiolab-presents-g\n\nHow Eugenics shaped Statistics\n\nhttps://nautil.us/issue/92/frontiers/how-eugenics-shaped-statistics"
  },
  {
    "objectID": "slides/week5-day1.html#section",
    "href": "slides/week5-day1.html#section",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Before…"
  },
  {
    "objectID": "slides/week5-day1.html#section-1",
    "href": "slides/week5-day1.html#section-1",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Now…"
  },
  {
    "objectID": "slides/week5-day1.html#section-2",
    "href": "slides/week5-day1.html#section-2",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "How?"
  },
  {
    "objectID": "slides/week5-day1.html#section-3",
    "href": "slides/week5-day1.html#section-3",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Offsets!\n\n\n\n\nsmoke_lm &lt;- lm(weight ~ weeks * habit, data = ncbirths)\n\nget_regression_table(smoke_lm)\n\n\n\n# A tibble: 4 × 3\n  term              estimate std_error\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept           -5.94      0.484\n2 weeks                0.341     0.013\n3 habit: smoker       -1.86      1.63 \n4 weeks:habitsmoker    0.039     0.042\n\n\n\n\n\nThe * means the variables are interacting!\n\n\n\n\n\nWhat is the regression equation for non-smoker mothers?\n\n\n\n\nWhat is the regression equation for smoker mothers?"
  },
  {
    "objectID": "slides/week5-day1.html#section-4",
    "href": "slides/week5-day1.html#section-4",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "What if we have a second numerical explanatory variable?"
  },
  {
    "objectID": "slides/week5-day1.html#section-6",
    "href": "slides/week5-day1.html#section-6",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Multiple slopes\n\n\nage_lm &lt;- lm(weight ~ weeks + mage, data = ncbirths)\n\nget_regression_table(age_lm)\n\n\n\n# A tibble: 3 × 3\n  term      estimate std_error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept   -6.68      0.492\n2 weeks        0.346     0.012\n3 mage         0.02      0.006\n\n\n\n\n\nHow do you interpret the value of 0.346?\n\n\n\n\nHow do you interpret the value of 0.02?"
  },
  {
    "objectID": "slides/week5-day1.html#section-7",
    "href": "slides/week5-day1.html#section-7",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "But how do we decide if the interaction model is “best” without a p-value??????"
  },
  {
    "objectID": "slides/week5-day1.html#section-8",
    "href": "slides/week5-day1.html#section-8",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "When investigating if a relationship differs…\n\n\n\nAlways start with the “interaction” / different slopes model.\n\n\n\n\nIf the slopes look different, you’re done!\n\n\n\n\nIf the slopes look similar, then fit the “additive” / parallel slopes model."
  },
  {
    "objectID": "slides/week5-day1.html#section-9",
    "href": "slides/week5-day1.html#section-9",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Different Enough?"
  },
  {
    "objectID": "slides/week5-day1.html#section-10",
    "href": "slides/week5-day1.html#section-10",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Behind the Plot\n\n\nggplot(data = MA_schools, \n       mapping = aes(y = average_sat_math, \n                       x = perc_disadvan, \n                       color = size)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"Percent Economically Disadvantaged\", \n       y = \"Average SAT Math\", \n       color = \"Size of School\")\n\n\ngeom_smooth() allows for both the intercepts and the slopes to differ"
  },
  {
    "objectID": "slides/week5-day1.html#section-11",
    "href": "slides/week5-day1.html#section-11",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "What about now?\n\n\n\n# A tibble: 6 × 3\n  term                     estimate std_error\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept                 594.       13.3  \n2 perc_disadvan              -2.93      0.294\n3 size: medium              -17.8      15.8  \n4 size: large               -13.3      13.8  \n5 perc_disadvan:sizemedium    0.146     0.371\n6 perc_disadvan:sizelarge     0.189     0.323\n\n\n\n\n🤨"
  },
  {
    "objectID": "slides/week5-day1.html#section-12",
    "href": "slides/week5-day1.html#section-12",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Who is baseline?"
  },
  {
    "objectID": "slides/week5-day1.html#section-13",
    "href": "slides/week5-day1.html#section-13",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Deciphering groups – Small schools\n\n\n\n\\[\\widehat{SAT}_{small} = 594 - 2.93 \\times \\text{percent disadvan}\\]"
  },
  {
    "objectID": "slides/week5-day1.html#section-14",
    "href": "slides/week5-day1.html#section-14",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Deciphering groups – Medium schools\n\n\n\n\\[\\widehat{SAT}_{medium} = (594 - 17.8) + (- 2.93 + 0.146) \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{medium} = 576.2 - 2.784 \\times \\text{percent disadvan}\\]"
  },
  {
    "objectID": "slides/week5-day1.html#section-15",
    "href": "slides/week5-day1.html#section-15",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Deciphering groups – Large schools\n\n\n\n\\[\\widehat{SAT}_{large} = (594 - 13.3) + (- 2.93 + 0.189) \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{large} = 580.7 - 2.741 \\times \\text{percent disadvan}\\]"
  },
  {
    "objectID": "slides/week5-day1.html#section-16",
    "href": "slides/week5-day1.html#section-16",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "What if they’re not very different?"
  },
  {
    "objectID": "slides/week5-day1.html#section-17",
    "href": "slides/week5-day1.html#section-17",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Parallel Slopes\n\n\nlm(average_sat_math ~ perc_disadvan + size, data = MA_schools)\n\n\n\n\n# A tibble: 4 × 3\n  term          estimate std_error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept       588.       7.61 \n2 perc_disadvan    -2.78     0.106\n3 size: medium    -11.9      7.54 \n4 size: large      -6.36     6.92"
  },
  {
    "objectID": "slides/week5-day1.html#section-18",
    "href": "slides/week5-day1.html#section-18",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Group equations – Baseline\n\n\n\n# A tibble: 4 × 3\n  term          estimate std_error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept       588.       7.61 \n2 perc_disadvan    -2.78     0.106\n3 size: medium    -11.9      7.54 \n4 size: large      -6.36     6.92 \n\n\n\n\\[\\widehat{SAT}_{small} = 588 - 2.78 \\times \\text{percent disadvantaged}\\]"
  },
  {
    "objectID": "slides/week5-day1.html#section-19",
    "href": "slides/week5-day1.html#section-19",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Group equations – Offsets\n\n\n\n# A tibble: 4 × 3\n  term          estimate std_error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept       588.       7.61 \n2 perc_disadvan    -2.78     0.106\n3 size: medium    -11.9      7.54 \n4 size: large      -6.36     6.92 \n\n\n\n\\[\\widehat{SAT}_{medium} = (588 - 11.9) - 2.78 \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{medium} = 576.1 - 2.78 \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{large} = (588 - 6.36) - 2.78 \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{large} = 581.64 - 2.78 \\times \\text{percent disadvan}\\]"
  },
  {
    "objectID": "slides/week7-day2-why-bootstrap.html#section",
    "href": "slides/week7-day2-why-bootstrap.html#section",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Approximate the variability you’d expect to see in other samples!\n\n\n\n\nBootstrapping!"
  },
  {
    "objectID": "slides/week7-day2-why-bootstrap.html#section-1",
    "href": "slides/week7-day2-why-bootstrap.html#section-1",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "A Bootstrap Resample\n\n\n\nAssumes the original sample is “representative” of observations in the population.\n\n\n\n\nUses the original sample to generate new samples that might have occurred with additional sampling.\n\n\n\n\n\nWe can use the statistics from these bootstrap samples to approximate the true sampling distribution!"
  },
  {
    "objectID": "slides/week7-day2-why-bootstrap.html#section-2",
    "href": "slides/week7-day2-why-bootstrap.html#section-2",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Why???"
  },
  {
    "objectID": "slides/week7-day2-why-bootstrap.html#section-3",
    "href": "slides/week7-day2-why-bootstrap.html#section-3",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Estimating a population parameter\n\n\n\nWe are interested in knowing how a statistic varies from sample to sample.\n\n\n\n\nKnowing a statistic’s behavior helps us make better / more informed decisions!\n\n\n\n\nThis helps us estimate what values are more or less likely for the population parameter to have."
  },
  {
    "objectID": "slides/week7-day2-why-bootstrap.html#section-4",
    "href": "slides/week7-day2-why-bootstrap.html#section-4",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Confidence Intervals\n\n\n\nCapture a range of plausible values for the population parameter.\n\n\n\n\n\nAre more likely to capture the population parameter than a point estimate."
  },
  {
    "objectID": "slides/week2-day2.html#section",
    "href": "slides/week2-day2.html#section",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Suppose…\n\n\n“Overall this instructor was educationally effective.”\n\n\n\n\n\n\n\nyear\nquarter\naverage\n\n\n\n\n2021\nFall\n4.53\n\n\n2021\nFall\n4.36\n\n\n2022\nWinter\n4.18\n\n\n2022\nWinter\n4.24\n\n\n2022\nSpring\n4.83\n\n\n2022\nSpring\n4.41\n\n\n2022\nSpring\n4.00"
  },
  {
    "objectID": "slides/week2-day2.html#section-1",
    "href": "slides/week2-day2.html#section-1",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "How were these averages calculated?"
  },
  {
    "objectID": "slides/week2-day2.html#section-2",
    "href": "slides/week2-day2.html#section-2",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "What do these averages mean?"
  },
  {
    "objectID": "slides/week2-day2.html#section-3",
    "href": "slides/week2-day2.html#section-3",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "The Problem\n\n\nIt’s incredibly rare for scientists, including statisticians, to explicitly think about that conditions underlying their models."
  },
  {
    "objectID": "slides/week2-day2.html#section-4",
    "href": "slides/week2-day2.html#section-4",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Why so much resistance?\n\n\nDepartments hold specific expectations of statistics courses\n\nThese expectations are conditional on the assumption that means represent the magic quantity of interest\n\nI’m then expected to educate you to “play the game” in the scientific culture of averages"
  },
  {
    "objectID": "slides/week2-day2.html#section-5",
    "href": "slides/week2-day2.html#section-5",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Averagarianism\n\n\n\n“The primary research method of averagarianism is aggregate, then analyze: First, combine many people together and look for patterns in the group. Then, use these group patterns (such as averages and other statistics) to analyze and model individuals. The science of the individual instead instructs scientists to analyze, then aggregate: First, look for pattern within each individual. Then, look for ways to combine these individual patterns into collective insight.”\nThe End of Average by Todd Rose"
  },
  {
    "objectID": "slides/week2-day2.html#section-6",
    "href": "slides/week2-day2.html#section-6",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "What else then?\n\n\n\n\n\n\n\n60-second exercise\n\n\nIf you could not use averages to evaluate, model, and select individuals, well then…what could you use?\n\n\n\n\n\nThe difficulty in responding to this question underscores how averagarianism has endured for so long and become so deeply ingrained throughout society."
  },
  {
    "objectID": "slides/week2-day2.html#section-7",
    "href": "slides/week2-day2.html#section-7",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "“We’ve always done it this way”\n\nMethods based on averages are available, easy, convenient, and take little creativity — and they are expected in our scientific culture.\n\nJustification for using averages is simply not demanded — though justification for use of anything but averages is incredibly difficult to sell."
  },
  {
    "objectID": "slides/week2-day2.html#section-8",
    "href": "slides/week2-day2.html#section-8",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Some Rules to Play By\n\n\n\n\n\nLook at and understand your raw data before aggregating\n\n\n\n\n\n\nBoxplots (and such) don’t count as visualizing the raw data\n\n\n\n\n\n\nWe should only average things we are convinced are measuring the same thing"
  },
  {
    "objectID": "slides/week9-day1.html#section",
    "href": "slides/week9-day1.html#section",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Now…"
  },
  {
    "objectID": "slides/week9-day1.html#section-1",
    "href": "slides/week9-day1.html#section-1",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Goal of an ANOVA\n\nAnalysis of variance (ANOVA) compares the means of three of more groups to detect if the means of the groups are different."
  },
  {
    "objectID": "slides/week9-day1.html#section-2",
    "href": "slides/week9-day1.html#section-2",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "How???\n\n\nCompare how different a group of means are\nScale the differences relative to the variability of the groups\nSummarize the differences with one number"
  },
  {
    "objectID": "slides/week9-day1.html#section-3",
    "href": "slides/week9-day1.html#section-3",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Visualizing Group Differences\n\n\nWe want visualizations that allow for us to easily compare:\n\nthe center (mean) of the groups\nthe spread (variability) of the groups"
  },
  {
    "objectID": "slides/week9-day1.html#section-4",
    "href": "slides/week9-day1.html#section-4",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "What can you say about the differences between the groups?\nWhat can you say about the variability within the groups?"
  },
  {
    "objectID": "slides/week9-day1.html#section-5",
    "href": "slides/week9-day1.html#section-5",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Carrying out an ANOVA"
  },
  {
    "objectID": "slides/week9-day1.html#section-6",
    "href": "slides/week9-day1.html#section-6",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 1: Compare your groups"
  },
  {
    "objectID": "slides/week9-day1.html#section-7",
    "href": "slides/week9-day1.html#section-7",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 2: Find the overall mean\n\n\n\nThis ignores the groups and finds one mean for every observation!"
  },
  {
    "objectID": "slides/week9-day1.html#section-8",
    "href": "slides/week9-day1.html#section-8",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 3: Find the group means"
  },
  {
    "objectID": "slides/week9-day1.html#section-9",
    "href": "slides/week9-day1.html#section-9",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 4: Calculate the sum of squares"
  },
  {
    "objectID": "slides/week9-day1.html#section-10",
    "href": "slides/week9-day1.html#section-10",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 5: Calculate the F-statistic\n\n\n\n\n\nCan an F-statistic be negative?"
  },
  {
    "objectID": "slides/week9-day1.html#section-11",
    "href": "slides/week9-day1.html#section-11",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 6: Find the p-value"
  },
  {
    "objectID": "slides/week9-day1.html#section-12",
    "href": "slides/week9-day1.html#section-12",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "F-distribution\n\nAn \\(F\\)-distribution is a variant of the \\(t\\)-distribution, and is also defined by degrees of freedom.\n\n\nThis distribution is defined by two different degrees of freedom:\n\nfrom the numerator (MSG) : \\(k - 1\\)\nfrom the denominator (MSE) : \\(n - k\\)"
  },
  {
    "objectID": "slides/week9-day1.html#section-13",
    "href": "slides/week9-day1.html#section-13",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Two degrees of freedom!\n\n\n\n\nChanging the numerator degrees of freedom\n\n\n\n\nChanging the denominator degrees of freedom"
  },
  {
    "objectID": "slides/week9-day1.html#section-14",
    "href": "slides/week9-day1.html#section-14",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Do you always use an F-distribution to get the p-value?\n\n\n\n\nNO!"
  },
  {
    "objectID": "slides/week9-day1.html#section-15",
    "href": "slides/week9-day1.html#section-15",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Conditions of an ANOVA\n\n\nIndependence\n\nWithin groups\nBetween groups\n\nNormality of the responses\n\nThe distribution of each group is approximately normal\n\nEqual variability of the groups\n\nThe spread of the distributions are similar across groups"
  },
  {
    "objectID": "slides/week9-day1.html#section-16",
    "href": "slides/week9-day1.html#section-16",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "What do you think?\n\n\n\n\nIf the normality condition is violated what type of method should we use?"
  },
  {
    "objectID": "slides/week9-day1.html#section-17",
    "href": "slides/week9-day1.html#section-17",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Simulation-based Methods"
  },
  {
    "objectID": "slides/week9-day1.html#section-18",
    "href": "slides/week9-day1.html#section-18",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 1: Calculating the Observed F-statistic\n\n\nobs_F &lt;- evals_small %&gt;% \n  specify(response = min_eval, \n          explanatory = age_cat) %&gt;% \n  calculate(stat = \"F\")\n\n\n\nResponse: min_eval (numeric)\nExplanatory: age_cat (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  1.41"
  },
  {
    "objectID": "slides/week9-day1.html#section-19",
    "href": "slides/week9-day1.html#section-19",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 2: Simulating what could have happened under \\(H_0\\)\n\n\n\n\nHow could we use cards to simulate what minimum evaluation score a professor would have gotten, if their score was independent from their age?"
  },
  {
    "objectID": "slides/week9-day1.html#section-20",
    "href": "slides/week9-day1.html#section-20",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Another Permutation Distribution\n\n\nnull_dist &lt;- evals_small %&gt;% \n  specify(response = min_eval, \n          explanatory = age_cat) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 2000, type = \"permute\") %&gt;% \n  calculate(stat = \"F\")"
  },
  {
    "objectID": "slides/week9-day1.html#section-21",
    "href": "slides/week9-day1.html#section-21",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Another Permutation Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nWhy doesn’t the distribution have negative numbers?"
  },
  {
    "objectID": "slides/week9-day1.html#section-22",
    "href": "slides/week9-day1.html#section-22",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Visualizing the p-value\n\n\nvisualise(null_dist) + \n  shade_p_value(obs_stat = obs_F, direction = \"greater\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat would you conclude regarding the mean minimum evaluation score and different age groups of professors?"
  },
  {
    "objectID": "slides/week9-day1.html#section-23",
    "href": "slides/week9-day1.html#section-23",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Theory-based Methods"
  },
  {
    "objectID": "slides/week9-day1.html#section-24",
    "href": "slides/week9-day1.html#section-24",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Using aov()\n\naov(min_eval ~ age_cat, \n    data = evals_small)  %&gt;% \n  tidy()\n\n\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 age_cat       3  1.24  0.414      1.41   0.244\n2 Residuals    90 26.4   0.293     NA     NA    \n\n\n\n\n\nHow was the statistic calculated?\n\n\n\n\nWhat distribution was used to calculate the p.value?\n\n\n\n\n\nWhat would you conclude regarding the mean minimum evaluation score and different age groups of professors?"
  },
  {
    "objectID": "slides/week9-day1.html#section-25",
    "href": "slides/week9-day1.html#section-25",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Did the two methods yield different results?"
  },
  {
    "objectID": "slides/week9-day1.html#section-26",
    "href": "slides/week9-day1.html#section-26",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Next week…"
  },
  {
    "objectID": "slides/week3-day1.html#section",
    "href": "slides/week3-day1.html#section",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "How would you describe a categorical variable?"
  },
  {
    "objectID": "slides/week3-day1.html#in-r",
    "href": "slides/week3-day1.html#in-r",
    "title": "Incorporating Categorical Variables",
    "section": "In R…",
    "text": "In R…\ncategorical variables can have either character or factor data types\n\n\nfactor – structured & fixed number of levels / options\n\ncan be ordered or unordered\n\n\n\n\ncharacter – unstructured & variable number of levels\n\nis inherently unordered"
  },
  {
    "objectID": "slides/week3-day1.html#section-1",
    "href": "slides/week3-day1.html#section-1",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "Fill in the associated data types (e.g. character, factor, integer, double) with each type of variable.\n\n\n\n\nVariable\nData Type in R\n\n\n\n\nCategorical variable\n\n\n\nContinuous numerical variable\n\n\n\nDiscrete numerical variable"
  },
  {
    "objectID": "slides/week3-day1.html#dplyr-a-tool-bag-for-data-wrangling",
    "href": "slides/week3-day1.html#dplyr-a-tool-bag-for-data-wrangling",
    "title": "Incorporating Categorical Variables",
    "section": "dplyr – a tool bag for data wrangling",
    "text": "dplyr – a tool bag for data wrangling\n\n\n\n\n\n\nfilter()\nselect()\nmutate()\nsummarize()\narrange()\ngroup_by()"
  },
  {
    "objectID": "slides/week3-day1.html#section-2",
    "href": "slides/week3-day1.html#section-2",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "Brainstorm definitions for each verb\n\n\n\nfilter()\nselect()\nmutate()\ngroup_by()\nsummarize()\narrange()"
  },
  {
    "objectID": "slides/week3-day1.html#section-3",
    "href": "slides/week3-day1.html#section-3",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "The Pipe %&gt;%"
  },
  {
    "objectID": "slides/week3-day1.html#section-4",
    "href": "slides/week3-day1.html#section-4",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "If you wanted means for each level of a categorical variable, what would you do?"
  },
  {
    "objectID": "slides/week3-day1.html#trout-size",
    "href": "slides/week3-day1.html#trout-size",
    "title": "Incorporating Categorical Variables",
    "section": "Trout Size",
    "text": "Trout Size\n\nThe HJ Andrews Experimental Forest houses one of the larges long-term ecological research stations, specifically researching cutthroat trout and salamanders in clear cut or old growth sections of Mack Creek.\n\n\n\ntrout %&gt;% \n  group_by(section) %&gt;% \n  summarize(mean_length = mean(length_1_mm, na.rm = TRUE)\n            )\n\n\n\n# A tibble: 2 × 2\n  section                               mean_length\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 clear cut forest                             85.3\n2 upstream old growth coniferous forest        81.4\n\n\n\n\n\n\nWhy na.rm = TRUE?"
  },
  {
    "objectID": "slides/week3-day1.html#classifying-channel-types",
    "href": "slides/week3-day1.html#classifying-channel-types",
    "title": "Incorporating Categorical Variables",
    "section": "Classifying Channel Types",
    "text": "Classifying Channel Types\n\nThe channels of the Mack Creek which were sampled were classified into the following groups:\n\n\n\n\"C\"\n\"I\"\n\"IP\"\n\"P\"\n\"R\"\n\"S\"\n\"SC\"\nNA\n\n\n\ncascade\nriffle\nisolated pool\npool\nrapid\nstep (small falls)\nside channel\nnot sampled by unit"
  },
  {
    "objectID": "slides/week3-day1.html#filter-ing-specific-channel-types",
    "href": "slides/week3-day1.html#filter-ing-specific-channel-types",
    "title": "Incorporating Categorical Variables",
    "section": "filter()-ing Specific Channel Types",
    "text": "filter()-ing Specific Channel Types\n\nThe majority of the Cutthroat trout were captured in cascades (C), pools (P), and side channels (SC). Suppose we want to only retain these levels of the unittype variable.\n\n\n\n\nWhat would you do?\n\n\n\n\n\ntrout %&gt;% \n  filter(unittype %in% c(\"C\", \"P\", \"SC\"))\n\n\n\n\n\nWhy use %in% instead of ==?"
  },
  {
    "objectID": "slides/week3-day1.html#incorporating-categorical-variables-into-data-visualizations",
    "href": "slides/week3-day1.html#incorporating-categorical-variables-into-data-visualizations",
    "title": "Incorporating Categorical Variables",
    "section": "Incorporating Categorical Variables into Data Visualizations",
    "text": "Incorporating Categorical Variables into Data Visualizations\n\n\nAs a variable on the x- or y-axis\nAs a color / fill\nAs a facet"
  },
  {
    "objectID": "slides/week3-day1.html#salamander-size",
    "href": "slides/week3-day1.html#salamander-size",
    "title": "Incorporating Categorical Variables",
    "section": "Salamander Size",
    "text": "Salamander Size\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_2_mm)) + \n  geom_histogram(binwidth = 14) + \n  labs(x = \"Snout to Tail Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would this histogram look if there was no variation in salamander length?\n\nWhat are possible causes for the variation in salamander length?"
  },
  {
    "objectID": "slides/week3-day1.html#faceted-histograms",
    "href": "slides/week3-day1.html#faceted-histograms",
    "title": "Incorporating Categorical Variables",
    "section": "Faceted Histograms",
    "text": "Faceted Histograms\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm)) + \n  geom_histogram(binwidth = 14) + \n  facet_wrap(~ section, scales = \"free\") +\n  labs(x = \"Snout to Tail Length (mm)\")"
  },
  {
    "objectID": "slides/week3-day1.html#side-by-side-boxplots",
    "href": "slides/week3-day1.html#side-by-side-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                     y = species)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\") \n\n\n\n\n\n\n\n\n\n\nggplot(data = salamander, \n       mapping = aes(y = length_1_mm, \n                     x = species)\n         ) + \n  geom_boxplot() + \n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Salamander Species\")"
  },
  {
    "objectID": "slides/week3-day1.html#colors-in-boxplots",
    "href": "slides/week3-day1.html#colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Colors in Boxplots",
    "text": "Colors in Boxplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = unittype)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\", \n       color = \"Channel Type\")"
  },
  {
    "objectID": "slides/week3-day1.html#facets-colors-in-boxplots",
    "href": "slides/week3-day1.html#facets-colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = section)\n         ) + \n  geom_boxplot() + \n  facet_wrap(~ unittype) + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\", \n       color = \"Section in Mack Creek\")"
  },
  {
    "objectID": "slides/week3-day1.html#facets-colors-in-boxplots-output",
    "href": "slides/week3-day1.html#facets-colors-in-boxplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots"
  },
  {
    "objectID": "slides/week3-day1.html#facets-color-in-scatterplots",
    "href": "slides/week3-day1.html#facets-color-in-scatterplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = weight_g, \n                       color = section)\n         ) + \n  geom_point() + \n  facet_wrap(~species) +\n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Year\", \n       color = \"Salamander Species\")"
  },
  {
    "objectID": "slides/week3-day1.html#facets-color-in-scatterplots-output",
    "href": "slides/week3-day1.html#facets-color-in-scatterplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots"
  },
  {
    "objectID": "slides/week3-day1.html#your-turn",
    "href": "slides/week3-day1.html#your-turn",
    "title": "Incorporating Categorical Variables",
    "section": "Your Turn",
    "text": "Your Turn\n\n\nWhat are the aesthetics included in this plot?\n\n\nWhat is one aspect of this plot that you believe is well done? What is one aspect of the plot that could be improved?"
  },
  {
    "objectID": "slides/week3-day1.html#section-5",
    "href": "slides/week3-day1.html#section-5",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "Break"
  },
  {
    "objectID": "slides/week8-day2.html#section",
    "href": "slides/week8-day2.html#section",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Plan for Week 9\n\n\n\nAsynchronous class on Tuesday and Thursday\nTypical deadlines for reading (Tuesday) and tutorial (Thursday)\n“Checkpoints” for Final Project incorporated throughout the week\n\nIntroduction – Due Wednesday\nMethods – Due Friday\nFindings & Scope of Inference – Due Sunday"
  },
  {
    "objectID": "slides/week8-day2.html#section-1",
    "href": "slides/week8-day2.html#section-1",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Some advice on the your Final Project…"
  },
  {
    "objectID": "slides/week8-day2.html#section-2",
    "href": "slides/week8-day2.html#section-2",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What did we do on Tuesday?"
  },
  {
    "objectID": "slides/week8-day2.html#section-3",
    "href": "slides/week8-day2.html#section-3",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "We carried out a hypothesis test!\n\n\n\n\\[H_0: \\beta_1 = 0\\]\n\\[H_A: \\beta_1 \\neq 0\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do these hypotheses mean in words?"
  },
  {
    "objectID": "slides/week8-day2.html#section-4",
    "href": "slides/week8-day2.html#section-4",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "By creating a permutation distribution!\n\n\nnull_dist &lt;- evals %&gt;% \n  specify(response = score, \n          explanatory = bty_avg) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n\nWhat is happening in the generate() step?"
  },
  {
    "objectID": "slides/week8-day2.html#section-5",
    "href": "slides/week8-day2.html#section-5",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "And visualizing where our observed statistic fell on the distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat would you estimate the p-value to be?"
  },
  {
    "objectID": "slides/week8-day2.html#section-6",
    "href": "slides/week8-day2.html#section-6",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "And calculated the p-value\n\n\n\n\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n\n\n\n  What would you decide for your hypothesis test?"
  },
  {
    "objectID": "slides/week8-day2.html#section-7",
    "href": "slides/week8-day2.html#section-7",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "How would this process have changed if we used theory-based methods instead?"
  },
  {
    "objectID": "slides/week8-day2.html#section-8",
    "href": "slides/week8-day2.html#section-8",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Approximating the permutation distribution\n\n\n\nA \\(t\\)-distribution can be a reasonable approximation for the permutation distribution if certain conditions are not violated."
  },
  {
    "objectID": "slides/week8-day2.html#section-9",
    "href": "slides/week8-day2.html#section-9",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What about the observed statistic?\n\n\nBeforeNow\n\n\n\nobs_slope &lt;- evals %&gt;% \n  specify(response = score, \n          explanatory = bty_avg) %&gt;% \n  calculate(stat = \"slope\")\n\n\n\nResponse: score (numeric)\nExplanatory: bty_avg (numeric)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 0.0666\n\n\n\n\n\nevals_lm &lt;- lm(score ~ bty_avg,\n               data = evals)\n\nget_regression_table(evals_lm)\n\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    3.88      0.076     51.0        0    3.73     4.03 \n2 bty_avg      0.067     0.016      4.09       0    0.035    0.099"
  },
  {
    "objectID": "slides/week8-day2.html#section-10",
    "href": "slides/week8-day2.html#section-10",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "How did R calculate the \\(t\\)-statistic?\n\n\nStep 1: SEStep 2: t-statisticProof!\n\n\n\n\n\\(SE_{b_1} = \\frac{\\frac{s_y}{s_x} \\cdot \\sqrt{1 - r^2}}{\\sqrt{n - 2}}\\)\n\n\n\n\n\n[1] 0.01495204\n\n\n\n\n\n\n\n\n\\(t = \\frac{b_1}{SE_{b_1}}\\)\n\n\n\n\n\nbty_avg \n4.45672 \n\n\n\n\n\n\n\n\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    3.88      0.076     51.0        0    3.73     4.03 \n2 bty_avg      0.067     0.016      4.09       0    0.035    0.099"
  },
  {
    "objectID": "slides/week8-day2.html#section-11",
    "href": "slides/week8-day2.html#section-11",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "How does R calculate the p-value?\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow many degrees of freedom does this \\(t\\)-distribution have?"
  },
  {
    "objectID": "slides/week8-day2.html#section-12",
    "href": "slides/week8-day2.html#section-12",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Did we get similar results between these methods?"
  },
  {
    "objectID": "slides/week8-day2.html#section-13",
    "href": "slides/week8-day2.html#section-13",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Why not always use theoretical methods?\n\n\n\n\nTheory-based methods only hold if the sampling distribution is normally shaped.\n\n\n\nThe normality of a sampling distribution depends heavily on model conditions."
  },
  {
    "objectID": "slides/week8-day2.html#section-14",
    "href": "slides/week8-day2.html#section-14",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What are these “conditions”?\n\n\n\n\nFor linear regression we are assuming…\n\n\n\nLinear relationship between \\(x\\) and \\(y\\)\n\nIndepdent observations\n\nNormality of residuals\n\nEqual variance of residuals"
  },
  {
    "objectID": "slides/week8-day2.html#section-15",
    "href": "slides/week8-day2.html#section-15",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Linear relationship between \\(x\\) and \\(y\\)\n\n\n\nWhat should we do?"
  },
  {
    "objectID": "slides/week8-day2.html#section-16",
    "href": "slides/week8-day2.html#section-16",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Variable transformation!"
  },
  {
    "objectID": "slides/week8-day2.html#section-17",
    "href": "slides/week8-day2.html#section-17",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Independence of observations\n\n\n\nThe evals dataset contains 463 observations on 94 professors. Meaning, professors have multiple observations.\n\nWhat can we do?\n\n\n\nBest – use a random effects model\nReasonable – collapse the multiple scores into a single score"
  },
  {
    "objectID": "slides/week8-day2.html#section-18",
    "href": "slides/week8-day2.html#section-18",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Normality of residuals\n\n\n\nWhat should we do?"
  },
  {
    "objectID": "slides/week8-day2.html#section-19",
    "href": "slides/week8-day2.html#section-19",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Variable transformation!"
  },
  {
    "objectID": "slides/week8-day2.html#section-20",
    "href": "slides/week8-day2.html#section-20",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Equal variance of residuals\n\n\n\nWhat should we do?"
  },
  {
    "objectID": "slides/week8-day2.html#section-21",
    "href": "slides/week8-day2.html#section-21",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Variable transformation!"
  },
  {
    "objectID": "slides/week8-day2.html#section-22",
    "href": "slides/week8-day2.html#section-22",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Are these conditions required for both methods?\n\n\n\n\n\nSimulation-based Methods\n\n\n\nLinearity of Relationship\nIndependence of Observations\nEqual Variance of Residuals\n\n\n\n\n\n\nTheory-based Methods\n\n\n\nLinearity of Relationship\nIndependence of Observations\n\n\n\n\nNormality of Residuals\n\n\n\n\nEqual Variance of Residuals"
  },
  {
    "objectID": "slides/week8-day2.html#section-23",
    "href": "slides/week8-day2.html#section-23",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What happens if the conditions are violated?\n\n\nIn general, when the conditions associated with these methods are violated, the permutation and \\(t\\)-distributions will underestimate the true standard error of the sampling distribution."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the quarter. Note that this schedule will be updated as the quarter progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nLecture Slides\nActivity\nLab\nCritique\nProject\n\n\n\n\n0\nSun, January 7\nWelcome to Stat 313 / 513 Course Set-up\n📖\n\n\n\n\n\n\n\n1\nTue, January 9\nCourse Structure & Statistics Review\n📖\n🖥\n\n\n\n\n\n\n\nThu, January 11 (class held asynchronously)\nIntroduction to Data\n\n🎥\n📋\n💻  Due January 15  Lab 1 Feedback\n\n\n\n\n2\nTue, January 16 (no class)\n\n📖\n\n\n\n\n\n\n\n\nThu, January 18\nVisualizing & Summarizing Numerical Variables\n\n🖥\n📋\n💻  Due January 22\n\n\n\n\n3\nTue, January 23 (class held asynchronously)\nIncorporating Categorical Variables\n📖\n🖥\n\n\n\n\n\n\n\nThu, January 25 (class held asynchronously)\nEthics & Categorical Variables\n\n🖥\n\n💻  Due January 29\n\n\n\n\n\nMon, January 29\nStatistical Critique 1 Due\n\n\n\n\n✍\n\n\n\n4\nTue, January 30\nIntroduction to Linear Regression\n📖\n🖥\n\n\n\n\n\n\n\nThu, February 1\nThe Ugly History of Linear Regression\n\n🖥\n\n💻\n\n📁\n\n\n\nSun, February 4\nMidterm Project Proposal Due\n\n\n\n\n\n📁\n\n\n5\nTue, February 6\nIntroduction to Multiple Linear Regression\n📖\n🖥\n\n\n\n\n\n\n\nThu, February 8\nWork Day – Coding a Multiple Linear Regression\n\n🖥\n📋\n\n\n\n\n\n\nSun, February 11\nMidterm Project First Draft Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\n6\nTue, February 13\nVariable Selection in Multiple Linear Regression\n📖\n🖥\n📋\n\n\n\n\n\n\nThu, February 15\nMachine Learning\n\n🖥\n\n💻\n\n\n\n\n\nSun, February 18\nMidterm Project Final Version Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\n7\nTue, February 20\nSampling Variability\n📖\n🖥\n📋\n\n\n\n\n\n\nThu, February 22\nConfidence Intervals\n\n🖥\n📋\n💻  Due February 26\n\n\n\n\n8\nTue, February 27\np-values & Hypothesis Tests\n📖\n🖥\n\n\n\n\n\n\n\nThu, February 29\nSimulation-based Methods vs. Theory-based Methods\n\n🖥\n\n💻  Due March 4\n\n\n\n\n\nMon, March 4\nStatistical Critique 2 Due\n\n\n\n\n✍\n\n\n\n9\nTue, March 5\nOne-Way Analysis of Variance (OWA)\n📖\n🖥\n\n\n\n\n\n\n\n\nThur, March 7\nInference for OWA & Model Selection\n\n\nSelecting a OWA Model: 📋\n\n\n\n\n\n\nSun, March 10\nFinal Project First Draft Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\n10\nTue, March 12\nTwo-Way ANOVA (TWA) Models\n📖\n🖥\nSelecting a TWA Model: 📋\n\n\n\n\n\n\nThu, March 14\nLast Day of STAT 313\n\n🖥\nAssessing Independence: 📋\n\n\nWhat should not be in your project: 📋\n\n\n\nSun, March 17\nFinal Project Final Version Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\nFinals Week\n\nFinal Presentations\n\n\n\n\n\nPresentation Instructions:📁\n\n\n\nTues, March 19\n4:10-7:00pm\nSTAT 313-02\n\n\n\n\n\n\n\n\n\nThur, March 21\n10:10am-1:00pm\nSTAT 513-01\n\n\n\n\n\n\n\n\n\nThur, March 21\n1:10-4:00pm\nSTAT 313-01"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "This course uses Posit Cloud for lab assignments and projects. There is a link to join the STAT 313 Posit Cloud Workspace at the bottom of the Week 1 Module on Canvas.\nIf you have worked with R & RStudio before, I do not want you using the local installation of RStudio to work on your Labs / Projects. I cannot help you on your code if things go sideways! If you use Posit Cloud, however, I can “peek in” to your project and help you debug your code. :)",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "resources/week-2.html",
    "href": "resources/week-2.html",
    "title": "Week 2 – Exploring Numerical Variables",
    "section": "",
    "text": "Tip\n\n\n\nWherever you see &lt; and &gt; characters, these need to be replaced with information from your dataset. For example,\n\n\n\n\n\n\n\n\nLoading in Packages\nlibrary(tidyverse)\nLoads a package into the R workspace, so you can use the functions and data it contains\n\n\n\n\n\n\n\nReading in Data\nIPEDS &lt;- read_csv(here::here(“data”,\n                             “&lt;NAME OF DATASET.csv&gt;”)\n                  )\nNote: The name of the dataset will change, but it will always need to have the .csv at the end of its name!\n\n\nAssignment Arrow\npenguins_2007 &lt;- filter(penguins, year == 2007)\nAssigns a value (e.g., dataframe) to the name of a variable\n\n\nFiltering a Dataset\nlarge_adelie_2008 &lt;- filter(penguins,\n                            species == “Adelie”,\n                            body_mass_g &gt; 3000,\n                            year == 2008)\nFilters observations (rows) out of / into a dataframe, where the inputs (arguments) are the conditions to be satisfied in the data that are kept\nNote: It makes your code more readable if you put each filter on a new line (hit enter after each comma)!\n\n\nMutating a Dataset\npenguins_large &lt;- mutate(penguins,\n    body_mass_kg = body_mass_g / 1000)\nCreates new variables or modifies existing variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Summary Statistics for Numerical Variables\nsummarize(&lt;NAME OF DATASET&gt;,\n          &lt;NAME OF STAT&gt; = &lt;STAT FUNCTION&gt;(&lt;NAME OF VARIABLE&gt;)\n         )\nFor example, to calculate the mean and median of the dep_delay variable from the nycflights dataset we have:\nsummarize(nycflights,\n          mean_dep_delay = mean(dep_delay),\n          median_dep_delay = median(dep_delay)\n          )\n\n\nHistogram\nggplot(data = &lt;NAME OF DATASET&gt;,\n       mapping = aes(x = &lt;NAME OF VARIABLE&gt;)) +\n  geom_histogram(binwidth = &lt;WIDTH OF BINS&gt;) +\n  labs(x = “&lt;TITLE FOR THE X-AXIS&gt;”)\nNote: A histogram must have the variable on the x-axis!\n\n\nBoxplot\nggplot(data = &lt;NAME OF DATASET&gt;,\n       mapping = aes(x = &lt;NAME OF VARIABLE&gt;)) +\n  geom_boxplot() +\n  labs(x = “&lt;TITLE FOR THE X-AXIS&gt;”)\nNote: This boxplot is horizontal. If you want for your boxplot to be vertical, you use y = instead of x = . Keep in mind you will need to change the location of you axis label, too!\n\n\nScatterplot\nggplot(data = &lt;NAME OF DATASET&gt;,\n       mapping = aes(x = &lt;NAME OF X-VARIABLE&gt;,\n                     y = &lt;NAME OF Y-VARIABLE&gt;)\n       ) +\n  geom_point() +\n  labs(x = “&lt;TITLE FOR THE X-AXIS&gt;”,\n       y = \"&lt;TITLE FOR THE Y-AXIS&gt;\")"
  },
  {
    "objectID": "activity/week-1-quarto.html",
    "href": "activity/week-1-quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Quarto is possibly my favorite thing that happened in 2023! It is an easy-to-use tool for creating reproducible data analyses. Our weekly lab assignments will be done in Quarto, so this week each of you is going to complete your own lab assignment, so you can get to know Quarto a bit on your own.\n\nAccessing Lab 1\nWe will be using Posit Cloud to work with R, so there is no need for you to download any software. However, you will need to create an account for you to be able to access the weekly labs.\nHere are the steps you need to complete for you to access this week’s (and every week’s) lab assignment:\n\nClick on the Posit Cloud link below the “Lab” assignment module on Canvas\nCreate a log-in for Posit Cloud\n\n\n\n\nWhat you will see when you first click on the link to join the Posit Cloud workspace for STAT 313 / 513\n\n\n\nClick “Yes” to join the STAT 313 / 513 workspace\n\n\n\n\nPrompt you should see after you make an account, asking if you want to join the workspace – you should click “Yes”\n\n\n\nIf you successfully joined the workspace, you should see a page that looks like this:\n\n\n\n\nThe “Welcome to Stat 313 / 513 Winter 2024” welcome message you should see if you successfully joined the workspace\n\n\n\n\n\n\n\n\nYour welcome page should reflect the course you are registered for\n\n\n\nIn the image above, the message says “Welcome to Stat 513 Winter 2024”. If you are registered for Stat 313, your welcome message should say “Welcome to Stat 313 Winter 2024”.\n\n\n\nOnce you are in the workspace, you need to access the Content tab, which is where the lab assignments will be listed\n\n\n\n\nWhat you should see if you click on the “Content” tab in the Stat 313 / 513 workspace\n\n\n\nClick on the Lab 1 project to open this week’s lab assignment\n\n 7. Once you are in the Lab 1 project, the final step is to open the Lab 1 Quarto document. To do this, you need to click on the lab-1.qmd document, located in the lower right pane.\n\n\n\nWhere you need to click to open the Lab 1 assignment (lab-1.qmd) and what you should see pop up once you open the document\n\n\nNow that you have Lab 1 open, you are ready to get started! Use the resources below to learn more about working in a Quarto document.\n\n\nLearning About Quarto\n\n🎥 Video Guide of Quarto\nLink to video: https://www.youtube.com/watch?v=_f3latmOhew\n\n\n\n\n\n\nWatch the first 10-minutes!\n\n\n\n\n\n\n\n\n📖 Textbook Guide of Quarto\nLink to textbook chapter: https://r4ds.hadley.nz/quarto.html\n\n\n\n\n\n\nRead sectios 28.1 through 28.5\n\n\n\n\n\n\n\n\n💻 Tutorial of Quarto\nLink to tutorial: https://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\n\n\n\nI’d recommend also watching the video!"
  },
  {
    "objectID": "activity/week-6-model-selection.html",
    "href": "activity/week-6-model-selection.html",
    "title": "STAT 313 / 513 - Winter 2024",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(moderndive)"
  },
  {
    "objectID": "activity/week-6-model-selection.html#full-model-all-explanatory-variables-included",
    "href": "activity/week-6-model-selection.html#full-model-all-explanatory-variables-included",
    "title": "STAT 313 / 513 - Winter 2024",
    "section": "Full Model – ALL Explanatory Variables Included",
    "text": "Full Model – ALL Explanatory Variables Included\n\nlm(body_mass_g ~ . , data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.877         0.873 79631.  282.  287.      255.       0     9   333\n\n\n\n\n\n\n\n\nStarting adjusted \\(R^2\\)\n\n\n\nNote the adjusted \\(R^2\\) value you are starting with, as this will influence your decisions."
  },
  {
    "objectID": "activity/week-6-model-selection.html#variable-selection",
    "href": "activity/week-6-model-selection.html#variable-selection",
    "title": "STAT 313 / 513 - Winter 2024",
    "section": "Variable Selection",
    "text": "Variable Selection\nNow, starting with our full model, we will use backwards selection to decide what variable(s) should be removed from the model.\nYou can only delete a variable if it increases adjusted \\(R^2\\).\n\n\n\n\n\n\nNote\n\n\n\nNote we are not saying how much adjusted \\(R^2\\) needs to be increased, simply that it must be bigger."
  },
  {
    "objectID": "activity/week-6-model-selection.html#models-deleting-one-explanatory-variable",
    "href": "activity/week-6-model-selection.html#models-deleting-one-explanatory-variable",
    "title": "STAT 313 / 513 - Winter 2024",
    "section": "Models Deleting One Explanatory Variable",
    "text": "Models Deleting One Explanatory Variable\n\nlm(body_mass_g ~ . -year, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.875         0.872 80659.  284.  288.      284.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -sex, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.853         0.849 94951.  308.  312.      235.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -flipper_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.863          0.86 88256.  297.  301.      256.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -bill_depth_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.873          0.87 81908.  286.  290.      279.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -bill_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.871 81384.  285.  289.      281.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -island, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.876         0.874 79869.  283.  286.      329.       0     7   333\n\n\n\n\nlm(body_mass_g ~ . -species, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.847         0.843 99062.  315.  319.      257.       0     7   333\n\n\n\n\n\n\n\n\n\nWhich variable should be deleted?\n\n\n\nBased on the adjusted \\(R^2\\) values, which variable should be deleted from the model.\nRemember: You are looking for the model that has a higher adjusted \\(R^2\\) than what you started with!"
  },
  {
    "objectID": "activity/week-6-model-selection.html#models-deleting-two-explanatory-variables",
    "href": "activity/week-6-model-selection.html#models-deleting-two-explanatory-variables",
    "title": "STAT 313 / 513 - Winter 2024",
    "section": "Models Deleting Two Explanatory Variables",
    "text": "Models Deleting Two Explanatory Variables\n\nlm(body_mass_g ~ . - island - year, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.875         0.873 80828.  284.  287.      380.       0     6   333\n\n\n\nlm(body_mass_g ~ . - island - sex, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.852         0.849 95580.  309.  312.      313.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - flipper_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.863         0.861 88278.  297.  300.      344.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.873         0.871 82154.  287.  290.      373.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.871 81636.  286.  289.      376.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - species, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.831         0.829 108979.  330.  333.      323.       0     5   333\n\n\n\n\n\n\n\n\n\n\nWhich variable should be deleted?\n\n\n\nBased on the adjusted \\(R^2\\) values, which variable should be deleted from the model.\nRemember: You are looking for the model that has a higher adjusted \\(R^2\\) than what you started with!"
  },
  {
    "objectID": "activity/week-6-model-selection.html#new-rules",
    "href": "activity/week-6-model-selection.html#new-rules",
    "title": "STAT 313 / 513 - Winter 2024",
    "section": "New Rules",
    "text": "New Rules\nNow, choose the simplest model that is within 1% of the best adjusted \\(R^2\\) you obtained.\n\n\n\n\n\n\nNote\n\n\n\nNote, when I say “simplest model” I mean the model with the fewest variables included.\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - bill_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.869         0.867 84623.  291.  294.      434.       0     5   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.871         0.869 83702.  289.  292.      440.       0     5   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - species, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.818         0.815 117911.  343.  346.      368.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - flipper_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.856         0.853 93353.  306.  308.      387.       0     5   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - sex, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.832         0.829 108823.  330.  333.      323.       0     5   333\n\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.867         0.865 86047.  293.  296.      534.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - flipper_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.856         0.854 93385.  306.  308.      486.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - sex, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.824         0.822 113574.  337.  340.      385.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - species, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.807         0.805 125076.  354.  356.      457.       0     3   333\n\n\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm - sex, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.787         0.785 137667.  371.  373.      405.       0     3   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm - species, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.806         0.805 125512.  354.  356.      685.       0     2   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm - flipper_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.847         0.845 99037.  315.  317.      606.       0     3   333\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is your best model?\n\n\n\nWhat variables are included in the final model you chose?"
  },
  {
    "objectID": "activity/week-5-model-justification.html",
    "href": "activity/week-5-model-justification.html",
    "title": "Model Justification",
    "section": "",
    "text": "This study seeks to investigate how the relationship between average SAT math scores and the percentage of students who are economically disadvantaged differs based on the size of the school. As seen in the plot below, the slope for this relationship was found to be similar across the different levels of schools. Therefore, this study used a parallel slopes (additive) multiple linear regression model.\n\n\n\n\n\n\n\n\n\nWhat do you feel is missing from this model justification?"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are transitioning into our last topic of the quarter—comparing multiple means. These comparisons have a specific name, ANalysis of VAriance (ANOVA).\nWe are going to use all of the simulation-based methods we learned previously for this new context. An ANOVA relies on a new statistic, the \\(F\\)-statistic, to summarize how different 3 or more means are from each other.\nThe primary focus of an ANOVA is to detect if the means of 3 or more groups are different. Because of this we focus on hypothesis tests for a difference in the group means and do not use confidence intervals. We will generate permutation distributions of \\(F\\)-statistics that we could have observed if the null hypothesis was true (there is no difference in the group means)."
  },
  {
    "objectID": "weeks/week-9.html#learning-outcomes",
    "href": "weeks/week-9.html#learning-outcomes",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe what an ANOVA tests for\nuse a visualization to outline how the following are calculated:\n\ntotal sum of squares\ngroup sum of squares\nresidual sum of squares\n\ndescribe why the mean squares of groups is called the “between group variability”\ndescribe why the mean square error is called the “within group variability”\noutline how an F-statistic is calculated\nexplain what a “large” or a “small” F-statistic indicates\ndescribe the conditions for performing an ANOVA procedure\noutline when it is appropriate to use an \\(F\\)-distribution in an ANOVA\nexplain the similarities and differences between parametric (\\(F\\)-based) methods and non-parametric (simulation-based) methods\nuse R to:\n\ngenerate a permutation distribution for F-statistics\nvisualize the permutation distribution\ncalculate the observed F-statistic statistic\ncalculate a p-value for a hypothesis test"
  },
  {
    "objectID": "weeks/week-9.html#textbook-reading",
    "href": "weeks/week-9.html#textbook-reading",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n\n\n\n\n\n\n\n\n\n\n\nRequired Reading: Inference for Comparing Many Means\n\nReading Guide – Due Tuesday by noon\nDownload the Word Document"
  },
  {
    "objectID": "weeks/week-9.html#concept-quiz-due-tuesday-by-noon",
    "href": "weeks/week-9.html#concept-quiz-due-tuesday-by-noon",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "1.2 Concept Quiz – Due Tuesday by noon",
    "text": "1.2 Concept Quiz – Due Tuesday by noon\n\nWhich of the following are true about the mean squares between groups?\n\n\nit is a standardized measure of the variability in responses between groups\nit compares the mean of each group to the overall mean across all groups\nit compares the observations within each group to the mean of that group\nit is used as the numerator in an F-statistic\nit is used as the denominator in an F-statistic\nit is found by dividing the sum of squares between groups by the number of groups minus 1 (\\(k\\) - 1)\nit is found by dividing the sum of squares between groups by the sample size minus the number of groups (\\(n - k\\))\n\n\nWhich of the following are true about the mean square errors?\n\n\nit is a standardized measure of the variability in responses within each group\nit compares the mean of each group to the overall mean across all groups\nit compares the observations within each group to the mean of that group\nit is used as the numerator in an F-statistic\nit is used as the denominator in an F-statistic\nit is found by dividing the sum of square errors by the number of groups minus 1 (\\(k\\) - 1)\nit is found by dividing the sum of square errors by the sample size minus the number of groups (\\(n - k\\))\n\n\nAn F-statistic uses which formula?\n\n\n\\(\\frac{MSG}{MSE}\\)\n\\(\\frac{SSG}{SSE}\\)\n\\(\\frac{MSE}{MSG}\\)\n\\(\\frac{SSE}{SSG}\\)\n\n\nIdeally, in an ANOVA we’d like to see… (select all that apply)\n\n\nlarge variability in the means of the groups\nsmall variability in the means of the groups\nlarge variability in the observations within each group\nsmall variability in the observations within each group\n\n\nIf the null hypothesis that the means of four groups are all the same is rejected using ANOVA at a 5% significance level, then… (select all that apply)\n\n\nwe can then conclude that all the means are different from one another.\nthe variability between groups is higher than the variability within groups.\nthe pairwise analysis will identify at least one pair of means that are significantly different.\nan appropriate \\(\\alpha\\) to be used in pairwise comparisons is \\(\\frac{0.05}{4} = 0.0125\\) since there are four groups.\n\n\nAs the total sample size increases, the degrees of freedom for the residuals increases.\nThe constant variance condition can be somewhat relaxed when the sample sizes are large.\nThe independence assumption can be relaxed when the total sample size is large.\nThe normality condition is very important when the sample sizes of each group are small."
  },
  {
    "objectID": "weeks/week-9.html#r-tutorial-due-thursday-by-noon",
    "href": "weeks/week-9.html#r-tutorial-due-thursday-by-noon",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "1.3 R Tutorial – Due Thursday by noon",
    "text": "1.3 R Tutorial – Due Thursday by noon\nRequired Tutorial: Comparing many means with ANOVA"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to continuing exploring data, through data summaries and visualizations, focusing specifically on numerical variables. We will be using the dplyr package in R to wrangle our data and the ggplot2 package to created data visualizations."
  },
  {
    "objectID": "weeks/week-2.html#learning-outcomes",
    "href": "weeks/week-2.html#learning-outcomes",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\noutline the differences between numerical and categorical variables\ndescribe what type of summary statistic is appropriate for a given distribution of a numerical variable\ndiscuss when it is / is not appropriate to summarize a variable with a mean\ncreate visualizations of one and two numerical variables\ndiscuss the benefits and shortcomings of different visualizations"
  },
  {
    "objectID": "weeks/week-2.html#textbook-reading-part-1",
    "href": "weeks/week-2.html#textbook-reading-part-1",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.1 Textbook Reading – Part 1",
    "text": "1.1 Textbook Reading – Part 1\n\n\n\n\n\n\n\nRequired Reading: Exploring Numerical Data\n\nReading Guide – Due Thursday by the Beginning of Class\nDownload the Word Document\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit your completed reading guide to the Canvas assignment portal!"
  },
  {
    "objectID": "weeks/week-2.html#concept-quiz",
    "href": "weeks/week-2.html#concept-quiz",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.2 Concept Quiz",
    "text": "1.2 Concept Quiz\n\n\n\n\n\n\nNote\n\n\n\nThe two concept quizzes from each chapter have been combined into one concept quiz on Canvas.\n\n\n\nSuppose we have data on the departure delays of flights flying out of New York. What shape would you expect the distribution of departure delays to have?\n\n\nright skew\nleft skew\nbimodal\nmutimodal\nuniform\n\nHint: Think about how you would “typically” expect flight delays to behave.\n\nTo better decide what summary statistic we should use to summarize the departure delays it would be best to create a data visualization of the distribution of departure delays. What type of visualizations could we make? Select all that apply!\n\n\nboxplot\nhistogram\nbarplot\ndensity plot\nscatterplot"
  },
  {
    "objectID": "weeks/week-2.html#textbook-reading-part-2",
    "href": "weeks/week-2.html#textbook-reading-part-2",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.3 Textbook Reading – Part 2",
    "text": "1.3 Textbook Reading – Part 2\nRequired Reading: Data Visualization\n\n\n\n\n\n\n\n\nReading Guide – Due Thursday by the Beginning of Class\nDownload the Word Document\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit your completed reading guide to the Canvas assignment portal!"
  },
  {
    "objectID": "weeks/week-2.html#concept-quiz-due-tuesday-by-noon",
    "href": "weeks/week-2.html#concept-quiz-due-tuesday-by-noon",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.4 Concept Quiz – Due Tuesday by noon",
    "text": "1.4 Concept Quiz – Due Tuesday by noon\n\nWhat aesthetics are being used in the following plot?\n\nHint: Think about what goes inside of the aes() function and what does not.\n\n\n\n\n\n\n\n\n\n\nx axis\ny axis\ncolor\nfacets\npoints\nlines\n\n\nWhat geometric objects are being used in the displayed visualization?\n\nHint: Think about what geoms you would use to make this plot!\n\npoints\nlines / smoothers\ncolors\nfacets\n\n\nWhat aspects of the distribution of departure delays can you see in the histogram that you could not see in the boxplot?\n\n\n\n\n\n\n\n\n\n\n\n\nshape of distribution\nmedian\noutliers\nmode\n\n\n\n\n\n\n\nNote\n\n\n\nThe two concept quizzes from each chapter have been combined into one concept quiz on Canvas."
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week One: Foundations of Statistics",
    "section": "",
    "text": "Welcome!\nIn this coursework, you’ll get a refresher on the foundational components of statistics and data, investigate how statistics is used in your major, and think critically about the philosophy of statistical inference."
  },
  {
    "objectID": "weeks/week-1.html#learning-outcomes",
    "href": "weeks/week-1.html#learning-outcomes",
    "title": "Week One: Foundations of Statistics",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe observations, variables, and data matrices\nexplain the different types of variables a study can have\nillustrate the difference between explanatory and response variables\ndelineate the difference between a population and a sample\ncompare and contrast different sampling methods\noutline what is necessary to make an experiment an “experiment”\ncharacterize the differences between observational studies and experiments"
  },
  {
    "objectID": "weeks/week-1.html#textbook-reading",
    "href": "weeks/week-1.html#textbook-reading",
    "title": "Week One: Foundations of Statistics",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n📖 Required Reading: Introduction to Modern Statistics – Hello Data\n📖 Required Reading: Introduction to Modern Statistics – Study Design\n\nReading Guide – Due Tuesday by noon\nDownload the Word Document\nNote: There is one combined reading guide for both chapters.\n\n\n\n\n\n\nSubmission (Due Tuesday, January 9 by the start of class)\n\n\n\nSubmit your completed reading guide to the Canvas assignment portal!"
  },
  {
    "objectID": "weeks/week-1.html#concept-quiz-due-tuesday-by-the-start-of-class",
    "href": "weeks/week-1.html#concept-quiz-due-tuesday-by-the-start-of-class",
    "title": "Week One: Foundations of Statistics",
    "section": "1.2 Concept Quiz – Due Tuesday by the start of class",
    "text": "1.2 Concept Quiz – Due Tuesday by the start of class\n1. What are the different types of variables that can appear in a dataset? Select all that apply!\n\ndiscrete numerical\nordinal categorical\nnominal categorical\ncontinuous numerical\ndiscrete categorical\n\n2. Just because a variable has numeric values, does not mean it is a numeric variable. Which of the following variables appear numerical but behave like a categorical variable? Select all that apply!\n\nzip code\nGPA\nheight\nyear in school\n\n3. Which of the following statements are true about observational studies and experiments? Select all that apply!\n\nExperiments randomly assign the explanatory variable\nObservational studies randomly assign the explanatory variable\nObservational studies can make causal statements about the relationship between the explanatory and response variables\nExperiments can make causal statements about the relationship between the explanatory and response variables\n\n4. What are different methods for sampling from a population? Select all that apply!\n\nsimple random sampling\nstratified random sampling\ncluster sampling\nmultistage sampling\nconvenience sampling\n\n5. Cluster sampling and stratified sampling both rely on grouping observations, but have important differences. Match each method to how observations are randomly sampled.\n\n\nstratified sampling\ncluster sampling\n\ngroups of observations are created, groups are randomly selected, every observation in the selected group is sampled\ngroups of observations are created, observations within a group are randomly sampled"
  },
  {
    "objectID": "weeks/week-1.html#statistical-critique-step-0-due-monday-by-5pm",
    "href": "weeks/week-1.html#statistical-critique-step-0-due-monday-by-5pm",
    "title": "Week One: Foundations of Statistics",
    "section": "3.1 Statistical Critique Step 0 – Due Monday by 5pm",
    "text": "3.1 Statistical Critique Step 0 – Due Monday by 5pm\nUpload a PDF of your journal article to the Canvas assignment portal."
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week Four: Introduction to Linear Regression (Basic Regression)",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to start exploring statistical methods—linear regression. We will start with the “basic” case or simple linear regression, with a single quantitative explanatory variable and a quantitative response. We will review the concepts behind linear regression, learn how to visualize regression models, and practice obtaining the estimated regression lines in R.",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#learning-outcomes",
    "href": "weeks/week-4.html#learning-outcomes",
    "title": "Week Four: Introduction to Linear Regression (Basic Regression)",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe the difference between explanatory and predictive modeling\nproduce data visualizations for a simple linear regression\ndescribe how R decides on the “best” regression line\nfit a simple linear regression in R\nwrite out an estimated regression line\ninterpret the slope of an estimated regression line\ninterpret the intercept of an estimated regression line\ndescribe what an “offset” is in a simple linear regression\ninterpret the offsets of an estimated regression line\ncalculate a residual for an observation",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#textbook-reading",
    "href": "weeks/week-4.html#textbook-reading",
    "title": "Week Four: Introduction to Linear Regression (Basic Regression)",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Chapter 5 (https://moderndive.com/5-regression.html)\n\n\n\n\n\n\n\n\nNote\n\n\n\nDon’t worry about Section 5.3.3, it goes a bit too in the weeds for what we are interested in.\n\n\n\nReading Guide\nDownload the Word Document\n\n\n\n\n\n\nDifferent colored answers\n\n\n\nPlease use a different color for your answers to the reading guide, so it is easier to find your responses! 😊",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#concept-quiz-due-tuesday-by-noon",
    "href": "weeks/week-4.html#concept-quiz-due-tuesday-by-noon",
    "title": "Week Four: Introduction to Linear Regression (Basic Regression)",
    "section": "1.2 Concept Quiz – Due Tuesday by noon",
    "text": "1.2 Concept Quiz – Due Tuesday by noon\n1. What does the correlation coefficient measure?\n\nstrength of a relationship between two variables\nstrength of a linear relationship between two variables\nstrength of a relationship between two numerical variables\nstrength of a linear relationship between two numerical variables\n\n2. How do you interpret a slope coefficient?\n\nFor any increase of \\(x\\), the slope is the expected increase in \\(y\\)\nFor any increase of \\(x\\), the slope is the expected increase in the mean of \\(y\\)\nFor a 1-unit increase in \\(x\\), the slope is the expected increase in \\(y\\)\nFor a 1-unit increase in \\(x\\), the slope is the expected increase in the mean of \\(y\\)\n\n3. How do you interpret an intercept coefficient?\n\nFor a 1-unit increase in \\(x\\), the intercept is the expected increase in \\(y\\)\nFor a 1-unit increase in \\(x\\), the intercept is the expected increase in the mean of \\(y\\)\nFor an \\(x\\) value of 0, the intercept is the expected value of \\(y\\)\nFor an \\(x\\) value of 0, the intercept is the expected value of the mean of \\(y\\)\n\n4. Match the explanatory and response variables to the correct variables in the lm() function syntax.\nlm(variable1 ~ variable2, data = &lt;NAME OF DATASET&gt;)\n5. What geom_ adds a linear regression line to a scatterplot?\n\ngeom_line()\ngeom_smoother()\ngeom_regression()\ngeom_smooth()",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#r-tutorial-due-thursday-by-noon",
    "href": "weeks/week-4.html#r-tutorial-due-thursday-by-noon",
    "title": "Week Four: Introduction to Linear Regression (Basic Regression)",
    "section": "1.3 💻 R Tutorial – Due Thursday by noon",
    "text": "1.3 💻 R Tutorial – Due Thursday by noon\n\nRegression modeling: Simple linear regression\nRegression modeling: Interpreting regression models",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7 – Inference for Regression",
    "section": "",
    "text": "1 Textbook Reading – Part 1\nRequired Reading: Exploring Sampling Variability\n\n\n\n\n\n\nReading Guide – Due Tuesday by noon\n\n\n\nDownload the Word Document\n\n\n\n\n2 Textbook Reading – Part 2\nRequired Reading: Confidence Intervals for the Slope\n\n\n\n\n\n\nReading Guide – Due Thursday by noon (note the new time!)\n\n\n\nDownload the Word Document\n\n\n\n\n3 Concept Quiz – Due Thursday by noon (note the new time!)\n1. Match each item to it’s respective analogy:\n\n\npoint estimate\nconfidence interval\n\n\n\nfishing with a net\nfishing with a spear\n\n\n2. To create a 95% confidence interval using the percentile method, what percentiles of the bootstrap distribution do you need to calculate?\n\n0th\n2.5th\n5th\n90th\n95th\n97.5th\n\n3. To create a 95% confidence interval using the standard error method, what standard error do you use?\n\nthe sample standard deviation\nthe bootstrap distribution standard deviation\na resample standard deviation\n1.96\n\n4. We almost never know if our confidence interval captured the true population parameter.\n\nTrue\nFalse\n\n5. What percentage of 99% confidence intervals do you expect to capture the true population parameter?\n6. The word “confident” in a confidence interval interpretation corresponds to what aspect of the interval?\n\nthe accuracy of the original sample\nthe reliability of the procedure for constructing confidence intervals\nthe precision of the bootstrap samples\n\n7. Which of the following are true?\n\nSmaller sample sizes tend to produce narrower confidence intervals.\nSmaller sample sizes tend to produce wider confidence intervals.\nLower confidence levels tend to produce wider confidence intervals.\nLower confidence levels tend to produce narrower confidence intervals.\n\n8. In a regression table, what does the “std_error” value associated with the slope represent?\n\nthe standard deviation of the sample\nthe standard deviation of the bootstrap distribution\nthe estimated standard deviation of the sampling distribution\nthe standard error of the sample\n\n9. In a regression table, how is the “std_error” value calculated?\n\na mathematical formula\nthe standard deviation of the sample\nthe standard deviation of the bootstrap distribution\n\n10. What percentage confidence interval is output in a regression table?\n\n99%\n95%\n90%\n\n\n\n4 R Tutorial – Due Thursday by noon\nRequired Tutorial: Practice the infer Workflow",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/chapters/week8-reading.html",
    "href": "weeks/chapters/week8-reading.html",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "",
    "text": "In this week’s coursework we are finally talking about p-values! What we learned last week should have helped make the connection between a sampling distribution and a bootstrap (resampling) distribution. Hopefully, you understand that we create confidence intervals based the statistics we saw in other samples.\nThis week we are going to connect these ideas to the framework of hypothesis testing. Hypothesis testing requires an additional component we didn’t see last week—the null hypothesis. We will learn how we integrate the null hypothesis in our resampling procedure to create a sampling distribution that could have happened if the null hypothesis was true. We will use this distribution to compare what we saw in our data and evaluate the plausibility of competing hypotheses.\nThis reading will walk you through the general framework for understanding hypothesis tests. By understanding this general framework, you’ll be able to adapt it to many different scenarios. The same can be said for confidence intervals. There was one general framework that applies to all confidence intervals. I believe this focus on the conceptual framework is better for long-term learning than focusing on specific details for specific instances."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#reading-guide",
    "href": "weeks/chapters/week8-reading.html#reading-guide",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "0.1 Reading Guide",
    "text": "0.1 Reading Guide\nDownload the reading guide as a Word Document here"
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#observed-data",
    "href": "weeks/chapters/week8-reading.html#observed-data",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.1 Observed data",
    "text": "1.1 Observed data\nFigure 1 visualizes the relationship between mage and weight for this sample of 1,000 birth records.\n\nggplot(data = births14, \n       mapping = aes(x = mage, y = weight)) + \n  geom_jitter() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight of Baby (lbs)\") + \n  my_theme\n\n\n\n\n\n\n\nFigure 1: Weight of baby at birth (in lbs) as explained by mother’s age.\n\n\n\n\n\nTable 1 displays the estimated regression coefficients for modeling the relationship between mage and weight for this sample of 1,000 birth records.\n\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n\nget_regression_table(births_lm)\n\n\n\n\n\nTable 1: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age.\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\nintercept\n6.793\n\n\nmage\n0.014\n\n\n\n\n\n\n\n\n\n\nBased on these coefficients, the estimated regression equation is:\n\\[ \\widehat{\\text{birth weight}} = -6.793 + 0.014 \\times \\text{mother's age}\\]\nBased on the regression equation, it appear that for every year older a mother is, we would expect the mean birth weight to increase by approximately 0.014 lbs. This seems like a small value, which is confirmed when we increase the mother’s age by 10-years, which is associated with a 0.14 lb increase in birth weight."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#research-question",
    "href": "weeks/chapters/week8-reading.html#research-question",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.2 Research question",
    "text": "1.2 Research question\nThis raises the question of whether this change is “large1 enough” to suggest that there is a relationship between the birth weight of a baby and the age of the mother. Could we obtain a slope statistic of 0.014 occur just by chance, in a hypothetical world where there is no relationship between a baby’s birth weight and a mother’s age? In other words, what role does sampling variation play in this hypothetical world? To answer this question, we’ll again rely on a computer to run simulations."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-ht-activity",
    "href": "weeks/chapters/week8-reading.html#sec-ht-activity",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.3 Simulating data",
    "text": "1.3 Simulating data\nFirst, try to imagine a hypothetical universe where there is no relationship between the birth weight of a baby and a mother’s age. In such a hypothetical universe, the birth weight of a baby would be entirely determined from other variables (e.g., genetics, mother’s habits, etc.)\nBringing things back to the births14 data frame, the mage variable would thus be an irrelevant label, as is has no relationship with the birth weight of the baby. Since is has no bearing on the birth weight, we could randomly reassign these ages by “shuffling” them!\nTo illustrate this idea, let’s narrow our focus to six arbitrarily chosen birth records (of the 1,000) Table 2. The weight column displays the birth weight of the baby. The mage column displays the age of the mother. However, in our hypothesized universe there is no relationship between a baby’s birth weight and a mother’s age, so it would be of no consequence to randomly “shuffle” the values of mage. The shuffled_mage column shows one such possible random shuffling.\n\n\n\n\nTable 2: One example of shuffling mage variable.\n\n\n\n\n\n\nID\nweight\nmage\nshuffled_mage\n\n\n\n\n726\n8.75\n18\n26\n\n\n602\n8.53\n32\n22\n\n\n326\n6.81\n28\n24\n\n\n79\n7.01\n22\n18\n\n\n974\n6.03\n24\n32\n\n\n884\n6.32\n26\n28\n\n\n\n\n\n\n\n\n\n\nAgain, such random shuffling of the mage label only makes sense in our hypothesized universe where there is no relationship between the birth weight of a baby and a mother’s age. How could we extend this shuffling of the mage variable to all 1,000 birth records by hand?\nOne way would be writing the 1,000 weights and mages on a set of 1,000 cards. We would then rip each card in half, as we are assuming there is no relationship between these variables. We would then be left with 1,000 weight cards in one hat and 1,000 mage cards in a different hat. You could then draw one card out of the weight hat and one card out of the mage hat and staple them together to make a new (weight, mage) ordered pair. This process of drawing one card out of each hat and stapling them together would be repeated until\nI’ve done one such reshuffling and plotted the original dataset and the shuffled dataset side-by-side in Figure 2. It appears that the slope of the regression line for the shuffled data is much less steep (closer to horizontal) than in the original data.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original values of mother’s age\n\n\n\n\n\n\n\n\n\n\n\n(b) Shuffled values of mother’s age\n\n\n\n\n\n\n\nFigure 2: Scatterplots of relationship between baby’s birth weight and mother’s age.\n\n\n\n\nLet’s compare these slope estimates between the two datasets:\n\nshuffled_births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  calculate(stat = \"slope\")\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 0.0142\n\n\nSo, in this hypothetical universe where there is no relationship between a baby’s birth weight and a mother’s age, we obtained a slope of 0.014.\nNotice that this slope statistic is not the same as the slope statistic of 0.014 that we originally observed. This is once again due to sampling variation. How can we better understand the effect of this sampling variation? By repeating this shuffling several times!"
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#shuffling-10-times",
    "href": "weeks/chapters/week8-reading.html#shuffling-10-times",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.4 Shuffling 10 times",
    "text": "1.4 Shuffling 10 times\nAlright, I’ve carried out the process of shuffling the dataset 9 more times. Table 3 displays the results of these shufflings.\n\n\n\n\nTable 3: One example of shuffling mage variable.\n\n\n\n\n\n\nID\nweight\nmage\nshuffle1\nshuffle2\nshuffle3\nshuffle4\nshuffle5\nshuffle6\nshuffle7\nshuffle8\nshuffle9\nshuffle10\n\n\n\n\n744\n10.13\n30\n37\n35\n28\n33\n24\n22\n23\n37\n28\n31\n\n\n426\n5.94\n36\n26\n27\n33\n23\n36\n33\n34\n23\n19\n26\n\n\n144\n5.07\n23\n36\n33\n30\n27\n37\n23\n38\n23\n34\n20\n\n\n132\n5.97\n28\n27\n22\n31\n20\n26\n28\n27\n33\n29\n27\n\n\n244\n6.50\n25\n33\n20\n30\n23\n36\n23\n22\n30\n39\n36\n\n\n581\n7.28\n29\n19\n27\n33\n38\n22\n35\n32\n27\n27\n14\n\n\n\n\n\n\n\n\n\n\n\nFor each of these 10 shuffles, I computed the slope statistic, and in Figure 3 I display their distribution in a histogram. I’ve also marked the observed slope statistic with a dark red line.\n\n\n\n\n\n\n\n\nFigure 3: Distribution of shuffled slope statistics.\n\n\n\n\n\nBefore we discuss the distribution of the histogram, we need to remember one key detail: this histogram represents relationship between a baby’s birth weight and mother’s age that one would observe in our hypothesized universe where there is no relationship between these variables.\nObserve first that the histogram is roughly centered at 0, which makes sense. A slope of 0 means there is no relationship between a baby’s birth weight and mother’s age, which is exactly what our hypothetical universe assumes!\nHowever, while the values are centered at 0, there is variation about 0. This is because even in a hypothesized universe of no relationship between a baby’s birth weight and mother’s age, you will still likely observe a slight relationship because of chance sampling variation. Looking at the histogram in Figure 3, such differences could even be as extreme as 0.02.\nTurning our attention to what we observed in the births14 dataset: the observed slope of 0.014 is is marked with a vertical dark line. Ask yourself: in a hypothesized world of no relationship between a baby’s birth weight and mother’s age, how likely would it be that we observe this slope statistic? That’s hard to say! It looks like there is only one statistic larger than the observed statistic out of ten, but that means we would expect to see a statistic bigger than what we saw 10% of the time in this hypothetical universe. To me, something happening 10% of the time doesn’t seem like a rare event."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#what-just-happened",
    "href": "weeks/chapters/week8-reading.html#what-just-happened",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.5 What just happened?",
    "text": "1.5 What just happened?\nWhat we just demonstrated in this activity is the statistical procedure known as hypothesis testing using a permutation test. The term ““permutation” is the mathematical term for “shuffling”: taking a series of values and reordering them randomly, as you did with the mage values.\nIn fact, permutations are another form of resampling, like the bootstrap method you performed last week. While the bootstrap method involves resampling with replacement, permutation methods involve resampling without replacement.\nThink back to the exercise involving the slips of paper representing the 1,000 birth records from last week. After sampling a paper, you put the paper back into the hat. However, in this scenario, once we drew a weight and age card they were stapled together and never redrawn.\nIn our example, we saw that the observed slope in the births14 dataset was somewhat inconsistent with the hypothetical universe, but only slightly. Thus, I would not be inclined to say that the observed relationship between a baby’s birth weight and a mother’s age (seen in the births14 dataset) is that different from what I would expect to see if there was no relationship between these variables."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#specify-variables",
    "href": "weeks/chapters/week8-reading.html#specify-variables",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.1 specify() variables",
    "text": "3.1 specify() variables\nRecall that we use the specify() verb to specify the response variable and, if needed, any explanatory variables for our study. In this case, since we are interested in the linear relationship between a baby’s birth weight and a mother’s age, we set weight as the response variable and mage as the explanatory variable.\n\nbirths14 %&gt;% \n  specify(response = weight, explanatory = mage) \n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 1,000 × 2\n   weight  mage\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   6.96    34\n 2   8.86    31\n 3   7.51    36\n 4   6.19    16\n 5   6.75    31\n 6   6.69    26\n 7   6.13    36\n 8   6.74    24\n 9   8.94    32\n10   9.12    26\n# ℹ 990 more rows\n\n\nAgain, notice how the births14 data itself doesn’t change, but the Response: weight (numeric) and Explanatory: mage (numeric) meta-data do. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Week 3."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#hypothesize-the-null",
    "href": "weeks/chapters/week8-reading.html#hypothesize-the-null",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.2 hypothesize() the null",
    "text": "3.2 hypothesize() the null\nIn order to conduct hypothesis tests using the infer workflow, we need a new step not present for confidence intervals: hypothesize(). Recall from Section 2 that our hypothesis test was\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_A: \\beta_1 \\neq 0\\)\n\nIn other words, the null hypothesis \\(H_0\\) corresponding to our “hypothesized universe” stated that there was no relationship between a baby’s birth weight and a mother’s age. We set this null hypothesis \\(H_0\\) in our infer workflow using the hypothesize() function. We do, however, need to declare what we are assuming about the relationship between our variables. For our regression we are assuming there is no relationship between our variables or that they are\"independent\".\n\nbirths14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  hypothesize(null = \"independence\")\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   weight  mage\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   6.96    34\n 2   8.86    31\n 3   7.51    36\n 4   6.19    16\n 5   6.75    31\n 6   6.69    26\n 7   6.13    36\n 8   6.74    24\n 9   8.94    32\n10   9.12    26\n# ℹ 990 more rows\n\n\nAgain, the data has not changed yet. In fact, we’ve just added one additional piece of meta-data the null hypothesis we are assuming."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#generate-replicates",
    "href": "weeks/chapters/week8-reading.html#generate-replicates",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.3 generate() replicates",
    "text": "3.3 generate() replicates\nAfter we hypothesize() the null hypothesis, we generate() replicates of “shuffled” datasets assuming the null hypothesis is true. We do this by repeating the shuffling exercise you performed in Section 1.3 several times. Instead of merely doing it 10 times, let’s use the computer to repeat this 1000 times by setting reps = 1000 in the generate() function. However, unlike for confidence intervals where we generated replicates using type = \"bootstrap\" resampling with replacement, we’ll now perform shuffles/permutations by setting type = \"permute\". Recall that shuffles / permutations are a kind of resampling, but unlike the bootstrap method, they involve resampling without replacement.\n\nbirths_generate &lt;- births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")\n\nbirths_generate\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000,000 × 3\n# Groups:   replicate [1,000]\n   weight  mage replicate\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;\n 1   6.56    34         1\n 2   9       31         1\n 3   6.96    36         1\n 4   7.8     16         1\n 5   8.39    31         1\n 6   6.56    26         1\n 7   8.95    36         1\n 8   7.94    24         1\n 9   8.52    32         1\n10   6.13    26         1\n# ℹ 999,990 more rows\n\n\nObserve that the resulting data frame has 1,000,000 rows. This is because we performed shuffles / permutations for each of the 1000 rows 1000 times and \\(1,000,000 = 1000 \\cdot 1000\\). If you explore the births_generate data frame, you would notice that the variable replicate indicates which resample each row belongs to. So it has the value 1 1000 times, the value 2 1000 times, all the way through to the value 1000 1000 times."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#calculate-summary-statistics",
    "href": "weeks/chapters/week8-reading.html#calculate-summary-statistics",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.4 calculate() summary statistics",
    "text": "3.4 calculate() summary statistics\nNow that we have generated 1000 replicates of “shuffles” assuming the null hypothesis is true, let’s calculate() the appropriate summary statistic for each of our 1000 shuffles. From Section 2, point estimates related to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the relationship between baby birth weights and mother’s ages for all births in the US in 2014, \\(\\beta_1\\), the test statistic here is the sample slope, \\(b_1\\). For each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"slope\".\nLet’s save the result in a data frame called null_distribution:\n\nnull_distribution &lt;- births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")%&gt;% \n  calculate(stat = \"slope\")\n\nnull_distribution\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate     stat\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1  0.00258\n 2         2  0.00641\n 3         3 -0.00803\n 4         4  0.00647\n 5         5 -0.00373\n 6         6 -0.0193 \n 7         7 -0.00970\n 8         8  0.00394\n 9         9 -0.0201 \n10        10 -0.00136\n# ℹ 990 more rows\n\n\nObserve that we have 1000 values of stat, each representing one instance of \\(b_1\\) in a hypothesized world of no relationship between these variables. Observe as well that we chose the name of this data frame carefully: null_distribution. Recall once again from Section 2 that sampling distributions when the null hypothesis \\(H_0\\) is assumed to be true have a special name: the null distribution.\nWhat was the observed relationship between a baby’s birth weight and a mother’s age in the births14 dataset? In other words, what was the observed test statistic \\(b_1\\)? We could calculate this statistic two ways:\n\nfitting a lm() and grabbing the coefficients using get_regression_table()\nusing infer to calculate() the observed statistic\n\nGoing with option two, the process would look like this:\n\nobs_slope &lt;- births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n mage \n0.014"
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#visualize-the-p-value",
    "href": "weeks/chapters/week8-reading.html#visualize-the-p-value",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.5 visualize() the p-value",
    "text": "3.5 visualize() the p-value\nThe final step is to measure how surprised we are by a slope of 0.014 in a hypothesized universe where there is no relationship between a baby’s birth weight and a mother’s age. If the observed slope of 0.014 is highly unlikely, then we would be inclined to reject the validity of our hypothesized universe.\nWe start by visualizing the null distribution of our 1000 values of \\(b_1\\) using visualize() in Figure 7. Recall that these are values of the sample slope assuming \\(H_0\\) is true.\n\n\n\n\n\n\n\n\nFigure 7: Null distribution of slope statistic.\n\n\n\n\n\nLet’s now add what happened in real life to Figure 7, the observed slope for the relationship between baby birth weights and mother’s ages of 0.014. Instead of merely adding a vertical line using geom_vline(), let’s use the shade_p_value() function with obs_stat set to the observed test statistic (observed slope statistic) value we saved in obs_slope.\nFurthermore, we’ll set the direction = \"two-sided\" reflecting our alternative hypothesis \\(H_A: \\beta_1 \\neq 0\\), stating that there is a relationship between a baby’s birth weight and a mother’s age. As stated in Section 2, a two-sided hypothesis test does not make any assumptions about the direction of the relationship, meaning \\(\\beta_1 &lt; 0\\) and \\(\\beta_1 &gt; 0\\) are equally plausible. So, when calculating statistics that are “more extreme” than what we observed, we need to look in both tails.\n\nvisualize(null_distribution, bins = 10) + \n  labs(x = \"Permuted (Shuffled) Slope Statistic\") +\n  shade_p_value(obs_stat = obs_slope, direction = \"two-sided\")\n\n\n\n\n\n\n\nFigure 8: Shaded histogram to show \\(p\\)-value.\n\n\n\n\n\nIn the resulting Figure 8, the solid dark line marks 0.014. However, what does the shaded-region correspond to? This is the \\(p\\)-value. Recall the definition of the \\(p\\)-value from Section 2:\n\nA \\(p\\)-value is the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nSo judging by the shaded region in Figure 8, it seems we would somewhat rarely observe a slope statistic of 0.014 or more extreme in a hypothesized universe where there is no relationship between a baby’s birth weight and a mother’s age. In other words, the \\(p\\)-value is somewhat small. Hence, we might be inclined to reject this hypothesized universe, or using statistical language we would “reject \\(H_0\\).”\nWhat fraction of the null distribution is shaded? In other words, what is the exact value of the \\(p\\)-value? We can compute it using the get_p_value() function with the same arguments as the previous shade_p_value() code:\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.054\n\n\nKeeping the definition of a \\(p\\)-value in mind, the probability of observing a slope statistic as large as 0.014 or something more extreme due to sampling variation alone in the null distribution is 0.054 = 5.4%. Since this \\(p\\)-value is larger than our pre-specified significance level \\(\\alpha\\) = 0.05, we fail to reject the null hypothesis \\(H_0: \\beta_1 = 0\\). In other words, this \\(p\\)-value is not small enough to reject our hypothesized universe where there is no relationship between a baby’s birth weight and a mother’s age. Notice how our interpretation of this p-value does not state that we accept the null hypothesis. Rather, we have insufficient evidence to reject it. I will discuss these differences more in Section 4.1.\nObserve that whether we reject the null hypothesis \\(H_0\\) or not depends in large part on our choice of significance level \\(\\alpha\\), as if we had chosen an \\(\\alpha\\) of 0.1, we would have had sufficient evidence to reject the null hypothesis. We’ll discuss this more in Section 4.2."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-trial",
    "href": "weeks/chapters/week8-reading.html#sec-trial",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "4.1 Two possible outcomes",
    "text": "4.1 Two possible outcomes\nIn Section 2, we mentioned that given a pre-specified significance level \\(\\alpha\\) there are two possible outcomes of a hypothesis test:\n\nIf the \\(p\\)-value is less than \\(\\alpha\\), then we reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\).\nIf the \\(p\\)-value is greater than or equal to \\(\\alpha\\), we fail to reject the null hypothesis \\(H_0\\).\n\nUnfortunately, the latter result is often misinterpreted as “accepting the null hypothesis \\(H_0\\).” While at first glance it may seem that the statements “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent, there actually is a subtle difference. Saying that we “accept the null hypothesis \\(H_0\\)” is equivalent to stating that “we think the null hypothesis \\(H_0\\) is true.” However, saying that we “fail to reject the null hypothesis \\(H_0\\)” is saying something else: “While \\(H_0\\) might still be false, we don’t have enough evidence to say so.” In other words, there is an absence of enough proof. However, the absence of proof is not proof of absence. 🧐\nTo further shed light on this distinction, let’s use the United States criminal justice system as an analogy. A criminal trial in the United States is a similar situation to hypothesis tests whereby a choice between two contradictory claims must be made about a defendant who is on trial:\n\nThe defendant is truly either “innocent” or “guilty.”\nThe defendant is presumed “innocent until proven guilty.”\nThe defendant is found guilty only if there is strong evidence that the defendant is guilty. The phrase “beyond a reasonable doubt” is often used as a guideline for determining a cutoff for when enough evidence exists to find the defendant guilty.\nThe defendant is found to be either “not guilty” or “guilty” in the ultimate verdict.\n\nIn other words, not guilty verdicts are not suggesting the defendant is innocent, but instead that “while the defendant may still actually be guilty, there wasn’t enough evidence to prove this fact.” Now let’s make the connection with hypothesis tests:\n\nEither the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_A\\) is true.\nHypothesis tests are conducted assuming the null hypothesis \\(H_0\\) is true.\nWe reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\) only if the evidence found in the sample suggests that \\(H_A\\) is true. The significance level \\(\\alpha\\) is used as a guideline to set the threshold on just how strong of evidence we require.\nWe ultimately decide to either “fail to reject \\(H_0\\)” or “reject \\(H_0\\).”\n\nSo while gut instinct may suggest “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent statements, they are not. “Accepting \\(H_0\\)” is equivalent to finding a defendant innocent. However, courts do not find defendants “innocent,” but rather they find them “not guilty.” Putting things differently, defense attorneys do not need to prove that their clients are innocent, rather they only need to prove that clients are not “guilty beyond a reasonable doubt”.\nSo going back to the births14 dataset, recall that our hypothesis test was \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\) and that we used a pre-specified significance level of \\(\\alpha\\) = 0.05. We found a \\(p\\)-value of 0.054. Since the \\(p\\)-value was smaller than \\(\\alpha\\) = 0.05, we rejected \\(H_0\\). In other words, we found needed levels of evidence in this particular sample to say that \\(H_0\\) is false at the \\(\\alpha\\) = 0.05 significance level. We also state this conclusion using non-statistical language: we found enough evidence in this data to suggest that there was gender discrimination at play."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-errors",
    "href": "weeks/chapters/week8-reading.html#sec-errors",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "4.2 Types of errors",
    "text": "4.2 Types of errors\nUnfortunately, there is some chance a jury or a judge can make an incorrect decision in a criminal trial by reaching the wrong verdict. For example, finding a truly innocent defendant “guilty”. Or on the other hand, finding a truly guilty defendant “not guilty.” This can often stem from the fact that prosecutors don’t have access to all the relevant evidence, but instead are limited to whatever evidence the police can find.\nThe same holds for hypothesis tests. We can make incorrect decisions about a population parameter because we only have a sample of data from the population and thus sampling variation can lead us to incorrect conclusions.\nThere are two possible erroneous conclusions in a criminal trial: either (1) a truly innocent person is found guilty or (2) a truly guilty person is found not guilty. Similarly, there are two possible errors in a hypothesis test: either (1) rejecting \\(H_0\\) when in fact \\(H_0\\) is true, called a Type I error or (2) failing to reject \\(H_0\\) when in fact \\(H_0\\) is false, called a Type II error. Another term used for “Type I error” is “false positive,” while another term for “Type II error” is “false negative.”\nThis risk of error is the price researchers pay for basing inference on a sample instead of performing a census on the entire population. But as we’ve seen in our numerous examples and activities so far, censuses are often very expensive and other times impossible, and thus researchers have no choice but to use a sample. Thus in any hypothesis test based on a sample, we have no choice but to tolerate some chance that a Type I error will be made and some chance that a Type II error will occur.\nTo help understand the concepts of Type I error and Type II errors, we apply these terms to our criminal justice analogy in Figure 9.\n\n\n\n\n\n\nFigure 9: Type I and Type II errors in criminal trials.\n\n\n\nThus a Type I error corresponds to incorrectly putting a truly innocent person in jail, whereas a Type II error corresponds to letting a truly guilty person go free. Let’s show the corresponding table in Figure 10 for hypothesis tests.\n\n\n\n\n\n\nFigure 10: Type I and Type II errors in hypothesis tests."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-choosing-alpha",
    "href": "weeks/chapters/week8-reading.html#sec-choosing-alpha",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "4.3 How do we choose alpha?",
    "text": "4.3 How do we choose alpha?\nIf we are using a sample to make inferences about a population, we run the risk of making errors. For confidence intervals, a corresponding “error” would be constructing a confidence interval that does not contain the true value of the population parameter. For hypothesis tests, this would be making either a Type I or Type II error. Obviously, we want to minimize the probability of either error; we want a small probability of making an incorrect conclusion:\n\nThe probability of a Type I Error occurring is denoted by \\(\\alpha\\). The value of \\(\\alpha\\) is called the significance level of the hypothesis test, which we defined in Section 2.\nThe probability of a Type II Error is denoted by \\(\\beta\\). The value of \\(1-\\beta\\) is known as the power of the hypothesis test.\n\nIn other words, \\(\\alpha\\) corresponds to the probability of incorrectly rejecting \\(H_0\\) when in fact \\(H_0\\) is true. On the other hand, \\(\\beta\\) corresponds to the probability of incorrectly failing to reject \\(H_0\\) when in fact \\(H_0\\) is false.\nIdeally, we want \\(\\alpha = 0\\) and \\(\\beta = 0\\), meaning that the chance of making either error is 0. However, this can never be the case in any situation where we are sampling for inference. There will always be the possibility of making either error when we use sample data. Furthermore, these two error probabilities are inversely related. As the probability of a Type I error goes down, the probability of a Type II error goes up.\nWhat is typically done in practice is to fix the probability of a Type I error by pre-specifying a significance level \\(\\alpha\\) and then try to minimize \\(\\beta\\). In other words, we will tolerate a certain fraction of incorrect rejections of the null hypothesis \\(H_0\\), and then try to minimize the fraction of incorrect non-rejections of \\(H_0\\).\nSo for example if we used \\(\\alpha\\) = 0.01, we would be using a hypothesis testing procedure that in the long run would incorrectly reject the null hypothesis \\(H_0\\) one percent of the time. This is analogous to setting the confidence level of a confidence interval.\nSo what value should you use for \\(\\alpha\\)? Different fields have different conventions, but some commonly used values include 0.10, 0.05, 0.01, and 0.001. However, it is important to keep in mind that if you use a relatively small value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have a harder time being less than \\(\\alpha\\). Thus we would reject the null hypothesis less often. In other words, we would reject the null hypothesis \\(H_0\\) only if we have very strong evidence to do so. This is known as a “conservative” test.\nOn the other hand, if we used a relatively large value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have an easier time being less than \\(\\alpha\\). Thus we would reject the null hypothesis more often. In other words, we would reject the null hypothesis \\(H_0\\) even if we only have mild evidence to do so. This is known as a “liberal” test."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#statistical-versus-practical-significance",
    "href": "weeks/chapters/week8-reading.html#statistical-versus-practical-significance",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "5.1 Statistical versus practical significance",
    "text": "5.1 Statistical versus practical significance\nYou might be wondering, how could we have rejected the null hypothesis that \\(\\beta_1 = 0\\), when the lower bound of our confidence interval is really close to 0. That’s a great question! This question gets at the heart of the difference between statistical and practical significance.\nStatistical significance is defined by a \\(p\\)-value being smaller than some predetermined \\(\\alpha\\) threshold. However, the size of a p-value is not synonymous with the “effect size”. By this I mean, a study can obtain a small \\(p\\)-value but have the estimated effect of the variable(s) be so small they would not be practically meaningful. In the context of the births14 data, our hypothesis test concluded that there is a relationship between baby birth weights and mother’s ages. Yet, based on the confidence interval, the estimated increase in birth weight for a 10-year increase in age is at most 0.2 lbs. I would not believe that the medical profession needs to provide medical interventions for younger mothers whose babies would have lower birth weights.\nThis is why I frequently favor confidence intervals over hypothesis tests, as they allow for you to assess both the statistical significance–by checking if the null hypothesized value is included in the interval—and the practical significance–by assessing the size of the estimated effect."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#regression-interp",
    "href": "weeks/chapters/week8-reading.html#regression-interp",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "6.1 Interpreting regression tables",
    "text": "6.1 Interpreting regression tables\nPreviously, when we interpreted a regression table, like the one shown in Table 4, we focused entirely on the term and estimate columns. Let’s now shift our attention to the remaining columns: std_error, statistic, p_value, lower_ci and upper_ci.\n\n\n\n\nTable 4: Regression table for estimated coefficients from regression on birth weight and mother’s age.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n6.793\n0.208\n32.651\n0.000\n6.385\n7.201\n\n\nmage\n0.014\n0.007\n1.987\n0.047\n0.000\n0.028\n\n\n\n\n\n\n\n\n\n\nR, the programming language has been around since roughly 1997, a period when computers were not able to run the simulations we can today. Thus, the calculations R provides in the std_error, statistic, p_value, lower_ci and upper_ci columns are not found using simulation-based methods. Rather, R uses a theory-based approach using mathematical formulas. These formulas were derived in a time when computers didn’t exist, so it would’ve been incredibly labor intensive to run extensive simulations.\n\n6.1.1 Standard error\nThe third column of the regression table in Table 4, std_error, corresponds to the standard error of our estimates. Recall the definition of standard error we saw last week:\n\nThe standard error is the standard deviation of any point estimate computed from a sample.\n\nSo what does this mean in terms of the fitted slope \\(b_1\\) = 0.014? This value is just one possible value of the fitted slope resulting from this particular sample of \\(n\\) = 1000 birth records. However, if we collected a different sample of \\(n\\) = 1000 birth records, we will almost certainly obtain a different fitted slope \\(b_1\\). This is due to sampling variability.\nSay we hypothetically collected 1000 such birth records, computed the 1000 resulting values of the fitted slope \\(b_1\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b_1\\), which we defined last week. Further recall that the standard deviation of the sampling distribution of \\(b_1\\) has a special name: the standard error.\nLast week, we used the infer package to construct the bootstrap distribution for \\(b_1\\) in this case. Recall that the bootstrap distribution is an approximation to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar standard errors. However, unlike the sampling distribution, the bootstrap distribution is constructed from a single sample, which is a practice more aligned with what’s done in real life.\nRather than resampling from our original sample to create a bootstrap distribution, we could have instead used theory-based methods to estimated the standard error of the sampling distribution. In particular, there is a formula for the standard error of the fitted slope \\(b_1\\):\n\\[\\text{SE}_{b_1} = \\dfrac{\\dfrac{s_y}{s_x} \\cdot \\sqrt{1-r^2}}{\\sqrt{n-2}}\\]\nAs with many formulas in statistics, there’s a lot going on here, so let’s first break down what each symbol represents. First \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the explanatory variable mage and the response variable weight, respectively. Second, \\(r\\) is the sample correlation coefficient between mage and weight. This was computed as 0.063. Lastly, \\(n\\) is the number of pairs of points (birth weight, mother’s age) in the births14 data frame, here 1000.\nTo put this formula into words, the standard error of \\(b_1\\) depends on the relationship between the variability of the response variable and the variability of the explanatory variable as measured in the \\(s_y / s_x\\) term. Next, it looks into how the two variables relate to each other in the \\(\\sqrt{1-r^2}\\) term.\nHowever, the most important observation to make in the previous formula is that there is an \\(n - 2\\) in the denominator. In other words, as the sample size \\(n\\) increases, the standard error \\(\\text{SE}_{b_1}\\) decreases. Just as we demonstrated last week, when we increase our sample size the amount of sampling variation of the fitted slope \\(b_1\\) will depend on the sample size \\(n\\). In particular, as the sample size increases, both the sampling and bootstrap distributions narrow and the standard error \\(\\text{SE}_{b_1}\\) decreases. Hence, our estimates of \\(b_1\\) for the true population slope \\(\\beta_1\\) get more and more precise.\n\n\n6.1.2 Test statistic\nThe fourth column of the regression table in Table 4, statistic, corresponds to a test statistic relating to the following hypothesis test:\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_A: \\beta_1 \\neq 0\\)\n\nHere, our null hypothesis \\(H_0\\) assumes that the population slope \\(\\beta_1\\) is 0. If the population slope \\(\\beta_1\\) is truly 0, then this is saying that there is no relationship between a baby’s birth weight and a mother’s age for all births in the US in 2014. In other words, \\(x\\) = mother’s age would have no associated effect on \\(y\\) = baby’s birth weight.\nThe alternative hypothesis \\(H_A\\), on the other hand, assumes that the population slope \\(\\beta_1\\) is not 0, meaning it could be either positive or negative. This suggests either a positive or negative relationship between a baby’s birth weight and a mother’s age. Recall we called such alternative hypotheses two-sided.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBy convention, all hypothesis testing for regression assumes two-sided alternatives.\n\n\nThe statistic column in the regression table is a tricky one, however. It corresponds to a standardized t-test statistic. The null distribution can be mathematically proven to be a \\(t\\)-distribution, under specific conditions (described in Section 7). R uses the following \\(t\\)-statistic as the test statistic for hypothesis testing:\n\\[\nt = \\dfrac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}}\n\\]\nAnd since the null hypothesis \\(H_0: \\beta_1 = 0\\) is assumed during the hypothesis test, the \\(t\\)-statistic becomes\n\\[\nt = \\dfrac{ b_1 - 0}{ \\text{SE}_{b_1}} = \\dfrac{ b_1 }{ \\text{SE}_{b_1}}\n\\]\nWhat are the values of \\(b_1\\) and \\(\\text{SE}_{b_1}\\)? They are in the estimate and std_error column of the regression table in Table 4. Thus the value of 1.987 in the table is computed as 0.014 / 0.007 = 2. Note there is a difference due to some rounding error here.\n\n\n6.1.3 p-value\nThe fifth column of the regression table in Table 4, p_value, corresponds to the p-value of the hypothesis test \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\).\nAgain recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section 2, let’s focus on the definition of the \\(p\\)-value:\n\nA p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nRecall that you can intuitively think of the \\(p\\)-value as quantifying how “extreme” the observed fitted slope of \\(b_1\\) = 0.014 is in a “hypothesized universe” where there is no relationship between a baby’s birth weight and a mother’s age.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re a bit rusty on the \\(t\\)-distribution, here is an article describing how degrees of freedom relate to the shape of the \\(t\\)-distribution.\n\n\nMore precisely, however, the \\(p\\)-value corresponds to how extreme the observed test statistic of 0.014 is when compared to the appropriate null distribution. Recall from Section 2, that a null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. It can be mathematically proven that a \\(t\\)-distribution with degrees of freedom equal to \\(df = n - 2 = 1000 - 2 = 998\\) is a reasonable approximation to the null distribution, if certain conditions for inference are not violated. We will discuss these conditions shortly in Section 7.\n\n\n6.1.4 Confidence interval\nThe two rightmost columns of the regression table in Table 4, lower_ci and upper_ci, correspond to the endpoints of the 95% confidence interval for the population slope \\(\\beta_1\\). What are the calculations that went into computing the two endpoints of the 95% confidence interval for \\(\\beta_1\\)?\nRecall from the SE method for constructing confidence intervals, for any distribution that resemble a Normal Distribution2, we could use the empiracle rule to create a 95% confidence interval:\n\n\\(b_1 \\pm SE_{b_1} \\times\\) 1.96\n\n\nWhat is the value of the standard error \\(SE_{b_1}\\)? It is in fact in the third column of the regression table in Table 4: 0.007. Plugging in the respective values, we have:\n\n0.014 \\(\\pm\\) 0.007 \\(\\times\\) 1.96\n0.014 \\(\\pm\\) 0.01372\n(0.00028, 0.02772)\n\n\nThis closely matches the \\((0,0.028)\\) confidence interval in the last two columns of Table 4, with slight differences due to rounding.\nMuch like p-values, the results of this confidence interval also are only valid if the conditions for inference are not violated."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#conclusion",
    "href": "weeks/chapters/week8-reading.html#conclusion",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "6.2 Conclusion",
    "text": "6.2 Conclusion\nDon’t worry if you’re feeling a little overwhelmed at this point. There is a lot of background theory to understand before you can fully make sense of the equations for theory-based methods. That being said, theory-based methods and simulation-based methods for constructing confidence intervals and conducting hypothesis tests often yield consistent results. As mentioned before, in my opinion, two large benefits of simulation-based methods over theory-based are that (1) they are easier for people new to statistical inference to understand, and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#everything-is-one-test",
    "href": "weeks/chapters/week8-reading.html#everything-is-one-test",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "6.3 Everything is one test",
    "text": "6.3 Everything is one test\nWhile this is a lot to digest, especially the first time you encounter hypothesis testing, the nice thing is that once you understand this general framework, then you can understand any hypothesis test. In a famous blog post, computer scientist Allen Downey called this the “There is only one test” framework, for which he created the flowchart displayed in Figure 13.\n\n\n\n\n\n\nFigure 13: Allen Downey’s hypothesis testing framework.\n\n\n\nNotice its similarity with the “hypothesis testing with infer” diagram you saw in Figure 6. That’s because the infer package was explicitly designed to match the “There is only one test” framework. So if you can understand the framework, you can easily generalize these ideas for all hypothesis testing scenarios."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#conditions-for-regression",
    "href": "weeks/chapters/week8-reading.html#conditions-for-regression",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "7.1 Conditions for regression",
    "text": "7.1 Conditions for regression\nFor inference for regression, there are four conditions that cannot be violated. Note the first four letters of these conditions are highlighted in bold in what follows: LINE. This can serve as a nice reminder of what to check for whenever you perform linear regression.\n\nLinearity of relationship between variables\nIndependence of the residuals\nNormality of the residuals\nEquality of variance of the residuals\n\nConditions L, N, and E can be verified through what is known as a residual analysis. Condition I can only be verified through an understanding of how the data was collected.\n\n7.1.1 Residuals refresher\nRecall our definition of a residual: it is the observed value minus the fitted value denoted by \\(y - \\widehat{y}\\). Recall that residuals can be thought of as the error or the “lack-of-fit” between the observed value \\(y\\) and the fitted value \\(\\widehat{y}\\) on the regression line in Figure 1. In Figure 14, we illustrate one particular residual out of 1000 using an arrow, as well as its corresponding observed and fitted values using a circle and a square, respectively.\n\n\n\n\n\n\n\n\nFigure 14: Example of observed value, fitted value, and residual.\n\n\n\n\n\nFurthermore, we can automate the calculation of all \\(n\\) = 1000 residuals by applying the get_regression_points() function to our saved regression model in births_lm. Observe how the resulting values of residual are roughly equal to mage - mage_hat (there is potentially a slight difference due to rounding error).\n\n# Fit regression model:\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n# Get regression points:\nregression_points &lt;- get_regression_points(births_lm)\nregression_points\n\n# A tibble: 1,000 × 5\n      ID weight  mage weight_hat residual\n   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   6.96    34       7.28   -0.317\n 2     2   8.86    31       7.23    1.63 \n 3     3   7.51    36       7.31    0.204\n 4     4   6.19    16       7.02   -0.831\n 5     5   6.75    31       7.23   -0.484\n 6     6   6.69    26       7.16   -0.473\n 7     7   6.13    36       7.31   -1.18 \n 8     8   6.74    24       7.14   -0.395\n 9     9   8.94    32       7.25    1.69 \n10    10   9.12    26       7.16    1.96 \n# ℹ 990 more rows\n\n\nA residual analysis is used to verify conditions L, N, and E and can be performed using appropriate data visualizations. While there are more sophisticated statistical approaches that can also be done, we’ll focus on the much simpler approach of looking at plots.\n\n\n7.1.2 Linearity of relationship\nThe first condition is that the relationship between the outcome variable \\(y\\) and the explanatory variable \\(x\\) must be Linear. Recall the scatterplot in Figure 1 where we had the explanatory variable \\(x\\) as mother’s age and the outcome variable \\(y\\) as baby’s birth weight. Would you say that the relationship between \\(x\\) and \\(y\\) is linear? It’s hard to say because of the scatter of the points about the line. In my opinion, this relationship is “linear enough.”\nLet’s present an example where the relationship between \\(x\\) and \\(y\\) is clearly not linear in Figure 15. In this case, the points clearly follow a curved relationship, more closely resembling a logarithmic scale. In this case, any results from an inference for regression would not be valid.\n\n\n\n\n\n\n\n\nFigure 15: Example of a clearly non-linear relationship.\n\n\n\n\n\n\n\n7.1.3 Independence of residuals\nThe second condition is that the residuals must be Independent. In other words, the different observations in our data must be independent of one another.\nFor the evals dataset, while there is data on 463 courses, these 463 courses were actually taught by 94 unique instructors. In other words, the same professor is often included more than once in our data. For a professor in the evals data who taught multiple classes, it seems reasonable to expect that their teaching scores will be related to each other. If a professor gets a high score in one class, chances are fairly good they’ll get a high score in another. This dataset thus provides different information than if we had 463 unique instructors teaching the 463 courses.\nIn this case, we say there exists dependence between observations. So in this case, the independence condition is violated. The most appropriate analysis would take into account that we have repeated measures for the same profs, but that is beyond the scope of this class. What we could do, however, is devise a method to collapse the multiple observations for each professor into one observation. This could be done a variety of ways:\n\nrandomly selecting one observation per professor\ntaking the mean of their scores3\nselecting the maximum / minimum / median value\n\n\n\n7.1.4 Normality of residuals\nThe third condition is that the residuals should follow a Normal distribution. Furthermore, the center of this distribution should be 0. In other words, sometimes the regression model will make positive errors: \\(y - \\widehat{y} &gt; 0\\). Other times, the regression model will make equally negative errors: \\(y - \\widehat{y} &lt; 0\\). However, on average the errors should equal 0 and their shape should be similar to that of a bell.\nThe simplest way to check the normality of the residuals is to look at a histogram, which we visualize in Figure 16.\n\nregression_points &lt;- get_regression_points(births_lm)\n\nggplot(data = regression_points, \n       mapping = aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\")\n\n\n\n\n\n\n\n\n\nFigure 16: Histogram of residuals from regression model for baby birth weight as predicted by mother’s age.\n\n\n\n\n\nThis histogram shows that we have more negative residuals than negative. Since the residual \\(y-\\widehat{y}\\) is negative when \\(y &lt; \\widehat{y}\\), it seems our regression model’s fitted baby birth weights (\\(\\widehat{y}\\)) tend to overestimate the birth weight \\(y\\). Furthermore, this histogram has a left-skew in that there is a longer tail on the left. This is another way to say the residuals exhibit a negative skew.\nIs this a problem? Again, there is a certain amount of subjectivity in the response. In my opinion, while there is a slight skew to the residuals, I don’t believe it is so bad that I would say this condition is violated. On the other hand, others might disagree with our assessment.\nLet’s present examples where the residuals clearly do and don’t follow a normal distribution in Figure 17. In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid.\n\n\n\n\n\n\n\n\nFigure 17: Example of clearly normal and clearly not normal residuals.\n\n\n\n\n\n\n\n7.1.5 Equality of variance\nThe fourth and final condition is that the residuals should exhibit Equal variance across all values of the explanatory variable \\(x\\). In other words, the value and spread of the residuals should not depend on the value of the explanatory variable \\(x\\).\nRecall the scatterplot in Figure 1: we had the explanatory variable \\(x\\) of mother’s age on the x-axis and the outcome variable \\(y\\) of baby’s birth weight on the y-axis. Instead, let’s create a scatterplot that has the same values on the x-axis, but now with the residual \\(y-\\widehat{y}\\) on the y-axis as seen in Figure 18.\n\nggplot(data = regression_points, \n       mapping = aes(x = mage, y = residual)) +\n  geom_point() +\n  labs(x = \"Mother's Age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1)\n\n\n\n\n\n\n\n\n\nFigure 18: Plot of residuals over mother’s age.\n\n\n\n\n\nYou can think of Figure 18 a modified version of the plot with the regression line in Figure 1, but with the regression line flattened out to \\(y=0\\). Looking at this plot, would you say that the spread of the residuals around the line at \\(y=0\\) is constant across all values of the explanatory variable \\(x\\) of mother’s age? This question is rather qualitative and subjective in nature, thus different people may respond with different answers. For example, some people might say that there is slightly more variation in the residuals for ages between 20 and 40 than for ages below 20 and above 40. However, it can be argued that there isn’t a drastic non-constancy. Moreover, this could be an artifact of having smaller numbers of younger / older mothers in the sample.\nIn Figure 19 I present an example where the residuals clearly do not have equal variance across all values of the explanatory variable \\(x\\).\n\n\n\n\n\n\n\n\nFigure 19: Example of clearly non-equal variance.\n\n\n\n\n\nObserve how the spread of the residuals increases as the value of \\(x\\) increases. This is a situation known as heteroskedasticity. Any inference for regression based on a model yielding such a pattern in the residuals would not be valid.\n\n\n7.1.6 What’s the conclusion?\nLet’s list our four conditions for inference for regression again and indicate whether or not they were violated in our analysis:\n\nLinearity of relationship between variables: No\nIndependence of residuals: No\nNormality of residuals: Slight left-skew, but not too concerning\nEquality of variance: No\n\nSo what does this mean for the results of our confidence intervals and hypothesis tests in Section 4? Since none of the conditions were viola ted, we can have faith in the confidence intervals and \\(p\\)-values calculated before. If, however, any of these conditions were violated, we would need to pause and reconsider our model.\nWhen the Independence condition is violated, there exist dependencies between the observations in the dataset. To remedy this, you will need to use a statistical model which accounts for this dependency structure. One such technique is called hierarchical/multilevel modeling, which you may learn about in more advanced statistics courses.\nWhen conditions L, N, E are violated, it often means there is a shortcoming in our model. For example, it may be the case that using only a single explanatory variable is insufficient. We may need to incorporate more explanatory variables in a multiple regression model as we did in previously, or perhaps use a transformation of one or more of your variables, or use an entirely different modeling technique. To learn more about addressing such shortcomings, you’ll have to take a class on or read up on more advanced regression modeling methods.\nThe conditions for inference in regression problems are a key part of regression analysis that are of vital importance to the processes of constructing confidence intervals and conducting hypothesis tests. However, it is often the case with regression analysis in the real world that some of these conditions may be violated. Furthermore, as you saw, there is a level of subjectivity in the residual analyses to verify the L, N, and E conditions. So what can you do? As a statistics educator, I advocate for transparency in communicating all results. This lets the stakeholders of any analysis know about a model’s shortcomings or whether the model is “good enough.” So while this checking of assumptions has lead to some fuzzy “it depends” results, we decided as authors to show you these scenarios to help prepare you for difficult statistical decisions you may need to make down the road."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#footnotes",
    "href": "weeks/chapters/week8-reading.html#footnotes",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI prefer to use the term meaningful.↩︎\nmeaning they are bell shaped and symmetric↩︎\nTaking the mean of observations assumes they are inherently similar, so we shouldn’t calculate the mean of evaluation scores that are very different!↩︎"
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html",
    "href": "weeks/chapters/week7-reading1.html",
    "title": "Week 7 – Exploring Sampling",
    "section": "",
    "text": "This week, we begin our adventure into statistical inference by learning about sampling. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we’ll cover in the second portion of this week’s reading and next week. We will see that the skills you learned during the first three weeks of class, in particular data visualization and data wrangling, will also play an important role in the development of your understanding of sampling.\nThis week’s reading comes primarily from Chater 7 from ModernDive (Kim et al. 2020), with a smattering of my own ideas."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sampling-bowl-activity",
    "href": "weeks/chapters/week7-reading1.html#sampling-bowl-activity",
    "title": "Week 7 – Exploring Sampling",
    "section": "1 Sampling bowl activity",
    "text": "1 Sampling bowl activity\nLet’s start with a hands-on activity.\n\n1.1 What proportion of this bowl’s balls are red?\n\n\n\n\n\n\nFigure 1: A bowl with red and white balls.\n\n\n\nTake a look at the bowl in Figure 1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls.\nLet’s now ask ourselves, what proportion of this bowl’s balls are red?\nOne way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process.\n\n\n1.2 Using the shovel once\n\n\n\n\n\n\nFigure 2: Inserting a shovel into the bowl.\n\n\n\n\n\n\n\n\n\nFigure 3: Removing 50 balls from the bowl.\n\n\n\nInstead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 2. Using the shovel, let’s remove \\(5 \\times 10 = 50\\) balls, as seen in Figure 3.\nObserve that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make.\nHowever, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe?\n\n\n\n\n\n\n\n\n \n\n\n\n\nFigure 4: Repeating sampling activity 33 times.\n\n\n\nWhat if we repeated this activity several times following the process, as shown in Figure 4? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition.\n\n\n1.3 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 balls each.\nCount the number of red balls and thus compute the proportion of the 50 balls that are red.\nReturn the balls into the bowl.\nMix the contents of the bowl a little to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then marks their proportion of their 50 balls that were red in the appropriate bin in a hand-drawn histogram as seen in Figure 5.\n\n\n\n\n\n\nFigure 5: Constructing a histogram of proportions.\n\n\n\nRecall from Week 2 that histograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in Figure 5.\n\n\n\n\n\n\nFigure 6: Hand-drawn histogram of first 10 out of 33 proportions.\n\n\n\nObserve the following in the histogram in Figure 6:\n\nAt the low end, one group removed 50 balls from the bowl with proportion red between 0.20 and 0.25.\nAt the high end, another group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution.\nThe shape of this distribution is somewhat bell-shaped.\n\nLet’s construct this same hand-drawn histogram in R using your data visualization skills that you honed in Week 2! We saved our 33 groups of friends’ results in the tactile_prop_red data frame included in the moderndive package. Run the following to display the first 10 of 33 rows:\n\ntactile_prop_red\n\n# A tibble: 33 × 4\n   group            replicate red_balls prop_red\n   &lt;chr&gt;                &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n 1 Ilyas, Yohan             1        21     0.42\n 2 Morgan, Terrance         2        17     0.34\n 3 Martin, Thomas           3        21     0.42\n 4 Clark, Frank             4        21     0.42\n 5 Riddhi, Karina           5        18     0.36\n 6 Andrew, Tyler            6        19     0.38\n 7 Julia                    7        19     0.38\n 8 Rachel, Lauren           8        11     0.22\n 9 Daniel, Caroline         9        15     0.3 \n10 Josh, Maeve             10        17     0.34\n# ℹ 23 more rows\n\n\nObserve for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red.\nLet’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05 in Figure 7. This is a computerized and complete version of the partially completed hand-drawn histogram you saw in Figure 6. Note that setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. This helps us to more closely align this histogram with the hand-drawn histogram in Figure 6.\n\nggplot(tactile_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", \n       title = \"Distribution of 33 proportions red\") \n\n\n\n\n\n\n\nFigure 7: Distribution of 33 proportions based on 33 samples of size 50.\n\n\n\n\n\n\n\n1.4 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%.\nMoreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 7. This is known as the concept of sampling variation.\nThe purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling:\n\nUnderstanding the effect of sampling variation.\nUnderstanding the effect of sample size on sampling variation.\n\nIn the next section, we’ll mimic the hands-on sampling activity we just performed on a computer. This will allow us not only to repeat the sampling exercise much more than 33 times, but it will also allow us to use shovels with different numbers of slots than just 50.\nAfterwards, we’ll present you with definitions, terminology, and notation related to sampling. As in many disciplines, such necessary background knowledge may seem inaccessible and even confusing at first. However, as with many difficult topics, if you truly understand the underlying concepts and practice, practice, practice, you’ll be able to master them.\nTo close this chapter, we’ll generalize the “sampling from a bowl” exercise to other sampling scenarios and present a theoretical result known as the Central Limit Theorem."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sec-sampling-simulation",
    "href": "weeks/chapters/week7-reading1.html#sec-sampling-simulation",
    "title": "Week 7 – Exploring Sampling",
    "section": "2 Virtual sampling",
    "text": "2 Virtual sampling\nIn the previous section, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel.\n\n2.1 Using the virtual shovel once\nLet’s start by performing the virtual analog of the tactile sampling exercise we performed before. We first need a virtual analog of the bowl seen in Figure Figure 1. To this end, we included a data frame named bowl in the moderndive package. The rows of bowl correspond exactly with the contents of the actual bowl.\n\nbowl\n\n# A tibble: 2,400 × 2\n   ball_ID color\n     &lt;int&gt; &lt;chr&gt;\n 1       1 white\n 2       2 white\n 3       3 white\n 4       4 red  \n 5       5 white\n 6       6 white\n 7       7 red  \n 8       8 white\n 9       9 red  \n10      10 white\n# ℹ 2,390 more rows\n\n\nObserve that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable – none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. You can view the contents of the bowl in RStudio’s data viewer, scrolling through the contents to convince yourself that bowl is indeed a virtual analog of the actual bowl in Figure 1.\nNow that we have a virtual analog of our bowl, we next need a virtual analog to the shovel seen in Figure 3 to generate virtual samples of 50 balls. We’re going to use the rep_sample_n() function included in the infer package. This function allows us to take repeated, or replicated, samples of size n from a given dataset.\nLooking at the code below you should notice:\n\nThe dataset (bowl) is the first argument\nsize corresponds to the size of each sample\nreps corresponds to the number of samples\nreplace corresponds to whether each sampled observation should be replaced after it is drawn\n\n\nvirtual_shovel &lt;- rep_sample_n(bowl,\n                               size = 50, \n                               replace = FALSE,\n                               reps = 1)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1     306 red  \n 2         1     800 red  \n 3         1     996 white\n 4         1    1143 white\n 5         1     874 red  \n 6         1     189 white\n 7         1    2021 white\n 8         1    1093 white\n 9         1    1759 white\n10         1    1413 white\n# ℹ 40 more rows\n\n\nObserve that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color. However, what does the replicate variable indicate? In virtual_shovel’s case, replicate is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to the first repeated/replicated use of the shovel, in our case our first sample. We’ll see shortly that when we “virtually” take 33 samples, replicate will take values between 1 and 33.\nLet’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Week 3. First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function:\n\nmutate(.data = virtual_shovel, \n       is_red = (color == \"red\")\n       )\n\n# A tibble: 50 × 4\n# Groups:   replicate [1]\n   replicate ball_ID color is_red\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; \n 1         1     306 red   TRUE  \n 2         1     800 red   TRUE  \n 3         1     996 white FALSE \n 4         1    1143 white FALSE \n 5         1     874 red   TRUE  \n 6         1     189 white FALSE \n 7         1    2021 white FALSE \n 8         1    1093 white FALSE \n 9         1    1759 white FALSE \n10         1    1413 white FALSE \n# ℹ 40 more rows\n\n\nObserve that for every row where color == \"red\", the Boolean (logical) value TRUE is returned and for every row where color is not equal to \"red\", the Boolean FALSE is returned.\nSecond, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Week 3 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum() function:\n\nmutate(.data = virtual_shovel, \n       is_red = (color == \"red\")\n       ) %&gt;% \nsummarize(num_red = sum(is_red)\n          )\n\n# A tibble: 1 × 2\n  replicate num_red\n      &lt;int&gt;   &lt;int&gt;\n1         1      17\n\n\nWhy does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 17 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling.\nThird and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50:\n\nmutate(.data = virtual_shovel, \n       is_red = (color == \"red\")\n       ) %&gt;% \nsummarize(num_red = sum(is_red) / 50\n          )\n\n# A tibble: 1 × 2\n  replicate num_red\n      &lt;int&gt;   &lt;dbl&gt;\n1         1    0.34\n\n\nGreat! 34% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 34%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 34% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 6. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n\n2.2 Using the virtual shovel 33 times\nRecall that in our tactile sampling exercise, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated / replicated using the shovel 33 times. We can perform this repeated / replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times.\nWe’ll save these results in a data frame called virtual_samples. We provide a preview of the first 10 rows of virtual_samples below:\n\n\n# A tibble: 1,650 × 3\n# Groups:   replicate [33]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1      27 red  \n 2         1    1706 white\n 3         1     538 white\n 4         1    2309 white\n 5         1    1244 red  \n 6         1    1092 red  \n 7         1     894 white\n 8         1    1591 white\n 9         1      38 white\n10         1    2191 red  \n# ℹ 1,640 more rows\n\n\nObserve that the first 50 rows of replicate are equal to 1. If you were to continue scrolling through the dataset (like you can in RStudio’s data previewer), you would find that the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\times\\) 50 = 1650 rows.\nLet’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Week 3 that by assigning the grouping variable before we summarize(), we’ll obtain a different summary statistic for each level of our group variable. Thus, we should end up with 33 different proportions, since we have 33 different replicates. We display a preview of the first 10 out of 33 rows:\n\n\n# A tibble: 33 × 2\n   replicate prop_red\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1     0.44\n 2         2     0.34\n 3         3     0.34\n 4         4     0.44\n 5         5     0.4 \n 6         6     0.34\n 7         7     0.36\n 8         8     0.46\n 9         9     0.48\n10        10     0.26\n# ℹ 23 more rows\n\n\nAs with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram! In Figure 8 we’ve added binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well.\n\nggplot(data = virtual_prop_red, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\") \n\n\n\n\n\n\n\nFigure 8: Distribution of 33 proportions based on 33 samples of size 50.\n\n\n\n\n\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40% (for 11 out of 33 samples). Why do we have these differences in proportions red? Because of sampling variation.\nLet’s now compare our virtual results with our tactile results from the previous section in Figure 9. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\n\n\n\n\n\n\n\nFigure 9: Comparing 33 virtual and 33 tactile proportions red.\n\n\n\n\n\n\n\n2.3 Using the virtual shovel 1000 times\nNow say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000! We have two choices at this point:\n\nWe could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. This would be a tedious and time-consuming task and would likely leave our friends upset with us.\nWe could use the power of R to automate this repetitive task!\n\nI believe all of us would choose option 2, so let’s abandon tactile sampling in favor of only virtual sampling. Once again let’s use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates (reps) set to 1000.\n\nvirtual_samples &lt;- rep_sample_n(bowl, \n                                size = 50, \n                                reps = 1000, \n                                replace = FALSE)\nvirtual_samples\n\n# A tibble: 50,000 × 3\n# Groups:   replicate [1,000]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1    1178 white\n 2         1     212 red  \n 3         1    1023 white\n 4         1    1127 white\n 5         1      38 white\n 6         1     452 red  \n 7         1    2072 white\n 8         1    2254 white\n 9         1     111 white\n10         1     669 white\n# ℹ 49,990 more rows\n\n\nObserve that now virtual_samples has 1000 \\(\\times\\) 50 = 50000 rows, instead of the 33 \\(\\times\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\times\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls.\n\nvirtual_prop_red &lt;- virtual_samples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 50) \n\nvirtual_prop_red\n\n# A tibble: 1,000 × 2\n   replicate prop_red\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1     0.26\n 2         2     0.32\n 3         3     0.26\n 4         4     0.48\n 5         5     0.3 \n 6         6     0.24\n 7         7     0.3 \n 8         8     0.44\n 9         9     0.42\n10        10     0.4 \n# ℹ 990 more rows\n\n\nObserve that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 10.\n\nggplot(virtual_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", \n       title = \"Distribution of 1000 proportions red\") \n\n\n\n\n\n\n\nFigure 10: Distribution of 1000 proportions based on 1000 samples of size 50.\n\n\n\n\n\nOnce again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as low as between 20% and 25%, and others as high as between 55% and 60%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, approximated well by a normal distribution. At this point we recommend you read the “Normal distribution” section (Appendix Section 5) for a brief discussion on the properties of the normal distribution.\n\n\n2.4 Using different shovels\nNow say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100.\n\n\n\n\n\n\n\n\nA shovel with 25 slots\n\n\n\n\n\n\n\nA shovel with 50 slots\n\n\n\n\n\n\n\n\n\nA shovel with 100 slots\n\n\n\n\n\n\nFigure 11: Three shovels to extract three different sample sizes.\n\n\n\nIf your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection.\nUsing our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000:\n\nVirtually use the appropriate shovel to generate 1000 samples with size balls.\nCompute the resulting 1000 replicates of the proportion of the shovel’s balls that are red.\nVisualize the distribution of these 1000 proportions red using a histogram.\n\nRun each of the following code segments individually and then compare the three resulting histograms.\n\n# Segment 1: sample size = 25 ------------------------------\n# 1.a) Virtually use shovel 1000 times\nvirtual_samples_25 &lt;- rep_sample_n(bowl, \n                                   size = 25, \n                                   reps = 1000, \n                                   replace = FALSE)\n\n# 1.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 25)\n\n# 1.c) Plot distribution via a histogram\nggplot(data = virtual_prop_red_25, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 25 balls that were red\") \n\n# Segment 2: sample size = 50 ------------------------------\n# 2.a) Virtually use shovel 1000 times\nvirtual_samples_50 &lt;- rep_sample_n(bowl, \n                                   size = 50, \n                                   reps = 1000, \n                                   replace = FALSE)\n\n# 2.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") /50) \n\n# 2.c) Plot distribution via a histogram\nggplot(data = virtual_prop_red_50, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\")  \n\n# Segment 3: sample size = 100 ------------------------------\n# 3.a) Virtually using shovel with 100 slots 1000 times\nvirtual_samples_100 &lt;- rep_sample_n(bowl, \n                                    size = 100, \n                                    reps = 1000)\n\n# 3.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 100) \n\n# 3.c) Plot distribution via a histogram\nggplot(data = virtual_prop_red_100, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 100 balls that were red\") \n\n\n\n\n\n\n\n\n\n\n\n(a) n = 25\n\n\n\n\n\n\n\n\n\n\n\n(b) n = 50\n\n\n\n\n\n\n\n\n\n\n\n(c) n = 100\n\n\n\n\n\n\n\nFigure 12: Comparing the distributions of proportion red for different sample sizes.\n\n\n\n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure Figure 12.\nObserve that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 12, all three histograms appear to center around roughly 40%.\nWe can be numerically explicit about the amount of variation in our three sets of 1000 values of prop_red using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable (see Appendix Section 5) for a brief discussion on the properties of the standard deviation. For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function.\n\n# n = 25\nvirtual_prop_red_25 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# A tibble: 1 × 1\n      sd\n   &lt;dbl&gt;\n1 0.0945\n\n# n = 50\nvirtual_prop_red_50 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# A tibble: 1 × 1\n      sd\n   &lt;dbl&gt;\n1 0.0671\n\n# n = 100\nvirtual_prop_red_100 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# A tibble: 1 × 1\n      sd\n   &lt;dbl&gt;\n1 0.0482\n\n\nLet’s compare these three measures of distributional variation in Table 1.\n\n\n\n\nTable 1: Comparing standard deviations of proportions red for three different shovels.\n\n\n\n\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.095\n\n\n50\n0.067\n\n\n100\n0.048"
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sampling-framework",
    "href": "weeks/chapters/week7-reading1.html#sampling-framework",
    "title": "Week 7 – Exploring Sampling",
    "section": "3 Sampling framework",
    "text": "3 Sampling framework\nIn both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 12 and Table 1 – comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nNow that you have built some intuition relating to sampling, let’s now attach words and labels to the various concepts we’ve explored so far. Specifically in the next section, we’ll introduce terminology and notation as well as statistical definitions related to sampling. This will allow us to succinctly summarize and refer to the ideas behind sampling for the rest of this book.\n\n3.1 Terminology and notation\nLet’s now attach words and labels to the various sampling concepts we’ve seen so far by introducing some terminology and mathematical notation. While they may seem daunting at first, we’ll make sure to tie each of them to sampling bowl activities you performed earlier. Furthermore, throughout this book we’ll give you plenty of opportunity for practice, as the best method for mastering these terms is repetition.\nThe first set of terms and notation relate to populations:\n\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\).\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Canadians, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all \\(N\\) individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number \\(N\\) of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\nSo in our sampling activities, the population is the collection of \\(N\\) = 2400 identically sized red and white balls in the bowl shown in Figure 1. Recall that we also represented the bowl “virtually” in the data frame bowl:\n\nbowl\n\n# A tibble: 2,400 × 2\n   ball_ID color\n     &lt;int&gt; &lt;chr&gt;\n 1       1 white\n 2       2 white\n 3       3 white\n 4       4 red  \n 5       5 white\n 6       6 white\n 7       7 red  \n 8       8 white\n 9       9 red  \n10      10 white\n# ℹ 2,390 more rows\n\n\nThe population parameter here is the proportion of the bowl’s balls that are red. Whenever we’re interested in a proportion of some value in a population, the population parameter has a specific name: the population proportion. We denote population proportions with the letter \\(p\\). We’ll see later on that we can also consider other types of population parameters, like population means and population slopes.\nIn order to compute this population proportion \\(p\\) exactly, we need to first conduct a census by going through all \\(N\\) = 2400 and counting the number that are red. We then divide this count by 2400 to obtain the proportion red.\nYou might be now asking yourself: “Wait. I understand that performing a census on the actual bowl would take a long time. But can’t we conduct a ‘virtual’ census using the virtual bowl?” You are absolutely correct! In fact when the ModernDive authors created the bowl data frame, they made its contents match the contents of actual bowl not by doing a census, but by reading the contents written on the box the bowl came in!\nLet’s conduct this “virtual” census by using the same dplyr verbs you used earlier to count the number of balls that are red:\n\nsummarize(bowl, \n          total_red = sum(color == \"red\")\n          ) \n\n# A tibble: 1 × 1\n  total_red\n      &lt;int&gt;\n1       900\n\n\nSince 900 of the 2400 are red, the proportion is 900/2400 = 0.375 = 37.5%. So we know the value of the population parameter: in our case, the population proportion \\(p\\) is equal to 0.375.\nAt this point, you might be further asking yourself: “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Great question! Normally, you wouldn’t do any sampling! However, the sampling activities we did this chapter are merely simulations of how sampling is done in real-life! We perform these simulations in order to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nIn real-life sampling not only will the population size \\(N\\) be very large making a census expensive, but sometimes we won’t even know how big the population is! For now however, we press on with our next set of terms and notation.\nThe second set of terms and notation relate to samples:\n\nSampling is the act of collecting a sample from the population, which we generally only do when we can’t perform a census. We mathematically denote the sample size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census.\nA point estimate, also known as a sample statistic, is a summary statistic computed from a sample that estimates the unknown population parameter.\n\nSo previously we conducted sampling using a shovel with 50 slots to extract samples of size \\(n\\) = 50. To perform the virtual analog of this sampling, recall that we used the rep_sample_n() function as follows:\n\nvirtual_shovel &lt;- rep_sample_n(bowl, \n                               size = 50, \n                               reps = 1, \n                               replace = FALSE)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1     231 red  \n 2         1    1308 red  \n 3         1    1306 white\n 4         1    2274 red  \n 5         1    1627 white\n 6         1    2195 white\n 7         1     527 red  \n 8         1    1744 white\n 9         1    1054 red  \n10         1    1223 red  \n# ℹ 40 more rows\n\n\nUsing the sample of 50 balls contained in virtual_shovel, we generated an estimate of the proportion of the bowl’s balls that are red prop_red\n\nvirtual_shovel %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 50) \n\n# A tibble: 1 × 2\n  replicate prop_red\n      &lt;int&gt;    &lt;dbl&gt;\n1         1     0.44\n\n\nSo in our case, the value of prop_red is the point estimate of the population proportion \\(p\\) since it estimates the latter’s value. Furthermore, this point estimate has a specific name when considering proportions: the sample proportion. It is denoted using \\(\\widehat{p}\\) because it is a common convention in statistics to use a “hat” symbol to denote point estimates.\nThe third set of terms relate to sampling methodology: the method used to collect samples. You’ll see here and throughout the rest of your book that the way you collect samples directly influences their quality.\n\nA sample is said to be representative if it roughly “looks like” the population. In other words, if the sample’s characteristics are a “good” representation of the population’s characteristics.\nWe say a sample is generalizable if any results based on the sample can generalize to the population. In other words, if we can make “good” guesses about the population using the sample.\nWe say a sampling procedure is biased if certain individuals in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every individual in a population has an equal chance of being sampled.\n\nWe say a sample of \\(n\\) balls extracted using our shovel is representative of the population if it’s contents “roughly resemble” the contents of the bowl. If so, then the proportion of the shovel’s balls that are red can generalize to the proportion of the bowl’s \\(N\\) = 2400 balls that are red. Or expressed differently, \\(\\widehat{p}\\) is a “good guess” of \\(p\\). Now say we cheated when using the shovel and removed a number of white balls in favor of red balls. Then this sample would be biased towards red balls, and thus the sample would no longer be representative of the bowl.\nThe fourth and final set of terms and notation relate to the goal of sampling:\n\nOne way to ensure that a sample is unbiased and representative of the population is by using random sampling\nInference is the act of “making a guess” about some unknown. Statistical inference is the act of making a guess about a population using a sample.\n\nIn our case, since the rep_sample_n() function uses your computer’s random number generator, we were in fact performing random sampling.\nLet’s now put all four sets of terms and notation together, keeping our sampling activities in mind:\n\nSince we extracted a sample of \\(n\\) = 50 balls at random, we mixed all of the equally sized balls before using the shovel, then\nthe contents of the shovel are unbiased and representative of the contents of the bowl, thus\nany result based on the shovel can generalize to the bowl, thus\nthe sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus\ninstead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel.\n\nWhat you have been performing is statistical inference. This is one of the most important concepts in all of statistics. So much so, we included this term in the title of our book: “Statistical Inference via Data Science”. More generally speaking,\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can infer about the population using sampling.\n\nIn the second reading this week, we’ll introduce the infer package, which makes statistical inference “tidy” and transparent.\n\n\n3.2 Statistical definitions\nTo further attach words and labels to the various sampling concepts we’ve seen so far, we also introduce some important statistical definitions related to sampling. As a refresher of our 1000 repeated / replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 2, let’s display Figure 12 again.\n\n\n\n\n\n\n\n\n\nn = 25\n\n\n\n\n\n\n\nn = 50\n\n\n\n\n\n\n\nn = 100\n\n\n\n\n\nThese types of distributions have a special name: sampling distributions of point estimates. Their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. Unfortunately, the term sampling distribution is often confused with a sample’s distribution which is merely the distribution of the values in a single sample.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around 0.4 = 40%. Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of 0.2 = 20% when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 1, which we display again below:\n\n\n\nPreviously seen comparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.095\n\n\n50\n0.067\n\n\n100\n0.048\n\n\n\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error of a point estimate. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nSimilarly to confusion between sampling distributions with a sample’s distribution, people often confuse the standard error with the standard deviation. This is especially the case since a standard error is itself a kind of standard deviation. The best advice we can give is that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error.\nTo help reinforce these concepts, let’s re-display Figure 12 but using our new terminology, notation, and definitions relating to sampling in Figure 13.\n\n\n\n\n\n\n\n\nFigure 13: Three sampling distributions of the sample proportion \\(\\widehat{p}\\).\n\n\n\n\n\nFurthermore, let’s re-display Table 1 but using our new terminology, notation, and definitions relating to sampling in Table 2.\n\n\n\n\nTable 2: Standard errors of the sample proportion based on sample sizes of 25, 50, and 100\n\n\n\n\n\n\nSample size (n)\nStandard error of $\\widehat{p}$\n\n\n\n\nn = 25\n0.095\n\n\nn = 50\n0.067\n\n\nn = 100\n0.048\n\n\n\n\n\n\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n\n3.3 The moral of the story\nLet’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red.\nHowever, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion.\nIn our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate.\nRecall from earlier that the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the bowl was 900/2400 = 0.375 = prop_red * 100%. We computed this value by performing a virtual census of bowl. Let’s re-display our sampling distributions from Figure 12 and Figure 13, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5% in Figure 14. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\).\n\n\n\n\n\n\n\n\nFigure 14: Three sampling distributions with population proportion \\(p\\) marked by vertical line.\n\n\n\n\n\nWe also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 14. This is also known as having a precise estimate.\nSo random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 15 illustrates the difference.\n\n\n\n\n\n\nFigure 15: “Comparing accuracy and precision.”\n\n\n\nAt this point, you might be asking yourself: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is correct! Recall from earlier when we asked ourselves “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Similarly, we took 1000 repeated samples as a simulation of how sampling is done in real-life! We used these simulations to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nThis is not how sampling is done in real life! In a real-life scenario, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford. In the next chapter you will read about methods we can use to approximate these sampling distributions, when we have only one sample."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#central-limit-theorem",
    "href": "weeks/chapters/week7-reading1.html#central-limit-theorem",
    "title": "Week 7 – Exploring Sampling",
    "section": "4 Central Limit Theorem",
    "text": "4 Central Limit Theorem\nThis chapter began with (virtual) access to a large bowl of balls (our population) and a desire to figure out the proportion of red balls. Despite having access to this population, in reality, you almost never will have access to the population, either because the population is too large, ever changing, or too expensive to take a census of. Accepting this reality means accepting that we need to use statistical inference.\nIn Section 3.1, we stated that “statistical inference is the act of making a guess about a population using a sample.” But how do we do this inference? In the previous section, we defined the sampling framework only to state that in reality we take one large sample, instead of many samples as done in the sampling framework (which we modeled physically by taking many samples from the bowl).\nIn reality, we take only one sample and use that one sample to make statements about the population parameter. This ability of making statements about the population is allowable by a famous theorem, or mathematically proven truth, called the Central Limit Theorem. What you visualized in Figure 12 and Figure 13 and summarized in Table 1 and Table 2 was a demonstration of this theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow.\nIn other words, as our sample size gets larger (1) the sampling distribution of a point estimate (like a sample proportion) increasingly follows a normal distribution and (2) the variation of these sampling distributions gets smaller, as quantified by their standard errors. We discuss the properties of the normal distribution in Appendix Section 5.1.\nShuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created a 3-minute and 38-second video at https://youtu.be/jvoxEYmQHNM explaining this crucial statistical theorem using the average weight of wild bunny rabbits and the average wingspan of dragons as examples. It’s very clever!\n\n\n\n\n\n\nFigure 16\n\n\n\nHere’s what is so surprising about the Central Limit Theorem: regardless of the shape of the underlying population distribution, the sampling distribution of means (such as the sample mean of bunny weights or the sample mean of the length of dragon wings) and proportions (such as the sample proportion red in our shovels) will be normal. Normal distributions are defined by where they are centered and how wide they are, and the Central Limit Theorem gives us both:\n\nThe sampling distribution of the point estimate is centered at the true population parameter\nWe have an estimate for how wide the sampling distribution of the point estimate is, given by the standard error (which we will discuss in the next chapter on confidence intervals)\n\nWhat the Central Limit Theorem creates for us is a ladder between a single sample and the population. By the Central Limit Theorem, we can say that (1) our sample’s point estimate is drawn from a normal distribution centered at the true population parameter and (2)that the width of that normal distribution is governed by the standard error of our point estimate. Relating this to our bowl, if we pull one sample and get the sample proportion of red balls \\(\\widehat{p}\\), this value of \\(\\widehat{p}\\) is drawn from the normal curve centered at the true population proportion of red balls \\(p\\) with the computed standard error."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sec-appendix",
    "href": "weeks/chapters/week7-reading1.html#sec-appendix",
    "title": "Week 7 – Exploring Sampling",
    "section": "5 Appendix",
    "text": "5 Appendix\n\n5.1 Normal distribution\nLet’s review one specific distribution: the Normal Distribution. Bell shaped distributions like the Normal Distribution are defined by two values: (1) the mean \\(\\mu\\) (“mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (“sigma”) which determines the variation of the distribution. In Figure 17, we plot three normal distributions where:\n\nThe solid normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).\nThe dotted normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).\nThe dashed normal curve has mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\n\n\n\n\n\n\n\n\n\nFigure 17: Three normal distributions.\n\n\n\n\n\nNotice how the solid and dotted line Normal Distributions have the same center due to their common mean \\(\\mu\\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \\(\\sigma\\) = 5. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \\(\\sigma\\) = 2. However, they are centered at different locations.\nWhen the mean \\(\\mu\\) = 0 and the standard deviation \\(\\sigma\\) = 1, the normal distribution has a special name. It’s called the Standard Normal Distribution or the \\(z\\)-curve.\nFurthermore, if a variable follows a normal curve, there are three rules of thumb we can use:\n\n68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean.\n95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean.\n99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean.\n\nLet’s illustrate this on a standard normal curve in Figure 18. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example:\n\nThe middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values.\nThe middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5% = 95% of the area under the curve. In other words, 95% of values.\nThe middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values.\n\n\n\n\n\n\n\n\n\nFigure 18: Rules of thumb about areas under normal curves."
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html",
    "href": "weeks/tutorial/infer-CI-tutorial.html",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "",
    "text": "Per the California Privacy Rights Act (CPRA), the salaries of employees of the state of California are required to be public. We will revisit the 2019 salaries of head coaches from CSU and UC universities introduced this week.\nFirst, let’s load the coaches data set into your workspace and take a look at what the data look like.\n\n\nglimpse(coaches)"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#data",
    "href": "weeks/tutorial/infer-CI-tutorial.html#data",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "",
    "text": "Per the California Privacy Rights Act (CPRA), the salaries of employees of the state of California are required to be public. We will revisit the 2019 salaries of head coaches from CSU and UC universities introduced this week.\nFirst, let’s load the coaches data set into your workspace and take a look at what the data look like.\n\n\nglimpse(coaches)"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-1",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-1",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 1",
    "text": "Section 1\n\nStarting with one sample\nFor our investigation, we are interested in estimating the median salary for all CSU and UC coaches. In class we talked about creating the sampling distribution of the median salary, since I had the salaries of every CSU and UC head coach.\nToday, I’ve given you a simple random sample of 50 coaches from the master dataset of all CSU and UC coaches. This context should feel more familiar — you are interested in estimating the value of the true median salary of all CSU and UC head coaches, given a sample of 50 coaches.\nThe population median salary of all CSU and UC coaches is: $137,619. Let’s see how close our sample median is to this population median!\nUse the summarize() function to calculate the median Total Pay & Benefits for your sample of 50 coaches.\n\n\n\n\n\n\n\nsummarize(coaches)\n# What goes inside the summarize() function?\n\n\n\n\nsummarize(coaches, \n          median = median(`Total Pay & Benefits`))\n\n\nPlotting the data is possibly more important than calculating the sample statistic, since it gives us an idea of the distribution of these 50 salaries.\nCreate a histogram of the salaries from the sample of 50 coaches. I’ve given you some code to start with, but you’ll need to add to it!\n\n\nggplot(data = coaches, \n       mapping = aes(x = ___)) \n\n\n\n\n\n# Use the numerical variable we are interested in!\nggplot(data = coaches, \n       mapping = aes(x = `Total Pay & Benefits`)) \n\n\n\n\n# Add a histogram to the plot\nggplot(data = coaches, \n       mapping = aes(x = `Total Pay & Benefits`)) +\n  geom_histogram()\n\n\n\n\n# Make the histogram look a bit better with binwidths! \nggplot(data = coaches, \n       mapping = aes(x = `Total Pay & Benefits`)) + \n  geom_histogram(binwidth = 75000)"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-2",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-2",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 2",
    "text": "Section 2\nNow, let’s take our single sample of 50 and see what we might have gotten from other random samples!\n\nOne Resample\nWe’ll start with getting one bootstrap resample. To do this we take our original sample (stored in the coaches dataset) and resample with replacement 50 times. The rep_sample_n() function from the infer package helps us obtain one of these resamples. All we need to do is specify the size and replace arguments.\nKeep in mind: how large your resample should be and whether you should sample with or without replacement\n\n\none_resample &lt;- rep_sample_n(coaches, \n                             size = ___, \n                             replace = ___)\n\n\n\n\n\none_resample &lt;- rep_sample_n(coaches, \n                             size = 50, \n                             replace = ___)\n\n\n\n\none_resample &lt;- rep_sample_n(coaches, \n                             size = 50, \n                             replace = TRUE)\n\n\nLet’s compare our resample with our original sample. Your original sample had a median of $137,619. Use the summarize() function to find the median of this resample.\n\n\n\n\n\n\nHint: Remember to use the one_resample dataset!\n\n\n\nMultiple Resamples\nNow, let’s take what we learned about creating and summarizing one bootstrap resample and scale it up!\nFirst, we’re going to create 500 different bootstrap resamples, each of size 50. We’re still using the rep_sample_n() function to do this, but now we specify the number of resamples we want with the reps argument.\nModify your previous code to create 500 bootstrap resamples.\n\n\nmultiple_resamples &lt;- rep_sample_n(coaches, \n                                   size = ___, \n                                   replace = ___, \n                                   reps = ___)\n\n\n\n\n\n# These are the options we used for one resample! \nmultiple_resamples &lt;- rep_sample_n(coaches, \n                                   size = 50, \n                                   replace = TRUE, \n                                   reps = ___)\n\n\n\n\n# We want 500 resamples! \nmultiple_resamples &lt;- rep_sample_n(coaches, \n                                   size = 50, \n                                   replace = TRUE, \n                                   reps = 500)\n\n\nNow that we have 500 resamples we need to summarize each sample with a single statistic. In our investigation we are interested in the sample median, so that’s the statistic we will use.\nCalculate the median Total Pay & Benefits for each resample. The multiple_resamples dataset contains 25000 numbers — 50 observations for 500 samples. So, to get each resample’s median we need to group_by() the sample ID column (replicate) before we calculate the median.\n\n\nmultiple_resamples %&gt;% \n\n\n\n\n\n# Make different groups for each sample\nmultiple_resamples %&gt;% \n  group_by(replicate)\n\n\n\n\n# Find the median of each group\nmultiple_resamples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(median = median(`Total Pay & Benefits`))"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-3",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-3",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 3",
    "text": "Section 3\nFor confidence intervals, we’d like to have a method that is a bit more dynamic than what we’ve been doing with rep_sample_n(), group_by(), and summarize(). The infer package includes tools that help us create bootstrap resamples, visualize the distribution of our bootstrap statistics, and find confidence intervals.\n\nThe infer pipeline\nGenerating bootstrap statistics has a similar process to what we used previously, but it has a very different feel. The process looks something like this:\nLet’s walk through each of the components of the infer “pipeline.”\nStep 1: specify() your response (and explanatory) variable(s)\nThis step declares to R what variable(s) you are interested in. For this tutorial we are only interested in one variable: Total Pay & Benefits. In the future, we will include an explanatory variable to help explain the variability in the response variable.\n\nStep 2: generate() resamples\nThe generate() step takes your original data and generates bootstrap resamples. It knows how many resamples to generate from the quantity specified in the reps argument. Additionally, it knows to sample with replacement when the type argument is set to \"bootstrap\".\n\nStep 3: calculate() bootstrap statistics\nNow that you’ve generated lots of bootstrap resamples, you need to summarize them with a single statistic. That statistic is what you tell the calculate() function to find! There are lots of different statistics we will explore, but for this investigation our stat will be a \"median\".\n\n\n\nYour turn!\nTake what you’ve learned about the infer pipeline and create a new dataset, named coaches_resample that contains median salaries calculated from 1000 bootstrap resamples.\n\n\ncoaches_resample &lt;- coaches %&gt;% \n  specify(___) %&gt;% \n  generate(___) %&gt;% \n  calculate(___)\n\n\n\n\n\n# Specify what your response variable is!\ncoaches_resample &lt;- coaches %&gt;% \n  specify(response = `Total Pay & Benefits`) %&gt;% \n  generate(___) %&gt;% \n  calculate(___)\n\n\n\n\n# Generate 1000 bootstrap resamples!\ncoaches_resample &lt;- coaches %&gt;% \n  specify(response = `Total Pay & Benefits`) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(___)\n\n\n\n\n# Calculate the median for each resample! \ncoaches_resample &lt;- coaches %&gt;% \n  specify(response = `Total Pay & Benefits`) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(stat = \"median\")"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-4",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-4",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 4",
    "text": "Section 4\nAlright, our final step is to use our bootstrap statistics to calculate a confidence interval for the true median salary of all CSU and UC head coaches.\nA nice first step is to visualize how the distribution of bootstrap statistics looks. The coaches_resample object contains 1000 medians calculated from 1000 different bootstrap resamples. Let’s use the built-in visualize() function to make a quick histogram of these bootstrap medians.\n\nvisualise(coaches_resample)\n\n\n\n\n\n\n\n\nIt looks like the majority of medians fall between $170,398 and $131,463, but let’s quantify this range with a confidence interval.\nThe get_confidence_interval() function is what we use to find a confidence interval from a set of bootstrap statistics. This function takes three arguments:\n\na dataset containing bootstrap statistics\nthe level of confidence that should be used\nthe type of method to use when making the interval\n\nThis looks something like:\n\n\n\nUsing these tools, calculate a 99% confidence interval for the population median using the 1000 bootstrap statistics you found previously. Use the percentile method to calculate your confidence interval.\nWhen you are done, preview what your confidence interval looks like!\n\n\ncoaches_CI &lt;- coaches_resample %&gt;% \n  get_confidence_interval(level = ___, type = ___)\n\n\n\n\n\n# For a 99% interval your level is 0.99\ncoaches_CI &lt;- coaches_resample %&gt;% \n  get_confidence_interval(level = 0.99, type = ___)\n\n\n\n\n# For a percentile CI you use the \"percentile\" method\ncoaches_CI &lt;- coaches_resample %&gt;% \n  get_confidence_interval(level = 0.99, type = \"percentile\")\n\n\nHow would you interpret the interval you got?"
  },
  {
    "objectID": "labs/grading-guides/lab-4-grading-guide.html",
    "href": "labs/grading-guides/lab-4-grading-guide.html",
    "title": "Lab 4: Grading Guide",
    "section": "",
    "text": "Q1 -\nQ2 -\nQ3 -\nQ4 -\nQ5 -\nQ6 -\nQ7 -\nQ8 -\nQ9 -\nQ10 -\nQ11 -\n\n\nQuestion 1 – Size of ntl_icecover dataset\nSuccess:\n\nhas glimpse() somewhere in their code\nstates there are 334 rows and 5 columns\n\nGrowing:\n\nIf no code is present\nIf they provide incorrect size of data\n\n\n\n\n\n\n\nNote\n\n\n\nIf they say there are 431 rows, they got their answer from the help file!\nFeedback: Careful! The best way to obtain information about the dataset is using the glimpse() function!\n\n\n\n\nQuestion 2 – Scatterplot\nSuccess: Code should look like the following\n\nggplot(data = ntl_icecover, \n       mapping = aes(y = ice_duration, x = year)) +\n  geom_point() +\n  labs(x = \"Year\", \n       y = \"Ice Duration (days)\")\n\nGrowing:\n\nDoesn’t change y-axis labels\n\nFeedback: For Question 2, you were asked to give your visualization nice axis labels. Also, it is important for the axis label to contain the units of the variable. What unit was ice duration measured in?\n\nDoesn’t include units (days) in y-axis label\n\nFeedback: For Question 2, it is important for the axis label to contain the units of the variable. What unit was ice duration measured in?\n\n\nQuestion 3 – Describe the regression line\nSuccess: Addresses form, direction, strength and unusual points.\n\nForm: Linear\nDirection: Negative\nStrength: Moderate\nOutliers: Maybe the 2001 observation with ~20 days of ice?\n\nGrowing:\n\nIf they don’t discuss one of the for aspects.\n\nFeedback: For Question 3, you were asked to describe the form, direction, strength, and unusual points. Make sure you discuss all four aspects!\n\nIf they say there are unusual points / outliers but don’t say where.\n\nFeedback: For Question 3, if you believe there are unusual observations you need to tell the reader where these points are!\n\n\nQuestion 4 – Add a regression line\nSuccess:\n\nIncludes regression line with geom_smooth() and method = \"lm\" option\n\n\n\n\n\n\n\nNote\n\n\n\nThey don’t need to turn the standard errors off (se = FALSE), but great if they did!\n\n\nGrowing:\n\nIf they do not add regression line\n\nFeedback: For Question 4, you needed to add a regression line to the previous scatterplot!\n\nIf they do not use method = \"lm\" to get straight line\n\nFeedback: For Question 4, you need to have a straight line, not a wiggly line! Take a look at the R resources to see how you specify a straight line!\n\n\nQuestion 5 – Find correlation\nSuccess: Code should look like the following\n\nget_correlation(data = ntl_icecover, ice_duration ~ year, na.rm = TRUE)\n\nGrowing:\n\nDoesn’t remove NAs\n\nFeedback: For Question 5, it is important to remove the NAs before calculation the correlation! Is there another argument (input) for the get_correlation() function that allows you to remove the missing values? Hint: it looks like the option we used to remove missing values in the mean() function!\n\n\nQuestion 6 – Fit linear regression\nSuccess: Code should look like the following\n\nmy_model &lt;- lm(ice_duration ~ year, data = ntl_icecover)\n\nGrowing:\n\nSwaps response and explanatory\n\nFeedback: Careful! With the lm() function the response variable goes first (before the ~) and the explanatory comes second (after the ~).\n\n\nQuestion 7 – Get regression table\nSuccess: Code should look like the following\n\nget_regression_table(my_model)\n\nGrowing:\n\nDoesn’t use the get_regression_table() function\n\nFeedback: For Question 7, we want out model output to look tidy, which is why we use the get_regression_table() function!\n\n\nQuestion 8 – Write out regression equation\nSuccess:\n\\[ \\widehat{\\text{ice duration}} = 495 - 0.203 \\times (\\text{year})\\]\n\nIndicates the response (ice duration not y) is estimated / predicted / the mean with either a hat or with language\nInputs values in correct location\nReferences year(not x)\n\nGrowing:\n\nDoes not indicate the response (ice duration) is estimated / predicted / the mean\n\nFeedback: For Question 8, remember the estimated regression equation has a hat over y. What does that hat mean? How can you incorporate that into your regression equation?\n\nUses x and y instead of variable names\n\nFeedback: When writing out the regression equation, you need to reference the context of the line that was found. What variable was the response? What variable was the explanatory?\n\n\n\n\n\n\nNote\n\n\n\nIf they use x and y in the equation but then define what variables they are associated with, that is okay!\n\n\n\n\nQuestion 9 – Interpret slope\nSuccess:\n\nCorrect interpretation of slope for 1 year increase\n\n1 year increase\nmean / predicted / estimated ice duration\ndecreases by 0.203 days\n\nIncorrect interpretation of the slope\n\nFeedback: For slope interpretations, we always increase the explanatory variable by 1 unit and the slope coefficient is the expected change in the mean of the response variable.\n\nDoesn’t interpret the value of -0.203\n\nFeedback: Careful! You did not interpret what the value of -0.203 means in the context of the regression line!\n\nStates the duration of ice increases\n\nFeedback: Look at the sign associated with the slope coefficient! What does the sign tell you about whether ice duration should increase or decrease?\n\nDoesn’t indicate the response is mean / estimate / predicted ice duration\n\nFeedback: Careful! Your interpretation sounds like it is guaranteed that every 1 year increase in time will result in exactly a 0.203 day decrease in ice duration. Will there always be exactly this decrease? How do we indicate that our slope estimate is not exact?\n\n\nQuestion 10 – 100 Year Increase\n\nCorrect interpretation of slope for 100 year increase\n\n100 year increase\nmean / predicted / estimated ice duration\ndecreases by 20.3 days\n\n\nGrowing:\n\nUses y-intercept when increasing 100 years\n\nFeedback: When we increase 100 years, technically the only aspect of the equation that changes is the amount of time our slope is multiplied by. The y-intercept does not change! With that in mind, how does a 100 year increase in time affect what you would expect for the duration of the ice on these lakes?\n\nDon’t provide an estimate of the change\n\nFeedback: You need to use your estimated regression equation to state what the expected change in the ice duration would be if the number of years was increased by 100.\n\n\nQuestion 11 – Multivariate plot\nSuccess: Code should look like the following\n\nggplot(data = ntl_icecover, \n       mapping = aes(y = ice_duration, x = year, color = lakeid)) +\n  geom_point() +\n  labs(x = \"Year\", \n       y = \"Ice Duration (days)\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf they got a Growing for Question 4 from using a wiggly line, they don’t need to get a Growing here!\n\n\nGrowing:\n????"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html",
    "href": "labs/grading-guides/lab-7-grading-guide.html",
    "title": "Lab 7 Grading Guide",
    "section": "",
    "text": "Q1:\nQ2:\nQ3:\nQ4:\nQ5:\nQ6:\nQ7:\nQ8:\nQ9:\nQ10:\nQ11:\nQ12:"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-1",
    "title": "Lab 7 Grading Guide",
    "section": "Question 1",
    "text": "Question 1\nTo earn a Success:\n\nincludes either latitude or site inside of group_by()\n\nIf uses another variable:\n\nWhat variable(s) uniquely defines the location of each marsh?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-2",
    "title": "Lab 7 Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\nCreates scatterplot with:\n\nlatitude on x-axis\nwater temperature on y-axis\naxis labels include variable units (Celsius for water temp; degrees for latitude)\n\n\n\n\n\n\n\nNote\n\n\n\nThey are not required to include a regression line, but if they do, great!\n\n\nIf they don’t have axis labels with the units:\n\nIt is important to include an axis label stating what the units of each variable are!"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-3",
    "title": "Lab 7 Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success:\nDescription includes all of the following:\n\nform of relationship (linear)\ndirection of relationship (negative)\nstrength of relationship (strong)\nlocation of any outliers (e.g., at a latitude of 45 degrees and 17.5 C temperature)\n\nIf they forget one of these:\n\nCareful! You were asked to describe the form, direction, strength, and unusual points for the plot. Remember, it is important to explicitly state where you believe the outliers are, so the reader knows where to look!\n\nIf they don’t state where the outlier is:\n\nIt is important to explicitly state where you believe the outliers are, so the reader knows where to look!"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-4",
    "title": "Lab 7 Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\nCode should look like the following:\n\nget_correlation(data = pie_crab_clean, \n                formula = latitude ~ water_temp)"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-5",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-5",
    "title": "Lab 7 Grading Guide",
    "section": "Question 5",
    "text": "Question 5\nTo earn a Success:\nCode should look like the following:\n\nobs_slope &lt;- pie_crab_clean %&gt;% \n  specify(response = water_temp, \n          explanatory = latitude) %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-6",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-6",
    "title": "Lab 7 Grading Guide",
    "section": "Question 6",
    "text": "Question 6\nTo earn a Success:\nCode should look like the following:\n\nbootstrap_dist &lt;- pie_crab_clean %&gt;% \n  specify(response = water_temp, \n          explanatory = latitude) %&gt;% \n  generate(reps = 500, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-7",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-7",
    "title": "Lab 7 Grading Guide",
    "section": "Question 7",
    "text": "Question 7\nTo earn a Success:\nCode should look like the following:\n\nvisualise(bootstrap_dist) + \n  labs(x = \"&lt;SOME AXIS LABEL&gt;\")\n\nTheir axis label must describe what is being plotted (e.g., slope statistics, sample slope, etc.)\nIf they did not include an axis label:\n\nIt is important to include an axis label stating what is being plotted! What is being plotted on this bootstrap distribution?\n\nIf their axis label is not about the slope statistic:\n\nWhat is being plotted in this distribution? What is it a distribution of? What statistics did you calculate in the previous step that are being plotted here?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-8",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-8",
    "title": "Lab 7 Grading Guide",
    "section": "Question 8",
    "text": "Question 8\nTo earn a Success:\nCode should look like the following:\n\nget_confidence_interval(bootstrap_dist, \n                        level = 0.85)\n\nIf they use a level other than 0.85:\n\nLook back at the lab instructions, what percentage confidence interval were you asked to construct?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-9",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-9",
    "title": "Lab 7 Grading Guide",
    "section": "Question 9",
    "text": "Question 9\nTo earn a Success:\nThe interpretation must state:\n\nconfidence: they are 85% confident\nstatistic: the slope between water temperature and latitude\npopulation: for all marshes along the eastern US\ninterval: is between [lower bound] and [upper bound]\n\n\n\n\n\n\n\nNote\n\n\n\nNote that every group will get a different interval, due to the randomness of bootstrapping!\n\n\nIf they don’t state their confidence:\n\nHow much confidence do you have in your interval?\n\nIf they don’t state the statistic in context:\n\nWe need to be specific about the what parameter we believe is in our interval. What variables is the slope between?\n\nIf they have state an incorrect population:\n\nCareful! Are you analyzing data on crabs or on marshes? What population were these 13 marshes drawn from? Where were these marshes located? What population of marshes does your confidence interval apply to?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-10",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-10",
    "title": "Lab 7 Grading Guide",
    "section": "Question 10",
    "text": "Question 10\nTo earn a Success:\nCode should look like the following:\n\nget_confidence_interval(bootstrap_dist, \n                        level = 0.85, \n                        point_estimate = obs_slope, \n                        type = \"se\")\n\nIf they don’t have type = \"se\":\n\nWhat method do you want the function to use when calculating your confidence interval? Percentile intervals are the default!\n\n\n\n\n\n\n\nNote\n\n\n\nIf they type in the actual number for the estimated slope rather than using the obs_slope object, you can give them this reminder:\n\nIt is better practice not to “hard-code” numbers in our code, but to reference values stored in objects. This makes our code more resistant to changes in the dataset."
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-11",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-11",
    "title": "Lab 7 Grading Guide",
    "section": "Question 11",
    "text": "Question 11\nTo earn a Success:\n\nstates the intervals are similar\nstates similarity is due to the normality of the bootstrap distribution\n\nIf they say the intervals are different:\n\nGive these intervals another look, yes thier endpoints differ, but how different are they?\n\nIf they don’t talk about the bootstrap distribution being approximately normal:\n\nWhat condition do we need to check when using the SE method to construct confidence intervals? Would you say this condition is violated?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-12-incorrectly-labeled-11-again",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-12-incorrectly-labeled-11-again",
    "title": "Lab 7 Grading Guide",
    "section": "Question 12 (incorrectly labeled 11 again…)",
    "text": "Question 12 (incorrectly labeled 11 again…)\nTo earn a Success:\n\nstates whether they do / do not believe the sample of 13 marshes is representative of all marshes along the eastern US\njustifies why using something something that is true?\n\n\n\n\n\n\n\nNote\n\n\n\nI’m willing to be pretty flexible here, but the key aspect is that they need to justify their reasoning.\n\n\nIf their justification is about crabs:\n\nCareful! Are you analyzing data on crabs or on marshes? The population of marshes you define in #9 is what you should use to assess if the assumption of bootstrapping is reasonable.\n\nIf their justification is not reasonable:\n\nWhere were these 13 marshes located geographically? Do you believe that these 13 marshes are representative of the broader population they were sampled from?\n\nIf their justification is about the shape of the bootstrap distribution:\n\nYou are on the right track, we need for the sample to be representative of the population from which it was sampled. However, we cannot assess the representativeness of a sample using a bootstrap distribution. This can only be assessed by inspecting how the data were collected."
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html",
    "href": "labs/grading-guides/lab-6-grading-guide.html",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "Q1 -\nQ2 -\nQ3 -\nQ4 -\nQ5 -\nQ6 -\nPrediction -"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-1",
    "title": "Lab 6 Grading Guide",
    "section": "Question 1",
    "text": "Question 1\nTo earn a Success:\n\nfits other 6 models (rank, pic_outfit, pic_color, large_class, eval_completion, cls_level)\nfinds other 6 adjusted \\(R^2\\) values\nstates top model includes eval_completion variable\n\nIf they do not state the top model or what variable was chosen:\n\nWhen comparing multiple models it is important to explicitly state what model was selected as the top model. This can be a simple statement like, “The top model was selected to have the following variables: ….”."
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-2",
    "title": "Lab 6 Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\n\nfits 11 models that include the variable chosen in #1 as an explanatory variable\nfinds 11 adjusted \\(R^2\\) values\nstates top model includes eval_completion & pic_color variables\n\nIf they do not state the top model or what new variable was chosen:\n\nWhen comparing multiple models it is important to explicitly state what model was selected as the top model. This can be a simple statement like, “The top model was selected to have the following variables: ….”.\n\nOR\n\nYou need to state every variable that is included in your top model, not the name of the model that was fit.\n\nOR\n\nCareful! Your model contains more variables than just gender! What are all the variables included in your top model?"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-3",
    "title": "Lab 6 Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1 and #2 as an explanatory variables\nstates top model includes all the variables they’ve chosen\n\nIf they don’t include a variable from #1 or #2 as an explanatory variable\n\nCareful! You need to deciding what variable to add, given the variables you have already chosen! The variables you chose in #1 and #2 need to be included in the models you are comparing.\n\nIf they do not state the top model or what new variable was chosen:\n\nWhen comparing multiple models it is important to explicitly state what model was selected as the top model. This can be a simple statement like, “The top model was selected to have the following variables: ….”.\n\nOR\n\nYou need to state every variable that is included in your top model, not the name of the model that was fit.\n\nOR\n\nCareful! Your model contains more variables than just gender! What are all the variables included in your top model?"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-4",
    "title": "Lab 6 Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1, #2, and #3 as an explanatory variables\nstates top model includes all the variables they’ve chosen\n\nIf they don’t include a variable from #1, #2, or #3 as an explanatory variable\n\nCareful! You need to deciding what variable to add, given the variables you have already chosen! The variables you chose in #1, #2 AND #3 need to be included in the models you are comparing.\n\nIf they do not state the top model or what new variable was chosen:\n\nWhen comparing multiple models it is important to explicitly state what model was selected as the top model. This can be a simple statement like, “The top model was selected to have the following variables: ….”.\n\nIf they state they added both variables that were tied:\n\nYou need to choose one of the variables provided!"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-5",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-5",
    "title": "Lab 6 Grading Guide",
    "section": "Question 5",
    "text": "Question 5\nTo earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1, #2, #3, and #4 as an explanatory variables\nstates top model includes all the variables they’ve chosen\n\nIf they don’t include a variable from #1, #2, #3, or #4 as an explanatory variable\n\nCareful! You need to deciding what variable to add, given the variables you have already chosen! The variables you chose in #1, #2, #3 AND #4 need to be included in the models you are comparing.\n\nIf they do not state the top model or what new variable was chosen:\n\nWhen comparing multiple models it is important to explicitly state what model was selected as the top model. This can be a simple statement like, “The top model was selected to have the following variables: ….”."
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-6",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-6",
    "title": "Lab 6 Grading Guide",
    "section": "Question 6",
    "text": "Question 6\nTo earn a Success:\n\nincludes all variables from top model in regression\n\nIf the model fit here differs from their top model, they must state why they fit a different model."
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#prediction",
    "href": "labs/grading-guides/lab-6-grading-guide.html#prediction",
    "title": "Lab 6 Grading Guide",
    "section": "Prediction",
    "text": "Prediction\nTo earn a Success:\n\nmake predictions using the top model from #6 (or #5)\nif a different model is chosen then justification is provided for why"
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html",
    "href": "labs/grading-guides/lab1_feedback.html",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they have answers to all eight (8) questions, they earn a “Complete”.\n\n\nList of favorite animals, include a picture, change section header\nIf their image doesn’t appear in the document\nQuestion 2 – Careful! Your image is not appearing in your rendered HTML file. There are a few different ways to fix this. First is to change the YAML (title section) of your document, as outlined in the Canvas announcement. If you did this and your image is still not showing up, then the issue might be how you are including your image. Generally, uploading an image to your project works better than including a URL link to an image. There are instructions on how to do this in the #lab channel of #Week 1 on Discord.\n\n\n\nIf they say something about #| include: false:\nQuestion 4 – Yes! You are on the right track! The #| include: false option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something about the code not running / the code not producing output:\nQuestion 4 – Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something not relevant to loading in a package:\nQuestion 4 – The code for Question 4 is associated with loading in the tidyverse package. Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\n\n\n\nIf they say it gives a preview of the dataset:\nQuestion 5 – You are on the right track! The glimpse() function outputs a preview of the dataset, specifically the column names, their data types, and a preview of the rows of each column.\n\n\n\nIf they say something about the code not showing:\nQuestion 6 – Yes! The #| echo: false option tells Quarto that the code should not be included in the rendered document, but the output of the code (the plot) should be included!\nIf they say something not relevant to the code not showing:\nQuestion 6 – The #| echo: false option controls how the rendered HTML file looks. Specifically, the #| echo: false option tells Quarto that the code should not be output in the rendered file, but the output of the code (the plot) should be included."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-1-2-3",
    "href": "labs/grading-guides/lab1_feedback.html#question-1-2-3",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "List of favorite animals, include a picture, change section header\nIf their image doesn’t appear in the document\nQuestion 2 – Careful! Your image is not appearing in your rendered HTML file. There are a few different ways to fix this. First is to change the YAML (title section) of your document, as outlined in the Canvas announcement. If you did this and your image is still not showing up, then the issue might be how you are including your image. Generally, uploading an image to your project works better than including a URL link to an image. There are instructions on how to do this in the #lab channel of #Week 1 on Discord."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-4",
    "href": "labs/grading-guides/lab1_feedback.html#question-4",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they say something about #| include: false:\nQuestion 4 – Yes! You are on the right track! The #| include: false option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something about the code not running / the code not producing output:\nQuestion 4 – Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something not relevant to loading in a package:\nQuestion 4 – The code for Question 4 is associated with loading in the tidyverse package. Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-5",
    "href": "labs/grading-guides/lab1_feedback.html#question-5",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they say it gives a preview of the dataset:\nQuestion 5 – You are on the right track! The glimpse() function outputs a preview of the dataset, specifically the column names, their data types, and a preview of the rows of each column."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-6",
    "href": "labs/grading-guides/lab1_feedback.html#question-6",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they say something about the code not showing:\nQuestion 6 – Yes! The #| echo: false option tells Quarto that the code should not be included in the rendered document, but the output of the code (the plot) should be included!\nIf they say something not relevant to the code not showing:\nQuestion 6 – The #| echo: false option controls how the rendered HTML file looks. Specifically, the #| echo: false option tells Quarto that the code should not be output in the rendered file, but the output of the code (the plot) should be included."
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html",
    "href": "labs/grading-guides/lab-9-grading-guide.html",
    "title": "Lab 9 – Grading Guide",
    "section": "",
    "text": "Q1:\nQ2:\nQ3:\nQ4:\nQ5:\nQ6:\nQ7:\nQ8:\nQ9:\nQ10:\nQ11:\nQ12:\nQ13:\nQ14:\nQ15:\nQ16:\nQ17:\nQ18:\nQ19:\nQ20:"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-1",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 1",
    "text": "Question 1\nTo earn a Success:\n\nfills in code\nadds axis label\n\n\n\n\n\n\n\nWarning\n\n\n\nTheir axis label needs to indicate this is a score for Grade 8 Math\n\n\nIf they do not have “grade 8” or “math score” in their title:\n\nWhat context are these math scores for? What exam? What grade?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-2",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\n\ncompares centers of plots\ncompares spreads of plots"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-3",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success:\n\nfills in code\nadds axis label\n\n\n\n\n\n\n\nWarning\n\n\n\nTheir axis label needs to indicate this is a score for Grade 8 Math\n\n\nIf they do not have “grade 8” or “math score” in their title:\n\nWhat context are these math scores for? What exam? What grade?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-4",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\n\ncompares centers of plots\ncompares spreads of plots"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-5-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-5-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 5 – Important!",
    "text": "Question 5 – Important!\nTo earn a Success:\n\n\nUsing fill:\n\nggplot(data = math_scores, \n       mapping = aes(x = grade_8_math_score, \n                     y = year_cat, \n                     fill = continent)) +\n  geom_density_ridges(alpha = 0.5, scale = 1) + \n  labs(x = \"Grade 8 Math Score on TIMSS\", \n       y = \"Year\", \n       fill = \"Continent\")\n\n\n\n\nUsing facets:\n\nggplot(data = math_scores, \n       mapping = aes(x = grade_8_math_score, \n                     y = year_cat)) +\n  geom_density_ridges(alpha = 0.5, scale = 1) + \n  labs(x = \"Grade 8 Math Score on TIMSS\", \n       y = \"Year\") + \n  facet_wrap(~continent)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTheir axis label needs to indicate this is a score for Grade 8 Math\n\n\nIf they don’t change the transparency of their plots:\n\nRemember you can use alpha to change the transparency of the density ridges so you can see them all!\n\nIf they do not have “grade 8” or “math score” in their title:\n\nWhat context are these math scores for? What exam? What grade?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-6-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-6-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 6 – Important!",
    "text": "Question 6 – Important!\nTo earn a Success: compares if distributions of continents move over time or if they stay similar"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-7-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-7-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 7 – Important!",
    "text": "Question 7 – Important!\nTo earn a Success:\n\ncompares observations within a continent\nnotices countries are recorded multiple times\nstates the condition is violated\n\nIf they do not state the condition is violated:\n\nLook at the observations for Africa! How many observations for Ghana are there? Is it reasonable to assume those observations are independent?\n\nIf they say observations for continents are related in time:\n\nDo we have one observation per continent? If not, what are the observations of? Are these observations within a continent related?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-8-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-8-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 8 – Important!",
    "text": "Question 8 – Important!\nTo earn a Success:\n\nconnects comparison with observations within a year or between continents\n\n\n\n\nsays countries / continents are not related\njustifies decision (no)\n\n\n\n\n\nsays countries / continents are related\njustifies decision (spatial relationship)\n\n\n\nIf they do not justify their decision:\n\nAn essential part of evaluating model conditions is to justify the decisions you make. What was your reasoning for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-9-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-9-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 9 – Important!",
    "text": "Question 9 – Important!\nTo earn a Success:\n\ncompares observations between continents\nstates that a country can only belong to one continent\nsays condition is not violated\n\nIf they don’t discuss how many continents a county can belong to:\n\nHow many continents can a country belong to? What does that say about between continent independence?\n\nIf they do not justify their decision:\n\nAn essential part of evaluating model conditions is to justify the decisions you make. What was your reasoning for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-10-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-10-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 10 – Important!",
    "text": "Question 10 – Important!\nTo earn a Success:\n\ncompares observations between years\nstates that observations are related in time\nsays condition is violated\n\nIf they do not state the condition is violated:\n\nIs the observation for Ghana in 2003 related to the observation for Ghana in 2007?\n\nIf they do not justify their decision:\n\nAn essential part of evaluating model conditions is to justify the decisions you make. What was your reasoning for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-11-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-11-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 11 – Important!",
    "text": "Question 11 – Important!\nTo earn a Success:\n\ndiscusses the shape of the distributions\nsays some distributions have more than one mode\nsays condition is violated\n\nIf they do not state the condition is violated:\n\nHow many peaks does the Normal distribution have? How many peaks do you see in some of these distributions?\n\nIf they don’t make a decision (violated / not):\n\nA critical part of evaluation model conditions is to make a decision! Do you feel that this condition is or is not violated?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-12-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-12-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 12 – Important!",
    "text": "Question 12 – Important!\nTo earn a Success:\n\ncompares the log variances between continents / years\nsays there are large differences in the log variances\nsays condition is violated\n\nIf they do not state the condition is violated:\n\nHow many times larger is the largest variance than the smallest variance? If it is less than 2, they could be similar values, but larger than 2 they start to seem pretty different."
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-13",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-13",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 13",
    "text": "Question 13\nTo earn a Success: Code should look like:\n\naov(grade_8_math_score ~ year, data = math_scores) %&gt;% \n  broom::tidy()\n\nIf they swap \\(y\\) and \\(x\\):\n\nCareful! In the lm() function the response goes first!"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-14",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-14",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 14",
    "text": "Question 14\nTo earn a Success:\n\nreject \\(H_0\\)\nstate that the p-value was less than 0.1\n\n\n\n\n\n\n\nFine to not include p-value\n\n\n\nIf they don’t state what the p-value was:\n\nRemember it is important to be transparent with your decision, what p-value did you obtain for this test?\n\n\n\nIf they don’t state their \\(\\alpha\\):\n\nWhat alpha was used for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-15",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-15",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 15",
    "text": "Question 15\nTo earn a Success: Conclude at least on year has a different mean grade 8 math score\nIf they say all the years have different means:\n\nWhat is the alternative hypothesis for a one-way ANOVA? Is it that all of the years have different means?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-16",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-16",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 16",
    "text": "Question 16\nTo earn a Success: Code should look like:\n\nnull_dist &lt;- math_scores %&gt;%\n  specify(response = grade_8_math_score, explanatory = continent) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-17",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-17",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 17",
    "text": "Question 17\nTo earn a Success: Code should look like:\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat = obs_F, direction = \"greater\")\n\nIf they don’t use \"greater\" in their direction:\n\nCareful! F-statistics can only be positive! What direction should we use to calculate the p-value for statistics that cannot be negative?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-18",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-18",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 18",
    "text": "Question 18\nTo earn a Success: Code should look like:\n\nget_p_value(null_dist, \n            obs_stat = obs_F, \n            direction = \"greater\")\n\nIf they don’t use \"greater\" in their direction:\n\nCareful! F-statistics can only be positive! What direction should we use to calculate the p-value for statistics that cannot be negative?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-19",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-19",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 19",
    "text": "Question 19\nTo earn a Success:\n\nreject \\(H_0\\)\nstate that the p-value was less than 0.1\n\n\n\n\n\n\n\nFine to not include p-value\n\n\n\nIf they don’t state what the p-value was:\n\nRemember it is important to be transparent with your decision, what p-value did you obtain for this test?\n\n\n\nIf they don’t state their \\(\\alpha\\):\n\nWhat alpha was used for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-20",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-20",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 20",
    "text": "Question 20\nTo earn a Success: Conclude at least on continent has a different mean grade 8 math score\nIf they say all the continents have different means:\n\nWhat is the alternative hypothesis for a one-way ANOVA? Is it that all of the continents have different means?"
  },
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "",
    "text": "Today we will explore the pie_crab dataset contained in the lterdatasampler R package. The data is from a study by Johnson et al. at the Plum Island Ecosystem Long Term Ecological Research site, studying the relationship between the size (carapace width) of a Fiddler Crab and the geographical location of its habitat. These data can be used to investigate if Bergmann’s Rule applies to Fiddler Crabs, or specifically that the size of a crab increases as the distance from the equator increases.\n\n\nThe students who investigated this relationship for their midterm project found that when both latitude and water temperature are included as explanatory variables in the multiple regression model, the coefficient associated with water temperature doesn’t make sense. Namely, the model suggests warmer water temperatures are associated with larger crab sizes. However, we know that the water is warmer near the equator, which is where the crab sizes should be smaller. Rather perplexing!\nThe moral of the story is that water temperature and latitude are high correlated with each other, so including them both as explanatory variables leads to multicollinearity – something we do not want in our multiple linear regression.\n\n\n\nThe focus of this lab is on quantifying the relationship between water temperature (response) and latitude (explanatory) for marshes (sites) along the Atlantic coast."
  },
  {
    "objectID": "labs/lab-7.html#data",
    "href": "labs/lab-7.html#data",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "",
    "text": "Today we will explore the pie_crab dataset contained in the lterdatasampler R package. The data is from a study by Johnson et al. at the Plum Island Ecosystem Long Term Ecological Research site, studying the relationship between the size (carapace width) of a Fiddler Crab and the geographical location of its habitat. These data can be used to investigate if Bergmann’s Rule applies to Fiddler Crabs, or specifically that the size of a crab increases as the distance from the equator increases.\n\n\nThe students who investigated this relationship for their midterm project found that when both latitude and water temperature are included as explanatory variables in the multiple regression model, the coefficient associated with water temperature doesn’t make sense. Namely, the model suggests warmer water temperatures are associated with larger crab sizes. However, we know that the water is warmer near the equator, which is where the crab sizes should be smaller. Rather perplexing!\nThe moral of the story is that water temperature and latitude are high correlated with each other, so including them both as explanatory variables leads to multicollinearity – something we do not want in our multiple linear regression.\n\n\n\nThe focus of this lab is on quantifying the relationship between water temperature (response) and latitude (explanatory) for marshes (sites) along the Atlantic coast."
  },
  {
    "objectID": "labs/lab-7.html#cleaning-the-data",
    "href": "labs/lab-7.html#cleaning-the-data",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nThe data contains information on at total of 392 Fiddler Crabs caught at 13 marshes on the Atlantic coast of the United States in summer 2016. However, at each marsh, there is only one recorded water temperature. Meaning, we need to collapse our dataset to have only one observation per latitude (marsh).\n1. Fill in the code below to create a new dataset called pie_crab_clean which has 13 observations – one per latitude / marsh.\n\npie_crab_clean &lt;- pie_crab %&gt;% \n  group_by(____) %&gt;% \n  slice_sample(n = 1) %&gt;% \n  ungroup()\n\nFrom this point forward, you should use the pie_crab_clean dataset for EVERY problem. Keep in mind that you are no longer analyzing data on crabs! The dataset you have is on marshes along the Atlantic coast!"
  },
  {
    "objectID": "labs/lab-7.html#visualizing-relationships",
    "href": "labs/lab-7.html#visualizing-relationships",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Visualizing Relationships",
    "text": "Visualizing Relationships\n2. Create a scatterplot modeling the relationship between latitude (explanatory) and water temperature (response) for these 13 marshes.\nDon’t forget to add descriptive axis labels!\n3. Describe the relationship you see in the scatterplot. Be sure to address the four aspects we discussed in class: form, direction, strength, and unusual points! Keep in mind that you are no longer analyzing data on crabs! The dataset you have is on marshes along the Atlantic coast!\n\nSummarizing the Relationships\nNow that you’ve visualized the relationship, let’s summarize this relationship with some statistics. As we discussed in Week 4, there are two statistics you can use to summarize a linear relationship:\n\nthe correlation\nthe slope\n\nLet’s calculate both!\n4. Use the get_correlation() function to calculate the correlation between the water temperature and latitude for these 13 marshes.\n5. Fill in the code below to calculate the observed slope for the relationship between the water temperature (response) and latitude (explanatory).\nNote: Nothing will be output when you run this code!\n\nobs_slope &lt;- pie_crab_clean %&gt;% \n  specify(response = ____, \n          explanatory = ____) %&gt;% \n  calculate(stat = ____)"
  },
  {
    "objectID": "labs/lab-7.html#bootstrap-distribution",
    "href": "labs/lab-7.html#bootstrap-distribution",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\nNow that we have the observed slope statistic, let’s see what variability we might get in the slope statistic for other samples (marshes) we might have gotten from the population (the Atlantic coast of the US).\nAs a refresher, when we use resampling to obtain our bootstrap distribution, our steps look like the following:\nStep 1: specify() the response and explanatory variables\nStep 2: generate() lots of bootstrap resamples\nStep 3: calculate() for each of the generated() samples, calculate the statistic you are interested in\nLet’s give this a try!\n6. Fill in the code to generate 500 bootstrap slope statistics (from 500 bootstrap resamples).\n\nbootstrap &lt;- pie_crab_clean %&gt;% \n  specify(response = ____, \n          explanatory = ____) %&gt;% \n  generate(reps = ____, \n           type = ____) %&gt;% \n  calculate(stat = ____)\n\nAlright, now that we have the bootstrap slope statistics, let’s see what it looks like! Let’s use the visualize() function (not ggplot()!) to make a quick visualization of the statistics you calculated above.\n7. Use the visualize() function to create a simple histogram of your 500 bootstrap statistics.\nIt would be nice to change the x-axis label to describe what statistic is being plotted!\n\n# Code to visualize bootstrap statistics"
  },
  {
    "objectID": "labs/lab-7.html#confidence-interval",
    "href": "labs/lab-7.html#confidence-interval",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nThe next step to obtain our confidence interval! First we need to determine what percentage of statistics we want to keep in the confidence interval. 90%? 95%? 99%? 80%?\nThis seems like a study where we care a bit less about our interval capturing the true value, at least compared to something like a medical study. So, I think this could be a great instance to use an 85% confidence interval.\n8. Use the get_confidence_interval() function to find the 85% confidence interval from your bootstrap distribution, using the percentile method!\n\n# Code to obtain a 85% PERCENTILE based confidence interval\n\n9. Interpret the confidence interval you obtained in #8. Make sure to include the context of the data and the population of interest!\nJust for fun, let’s compare the confidence we obtained using a percentile method with an interval found using the SE method.\n10. Use the get_confidence_interval() function to find the 85% confidence interval from your bootstrap distribution, using the SE method!\nRemember – with the SE method, you need to specify the point estimate!\n\n# Code to obtain a 85% SE based confidence interval\n\n11. How do your confidence intervals compare? Based on the shape of the bootstrap distribution, would you expect for these methods to yield similar results?\nHint: Think about the conditions for using the SE method to obtain a confidence interval!"
  },
  {
    "objectID": "labs/lab-7.html#bootstrap-assumptions",
    "href": "labs/lab-7.html#bootstrap-assumptions",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Bootstrap Assumptions",
    "text": "Bootstrap Assumptions\nA bootstrap distribution aims to simulate the variability we’d get from other samples from our population. However, the accuracy of these samples relies on the quality of our original sample.\n12. Based on the information given, how do you feel about the assumption a bootstrap distribution makes about the original sample? What issues do you believe might prevent this assumption being appropriate?"
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Welcome to Posit Cloud!",
    "section": "",
    "text": "This is a Quarto document!\nQuarto is a software that allows you to interweave text and R code to create HTML, PDF, and Microsoft Word documents\nThere are two ways to view a Quarto document, (1) as the “Source” file, or (2) as the “Visual” file. We will only use the Visual option in this class, as it allows you to interact with Quarto similar to how you interact with Word.\n\n\nSimilar to a Word Doc, there are a variety of ways you can spice up a Quarto document! Let’s explore a few.\nQuestion 1: Using the formatting options, make a numbered list of your top three favorite animals.\nQuestion 2: Using the formatting options, insert an image of your favorite animal.\nQuestion 3: Now, change the “Formatting your Document” section name to the name of your favorite animal. Make sure your header is a level 1 – use the Header 1 formatting option!\n\n\n\nYou can differentiate the R code within a Quarto file from the body of the document, based on the gray boxes that start with an {r}.\nHere is an example of an R code chunk:\nNotice in the line after the {r} there are two lines that start with #| – this is the symbol that declares options for a code chunk. The #| label: allows us to specify a name for a code chunk, I typically choose a name that tells me what the code chunk does (e.g., load-packages, clean-data). The #| include: false option at the beginning of the code chunk controls how the code output looks in our final rendered document.\nThis code chunk has two things we want to pay attention to:\n\nThe library(tidyverse) code loads in an R package called the “tidyverse”. This is code you will have in every lab assignment for this class!\nCode comments which are denoted by a # symbol. Code comments are a way for you (and me) to write what the code is doing, without R thinking what we are writing is code it should execute.\n\n\n\n\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nQuestion 4: Do you see the above code chunk when you knit the document? Why do you think this is the case?\n\n\n\nYou can include code output in your knitted document:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\nQuestion 5: What do you think the above code does? What type of output does it give you?\nHint: You have saw this type of output on Tuesday!\n\n\n\n\nYou can also embed plots in the rendered document.\nHere is an example of a plot.\n\n\n\n\n\nQuestion 6: What do you think the echo: false option does in the above code chunk?\nQuestion 7: What do you think the mapping = aes(y = manufacturer, x = hwy)) code does?\nQuestion 8: What do you think the labs(x = \"Highway Miles Per Gallon\", y = \"Car Manufacturer\") code does?"
  },
  {
    "objectID": "labs/lab-1.html#formatting-your-document",
    "href": "labs/lab-1.html#formatting-your-document",
    "title": "Welcome to Posit Cloud!",
    "section": "",
    "text": "Similar to a Word Doc, there are a variety of ways you can spice up a Quarto document! Let’s explore a few.\nQuestion 1: Using the formatting options, make a numbered list of your top three favorite animals.\nQuestion 2: Using the formatting options, insert an image of your favorite animal.\nQuestion 3: Now, change the “Formatting your Document” section name to the name of your favorite animal. Make sure your header is a level 1 – use the Header 1 formatting option!"
  },
  {
    "objectID": "labs/lab-1.html#r-code",
    "href": "labs/lab-1.html#r-code",
    "title": "Welcome to Posit Cloud!",
    "section": "",
    "text": "You can differentiate the R code within a Quarto file from the body of the document, based on the gray boxes that start with an {r}.\nHere is an example of an R code chunk:\nNotice in the line after the {r} there are two lines that start with #| – this is the symbol that declares options for a code chunk. The #| label: allows us to specify a name for a code chunk, I typically choose a name that tells me what the code chunk does (e.g., load-packages, clean-data). The #| include: false option at the beginning of the code chunk controls how the code output looks in our final rendered document.\nThis code chunk has two things we want to pay attention to:\n\nThe library(tidyverse) code loads in an R package called the “tidyverse”. This is code you will have in every lab assignment for this class!\nCode comments which are denoted by a # symbol. Code comments are a way for you (and me) to write what the code is doing, without R thinking what we are writing is code it should execute."
  },
  {
    "objectID": "labs/lab-1.html#rendering",
    "href": "labs/lab-1.html#rendering",
    "title": "Welcome to Posit Cloud!",
    "section": "",
    "text": "When you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nQuestion 4: Do you see the above code chunk when you knit the document? Why do you think this is the case?"
  },
  {
    "objectID": "labs/lab-1.html#including-code-output",
    "href": "labs/lab-1.html#including-code-output",
    "title": "Welcome to Posit Cloud!",
    "section": "",
    "text": "You can include code output in your knitted document:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\nQuestion 5: What do you think the above code does? What type of output does it give you?\nHint: You have saw this type of output on Tuesday!"
  },
  {
    "objectID": "labs/lab-1.html#including-plots",
    "href": "labs/lab-1.html#including-plots",
    "title": "Welcome to Posit Cloud!",
    "section": "",
    "text": "You can also embed plots in the rendered document.\nHere is an example of a plot.\n\n\n\n\n\nQuestion 6: What do you think the echo: false option does in the above code chunk?\nQuestion 7: What do you think the mapping = aes(y = manufacturer, x = hwy)) code does?\nQuestion 8: What do you think the labs(x = \"Highway Miles Per Gallon\", y = \"Car Manufacturer\") code does?"
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Visualizing and Summarizing Numerical Data",
    "section": "",
    "text": "Let’s load the following packages:\n\nThe tidyverse “umbrella” package which houses a suite of many different R packages for data wrangling and data visualization\nNote: This did not work last week, but should this week!\nThe openintro R package: houses the dataset we will be working with\n\n\n# Package for functions \nlibrary(tidyverse)\n\n# Package for data\nlibrary(openintro)\n\n\n\n\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes transportation data available, such as the flights data we will be working with in this lab.\nFirst, we’ll view the nycflights data frame. Run the following code to load in the data:\n\ndata(nycflights)\n\nThe codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?nycflights\n\nRemember that you can use glimpse() to take a quick peek at your data to understand its contents better.\nQuestion 1\n(a) How large is the nycflights dataset? (i.e. How many rows and columns does it have?)\n(b) Are there numerical variables in the dataset? If so, what are their names?\n\n# You code for exercise 1 goes here! Yes, your answer should use code!\n\n\n\n\nLet’s start by examining the distribution of departure delays (dep_delay) of all flights with a histogram.\nQuestion 2 – Create a histogram of the dep_delay variable from the nycflights data\n\n# Your code for exercise 2 goes here! \n\nHistograms are generally a very good way to see the shape of a single distribution of numerical data, but that shape can change depending on how the data into different bins.\nYou can easily define the binwidth you want to use, by specifying the binwidth argument inside of geom_histogram(), like so:\ngeom_histogram(binwidth = 15)\nQuestion 3\n(a) Make two other histograms, one with a binwidth of 15 and one with a binwidth of 150.\n\n# Your code for exercise 3 goes here! \n\n\n\n\n(b) How do these three histograms compare? Are features revealed in one that are obscured in another?"
  },
  {
    "objectID": "labs/lab-2.html#getting-started",
    "href": "labs/lab-2.html#getting-started",
    "title": "Visualizing and Summarizing Numerical Data",
    "section": "",
    "text": "Let’s load the following packages:\n\nThe tidyverse “umbrella” package which houses a suite of many different R packages for data wrangling and data visualization\nNote: This did not work last week, but should this week!\nThe openintro R package: houses the dataset we will be working with\n\n\n# Package for functions \nlibrary(tidyverse)\n\n# Package for data\nlibrary(openintro)\n\n\n\n\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes transportation data available, such as the flights data we will be working with in this lab.\nFirst, we’ll view the nycflights data frame. Run the following code to load in the data:\n\ndata(nycflights)\n\nThe codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?nycflights\n\nRemember that you can use glimpse() to take a quick peek at your data to understand its contents better.\nQuestion 1\n(a) How large is the nycflights dataset? (i.e. How many rows and columns does it have?)\n(b) Are there numerical variables in the dataset? If so, what are their names?\n\n# You code for exercise 1 goes here! Yes, your answer should use code!\n\n\n\n\nLet’s start by examining the distribution of departure delays (dep_delay) of all flights with a histogram.\nQuestion 2 – Create a histogram of the dep_delay variable from the nycflights data\n\n# Your code for exercise 2 goes here! \n\nHistograms are generally a very good way to see the shape of a single distribution of numerical data, but that shape can change depending on how the data into different bins.\nYou can easily define the binwidth you want to use, by specifying the binwidth argument inside of geom_histogram(), like so:\ngeom_histogram(binwidth = 15)\nQuestion 3\n(a) Make two other histograms, one with a binwidth of 15 and one with a binwidth of 150.\n\n# Your code for exercise 3 goes here! \n\n\n\n\n(b) How do these three histograms compare? Are features revealed in one that are obscured in another?"
  },
  {
    "objectID": "labs/lab-2.html#sfo-dstinations",
    "href": "labs/lab-2.html#sfo-dstinations",
    "title": "Visualizing and Summarizing Numerical Data",
    "section": "SFO Dstinations",
    "text": "SFO Dstinations\nOne of the variables refers to the destination (i.e. airport) of the flight, which have three letter abbreviations. For example, flights into Los Angeles have a dest of \"LAX\", flights into San Francisco have a dest of \"SFO\", and flights into Chicago (O’Hare) have a dest of \"ORD\".\nIf you want to visualize only on delays of flights headed to Los Angeles, you need to first filter() the data for flights with that destination (e.g., filter(dest == \"LAX\")) and then make a histogram of the departure delays of only those flights.\nLogical operators: Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so, you can use the filter() function and a series of logical operators. The most commonly used logical operators for data analysis are as follows:\n\n== means “equal to”\n!= means “not equal to”\n&gt; or &lt; means “greater than” or “less than”\n&gt;= or &lt;= means “greater than or equal to” or “less than or equal to”\n\nQuestion 4 – Fill in the code to create a new dataframe named sfo_flights that is the result of filter()ing only the observations whose destination was San Francisco.\n\n# Fill in the code for exercise 4 here! \n\nsfo_flights &lt;- filter(nycflights, \n                      dest == )\n\n\nMultiple Data Filters\nYou can filter based on multiple criteria! Within the filter() function, each criteria is separated using commas. For example, suppose you are interested in flights leaving from LaGuardia (LGA) in February:\n\n## Remember months are coded as numbers!\nfilter(nycflights, \n       origin == \"LGA\", \n       month == 2)\n\nNote that you can separate the conditions using commas if you want flights that are both leaving from LGA and flights in February. If you are interested in either flights leaving from LGA or flights that happened in February, you can use the | instead of the comma.\nQuestion 5 – Fill in the code below to find the number of flights flying into SFO in July that arrived early.\n\n## Fill in the code for exercise 5 here! \n\nfilter(sfo_flights, \n       month == __, \n       arr_delay &gt; __) %&gt;% \n  dim()\n\nQuestion 6 – When you ran the code above it output two numbers. What do those numbers tell you about the number of flights that met your criteria (SFO, July, arrived early)?"
  },
  {
    "objectID": "labs/lab-2.html#data-summaries",
    "href": "labs/lab-2.html#data-summaries",
    "title": "Visualizing and Summarizing Numerical Data",
    "section": "Data Summaries",
    "text": "Data Summaries\nYou can also obtain numerical summaries for the flights headed to SFO, using the summarise() function:\n\nsummarise(sfo_flights, \n          mean_dd   = mean(dep_delay), \n          median_dd = median(dep_delay), \n          n         = n())\n\nNote that in the summarise() function I’ve created a list of three different numerical summaries that I’m interested in.\nThe names of these elements are user defined, like mean_dd, median_dd, n, and you can customize these names as you like (just don’t use spaces in your names!).\nCalculating these summary statistics also requires that you know the summary functions you would like to use.\nSummary statistics: Some useful function calls for summary statistics for a single numerical variable are as follows:\n\nmean(): calculates the average\nmedian(): calculates the median\nsd(): calculates the standard deviation\nvar(): calculates the variances\nIQR(): calculates the inner quartile range (Q3 - Q1)\nmin(): finds the minimum\nmax(): finds the maximum\nn(): reports the sample size\n\nNote that each of these functions takes a single variable as an input and returns a single value as an output."
  },
  {
    "objectID": "labs/lab-2.html#summaries-vs.-visualizations",
    "href": "labs/lab-2.html#summaries-vs.-visualizations",
    "title": "Visualizing and Summarizing Numerical Data",
    "section": "Summaries vs. Visualizations",
    "text": "Summaries vs. Visualizations\nIf I’m flying from New York to San Francisco, should I expect that my flights will typically arrive on time?\nLet’s think about how you could answer this question. One option is to summarize the data and inspect the output. Another option is to plot the delays and inspect the plots. Let’s try both!\nQuestion 7 – Calculate the following statistics for the arrival delays in the sfo_flights dataset:\n\nmean\nmedian\nmax\nmin\n\n\n## Code for exercise 7 goes here! \n\nQuestion 8 – Using the above summary statistics, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nQuestion 9 – Now, rather than calculating summary statistics, plot the distribution of arrival delays for the sfo_flights dataset.\nChoose the type of plot you believe is appropriate for visualizing the distribution of departure delays. Don’t forget to give your visualization informative axis labels!\n\n## Code for exercise 9 goes here! \n\nQuestion 10 – Using the plot above, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nQuestion 11 – How did your answer change when using the plot versus using the summary statistics? i.e. What were you able to see in the plot that could could not “see” with the summary statistics?"
  },
  {
    "objectID": "labs/lab-8.html",
    "href": "labs/lab-8.html",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gapminder)\nlibrary(infer)\nlibrary(moderndive)"
  },
  {
    "objectID": "labs/lab-8.html#question-of-interest",
    "href": "labs/lab-8.html#question-of-interest",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Question of Interest",
    "text": "Question of Interest\nThe objective of this data analysis is to answer the question:\n\nWhat is the relationship between life expectancy GDP per capita?"
  },
  {
    "objectID": "labs/lab-8.html#data-visualization",
    "href": "labs/lab-8.html#data-visualization",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Data Visualization",
    "text": "Data Visualization\n1. Create a scatterplot of the relationship between life expectancy (response) and GDP (explanatory).\nRemember to include nice axis labels (with units!).\nWhat you see should make you concerned about using a linear regression! So, let’s play with some variable transformations.\nYou can explore if a log-transformation of the y-variable would make the relationship more linear by adding a scale_y_log10() layer to your plot, like so:\n\nlterdatasampler::hbr_maples %&gt;% \n  ggplot(mapping = aes(x = stem_length, y = stem_dry_mass)) +\n  geom_point() + \n  scale_y_log10()\n\nSimilarly, you can a log-transformation of the x-variable would be helpful by adding a scale_x_log10() layer to your plot.\n2. Using scale_x_log10() and scale_y_log10(), decide on what relationship between life expectancy and GDP per capita appears the most linear. There should only be one plot for this problem!\nRemember to include nice axis labels (with any transformed units!)."
  },
  {
    "objectID": "labs/lab-8.html#assessing-model-conditions",
    "href": "labs/lab-8.html#assessing-model-conditions",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Assessing Model Conditions",
    "text": "Assessing Model Conditions\nThe next step is to check the conditions of our statistical model, we do this by analyzing our residuals and how the data were collected.\n\nIndependence of Observations\nEach row of the gapminder dataset is an observation for one country for one year (from 1952 to 2007).\n4. Do you believe is it reasonable to assume these observations are independent of one another?\nHint: This condition says the rows of the dataset are independent of each other. Look at the rows of the dataset, is there any reason to believe there are relationships between the rows?\n\n\nNormality of Residuals\nI’ve provided code to visualize the residuals from the model you fit in #3 below.\n\nbroom::augment(gapminder_lm) %&gt;% \n  ggplot(mapping = aes(x = .resid)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Residual\")\n\n5. Based on the distribution of residuals, do you believe the condition of normality is violated? Why or why not?\n\n\nEqual Variance of Residuals\nI’ve provided code to visualize the residuals versus fitted values from the model you fit in #3 below. With this plot, we want to assess if the variability (spread) of the residuals changes based on the values of the explanatory variable.\n\nbroom::augment(gapminder_lm) %&gt;% \n  ggplot(mapping = aes(y = .resid, x = `log(gdpPercap)`)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\", linewidth = 3) +\n  labs(x = \"Log Transformed GDP Per Capita\")\n\n6. Based on the plot above, do you believe the condition of equal variance is violated? Why or why not?"
  },
  {
    "objectID": "labs/lab-8.html#stating-the-hypotheses",
    "href": "labs/lab-8.html#stating-the-hypotheses",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Stating the Hypotheses",
    "text": "Stating the Hypotheses\nNow that you’ve decided which regression appears the most linear, let’s perform a hypothesis test for the slope coefficient.\n7. Write the hypotheses in words for testing if there is a linear relationship between the variables you used for your model in #3.\nKeep in mind, if you log-transformed y, you are testing if there is a linear relationship between log(y) and x!\n\\(H_0\\):\n\\(H_A\\):"
  },
  {
    "objectID": "labs/lab-8.html#obtaining-a-p-value-using-simulation",
    "href": "labs/lab-8.html#obtaining-a-p-value-using-simulation",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Obtaining a p-value Using Simulation",
    "text": "Obtaining a p-value Using Simulation\nNext, we will work through creating a permutation distribution using tools from the infer package.\n8. First, we need to find the observed slope statistic, which we will save as obs_slope.\nKeep in mind, if you log-transformed y, you need to use log(y) as your response variable!\n\nobs_slope &lt;-  gapminder %&gt;%\n  specify(response = ____, explanatory = ____) %&gt;%\n  calculate(stat = \"slope\")\n\nAfter you have calculated your observed statistic, you need to create a permutation distribution of statistics that might have occurred if the null hypothesis was true.\n9. Generate 500 permuted statistics for the permutation distribution and save these statistics in an object named null_dist.\n\nnull_dist &lt;- \n\nWe can visualize this null distribution with the following code:\n\nvisualise(null_dist) \n\n10. Use the shade_p_value() function to add (+) a vertical red line to the plot above, demonstrating where the observed slope statistic (obs_slope) falls on the distribution.\nAdd your code to the plot above!\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n11. Fill in the code below to calculate the p-value for the hypothesis test you stated in #7.\n\nget_p_value(null_dist, \n            obs_stat = ____, \n            direction = ____)\n\n12. Based on your p-value and an \\(\\alpha = 0.1\\), what decision would you reach regarding the hypotheses you stated in #7?"
  },
  {
    "objectID": "labs/lab-8.html#obtaining-a-p-value-using-theory",
    "href": "labs/lab-8.html#obtaining-a-p-value-using-theory",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Obtaining a p-value Using Theory",
    "text": "Obtaining a p-value Using Theory\nAs we saw in the reading this week, the output from the get_regression_table() function provides us with theory-based estimates of our standard error, \\(t\\)-statistic, and p-value.\n13. Use the get_regression_table() function to obtain the theory-based p-value for your hypothesis test.\nHint: You’ll want to use the model you fit in #3.\n14. How does this p-value compare to what you obtained in #11?\n15. Why do you believe these p-values were similar or different?\n16. Based on your answers to #4-6, which p-value do you believe is the most reliable? Why?\nNote: If you believe neither are reliable, say so and state why."
  },
  {
    "objectID": "labs/lab-9.html",
    "href": "labs/lab-9.html",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(infer)\nlibrary(ggridges)\nlibrary(broom)"
  },
  {
    "objectID": "labs/lab-9.html#todays-data",
    "href": "labs/lab-9.html#todays-data",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "Today’s Data",
    "text": "Today’s Data\nThese data come from the Gapminder Foundation, an organization interested in increasing the use and understanding of statistics and other information about social, economic and environmental development at local, national and global levels.\nToday we will be comparing math achievement scores across continents and years. Math achievement was measured for 42 countries based on their average score for the grade 8 international TIMSS test.\n\nmath_scores &lt;- read_csv(here::here(\"labs\", \n                                   \"data\",\n                                   \"math_scores.csv\")\n                        )\n\n# Creating a year_cat variable that is the categorical version of year\nmath_scores &lt;- mutate(math_scores, \n                      year_cat = as.factor(year)\n                      )\n\n# Removing the missing values from the grade_8_math_score variable\nmath_scores &lt;- drop_na(data = math_scores, \n                       grade_8_math_score)"
  },
  {
    "objectID": "labs/lab-9.html#data-visualizations",
    "href": "labs/lab-9.html#data-visualizations",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "Data Visualizations",
    "text": "Data Visualizations\nThe first step for a statistical analysis should always be creating visualizations of the data. Similar to what you are expected to do for your project, you will make three density ridge plots:\n\nvisualizing the relationship between math score and year\nvisualizing the relationship between math score and continent\nvisualizing the relationship between math score with both year and continent\n\nQuestion 1 – Fill in the code below to visualize the distribution of grade 8 math scores over time.\nDon’t forget to include axis labels!\n\nggplot(data = math_scores, \n       mapping = aes(x = ____, \n                     y = ____)) +\n  geom_density_ridges(scale = 1) \n\nNote: I’ve included a scale = 1 argument to show you how you can get the density plots not to overlap!\nQuestion 2 – What do you see in the plot you made? How do the centers (means) of the distributions compare? What about the variability (spread) of the distributions?\nQuestion 3 – Write the code to visualize the distribution of grade 8 math scores for the six different continents.\nDon’t forget to include axis labels!\nQuestion 4 – What do you see in the plot you made? How do the centers (means) of the distributions compare? What about the variability (spread) of the distributions?\nQuestion 5 – Write the code to visualize the distribution of grade 8 math scores for the six different continents for each of the four years.\nRemember, you could either include a facet or a color here!Also remember you can use alpha to change the transparency of your density ridges!\nQuestion 6 – What do you see in the plot you made? Does it seem that the relationship between year and grade 8 math scores changes based on the continent of the student?"
  },
  {
    "objectID": "labs/lab-9.html#statistical-model",
    "href": "labs/lab-9.html#statistical-model",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "Statistical Model",
    "text": "Statistical Model\nFor our analysis we will be using an analysis of variance (ANOVA) model. An ANOVA is an appropriate statistical model as we have a continuous response variable (grade 8 math score) and categorical explanatory variables (year, continent). Year is not considered to be a continuous numerical variable as we have only four measurements in time (1996, 1999, 2003, 2007).\n\nModel Conditions\nAn ANOVA has model conditions that are very similar to what we learned for linear regression. In this section we will evaluate the conditions of the model.\nFor this section, it might be helpful to know how many observations there are for each year and for each continent. I have written code below to provide you with a table of these numbers:\n\ncount(math_scores, continent, year) %&gt;% \n  pivot_wider(names_from = continent, \n              values_from = n, \n              values_fill = 0) %&gt;% \n  janitor::adorn_totals(where = c(\"row\", \"col\"))\n\n\nIndependence\nBased on the table we know:\n\neach year has measurements on about six continents\neach continent has measurements for about four years\n\nUse this information to evaluate the condition of independence of observations.\nQuestion 7 – Is it reasonable to assume that the observations within a continent are independent of each other?\nQuestion 8 – Is it reasonable to assume that the observations within a year are independent of each other?\nQuestion 9 – Is it reasonable to assume that the observations between continents are independent of each other?\nQuestion 10 – Is it reasonable to assume that the observations between a years are independent of each other?\n\n\nNormality\nNow we will evaluate the normality of the the distributions of grade 8 math scores across years and across continents – the plot you created in #5. Keep in mind, the normality condition is very important when the sample sizes for each group are relatively small.\nQuestion 11 – Is it reasonable to say that the grade 8 math scores across the four years and six continents are normally distributed?\n\n\nEqual Variance\nNow we will evaluate the normality of the the distributions of grade 8 math scores across years and across continents – the plot you created in #5. Keep in mind, the constant variance condition is especially important when the sample sizes differ between groups.\nFor this section, it might be helpful to know the standard deviations for each year / continent combo. I have written code below to provide you with a table of these numbers:\nKeep in mind a standard deviation of NA can happen for two reasons, (1) there is no data, or (2) there is only one observation.\n\nmath_scores %&gt;% \n  group_by(year, continent) %&gt;% \n  summarize(var = var(grade_8_math_score, na.rm = TRUE)\n            ) %&gt;% \n  pivot_wider(names_from = continent, values_from = var)\n\nLooking at the table, we can see that the largest variance of 10257 (North America, 2007) is nearly 27 times larger than the smallest variance of 381 (Europe, 2003). That’s a lot! So, our equal variance condition is definitely violated.\nBut, we have learned tools to attempt to remedy this issue! Let’s take the log of grade_8_math_score and see how the variances compare.\n\nmath_scores %&gt;% \n  group_by(year, continent) %&gt;% \n  summarize(log_var = var(log(grade_8_math_score))\n            ) %&gt;%\n  pivot_wider(names_from = continent, values_from = log_var)\n\nQuestion 12 – Based on the variances in the table above, is it reasonable to say that the log grade 8 math scores across the four years and six continents have equal variability?"
  },
  {
    "objectID": "labs/lab-9.html#one-way-anova-inference",
    "href": "labs/lab-9.html#one-way-anova-inference",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "One-Way ANOVA Inference",
    "text": "One-Way ANOVA Inference\nWe are going to test out both methods for conducting a hypothesis test for an ANOVA – theory-based and simulation-based methods. Keep in mind both methods require independence of observations and equal variability. Normality, however, is only a condition of theory-based methods.\n\nTesting for a Difference Between Years\nSince the distribution of grade 8 math scores across the four years wasn’t horribly not Normal, let’s give a theory-based method a try.\nQuestion 13 – Fill in the code below to conduct a one-way ANOVA modeling the relationship between mean grade 8 math score and the year\nKeep in mind the response variable comes first and the explanatory variable comes second!\n\naov(____ ~ ____, data = math_scores) %&gt;% \n  broom::tidy()\n\nQuestion 14 – At an \\(\\alpha = 0.1\\), what decision would you reach for your hypothesis test?\nQuestion 15 – What would you conclude about the relationship between the mean grade 8 math scores and year?\n\n\nTesting for a Difference Between Continents\nSince the distribution of grade 8 math scores across the six continents didn’t look very Normal, so let’s give a simulation-based method a try.\nI’ve gotten you started by calculating the observed F-statistic for the relationship between a country’s grade 8 math score and its continent.\n\nobs_F &lt;- math_scores %&gt;% \n  specify(response = grade_8_math_score, \n          explanatory = continent) %&gt;% \n  calculate(stat = \"F\")\n\nQuestion 16 – Write the code to generate a permutation distribution of resampled F-statistics.\nQuestion 17 – Visualize the null distribution and shade how the p-value should be calculated\nKeep in mind you only look at the right tail for an ANOVA!\nQuestion 18 – Calculate the p-value for the observed F-statistic\nQuestion 19 – At an \\(\\alpha = 0.1\\), what decision would you reach for your hypothesis test?\nQuestion 20 – What would you conclude about the relationship between the mean grade 8 math scores and continent?"
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using packages housed in the tidyverse suite of packages.\n\n## Package for ggplot and dplyr tools\nlibrary(tidyverse)\n\n## Package for ecological data\nlibrary(lterdatasampler)\n\n## Package for density ridge plots\nlibrary(ggridges)\n\n\n\n\nIn this lab we will work with data from the H.J. Andrews Experimental Forest. The following is a description of the data:\n\nPopulations of West Slope cutthroat trout (Onchorhyncus clarki clarki) in two standard reaches of Mack Creek in the H.J. Andrews Experimental Forest have been monitored since 1987. Monitoring of Pacific Giant Salamanders, Dicamptodon tenebrosus began in 1993. The two standard reaches are in a section of clearcut forest (ca. 1963) and an upstream 500 year old coniferous forest. Sub-reaches are sampled with 2-pass electrofishing, and all captured vertebrates are measured and weighed. Additionally, a set of channel measurements are taken with each sampling. This study constitutes one of the longest continuous records of salmonid populations on record.\n\nFirst, we’ll view the and_vertebrates dataframe where these data are stored.\n\nView(and_vertebrates)\n\n\n\n\nThe codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?and_vertebrates\n\nQuestion 1 – How large is the and_vertebrates dataset? (i.e. How many rows and columns does the dataset have?)\n\n## Your code for question 1 (and 2) goes here!\n\nQuestion 2 – Are there categorical variables in the dataset? If so, what are their names?\n\n\n\nThe species variable refers to the species of the animal which was captured. You can use the distinct() function to access the distinct values of a categorical variable (e.g., distinct(nycflights, carrier)). Notice the first input is the name of the dataset and the second input is the name of the categorical variable!\nQuestion 3 – Use the distinct() function to discover the levels / values of the species variable.\n\n## Your code for question 3 goes here!"
  },
  {
    "objectID": "labs/lab-3.html#load-packages",
    "href": "labs/lab-3.html#load-packages",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using packages housed in the tidyverse suite of packages.\n\n## Package for ggplot and dplyr tools\nlibrary(tidyverse)\n\n## Package for ecological data\nlibrary(lterdatasampler)\n\n## Package for density ridge plots\nlibrary(ggridges)"
  },
  {
    "objectID": "labs/lab-3.html#the-data",
    "href": "labs/lab-3.html#the-data",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "In this lab we will work with data from the H.J. Andrews Experimental Forest. The following is a description of the data:\n\nPopulations of West Slope cutthroat trout (Onchorhyncus clarki clarki) in two standard reaches of Mack Creek in the H.J. Andrews Experimental Forest have been monitored since 1987. Monitoring of Pacific Giant Salamanders, Dicamptodon tenebrosus began in 1993. The two standard reaches are in a section of clearcut forest (ca. 1963) and an upstream 500 year old coniferous forest. Sub-reaches are sampled with 2-pass electrofishing, and all captured vertebrates are measured and weighed. Additionally, a set of channel measurements are taken with each sampling. This study constitutes one of the longest continuous records of salmonid populations on record.\n\nFirst, we’ll view the and_vertebrates dataframe where these data are stored.\n\nView(and_vertebrates)"
  },
  {
    "objectID": "labs/lab-3.html#exploring-the-dataset",
    "href": "labs/lab-3.html#exploring-the-dataset",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "The codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?and_vertebrates\n\nQuestion 1 – How large is the and_vertebrates dataset? (i.e. How many rows and columns does the dataset have?)\n\n## Your code for question 1 (and 2) goes here!\n\nQuestion 2 – Are there categorical variables in the dataset? If so, what are their names?"
  },
  {
    "objectID": "labs/lab-3.html#accessing-the-levels-of-a-variable",
    "href": "labs/lab-3.html#accessing-the-levels-of-a-variable",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "The species variable refers to the species of the animal which was captured. You can use the distinct() function to access the distinct values of a categorical variable (e.g., distinct(nycflights, carrier)). Notice the first input is the name of the dataset and the second input is the name of the categorical variable!\nQuestion 3 – Use the distinct() function to discover the levels / values of the species variable.\n\n## Your code for question 3 goes here!"
  },
  {
    "objectID": "labs/lab-3.html#adding-categorical-variables",
    "href": "labs/lab-3.html#adding-categorical-variables",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "Adding Categorical Variables",
    "text": "Adding Categorical Variables\nWhen we are interested in comparing the distribution of a numerical variable across groups of a categorical variable, we “typically” see people use stacked histograms or side-by-side boxplots. I believe an unsung hero of these types of comparisons is the ridge plot.\nAs introduced in Introduction to Modern Statistics, a ridge plot essentially has multiple density plots stacked in the same plotting window. A key feature of ridge plots is a categorical variable is always on the y-axis, with a numeric variable on the x-axis.\nIn R, we use the geom_density_ridges() function from the ggridges package to create a ridge plot. Yes, this is new, but don’t worry! The function has the same layout as things you’ve seen before.\nQuestion 8 – Fill in the code below to create a ridge plot comparing the lengths of Cutthroat trout between the different types of channels (unittype).\nBe sure to add nice axis labels to your plot, which describe the variables being plotted (and their units)!\n\n## Your code for question 8 goes here!\n\nggplot(data = trout, \n       mapping = aes(x = &lt;NUMERICAL VARIABLE&gt;, \n                     y = &lt;CATEGORICAL VARIABLE&gt;)\n       ) +\n  geom_density_ridges() \n\nQuestion 9 – Modify your plot from #8 to incorporate the section of the forest into your plot using either color or facets.\nHint: The fill aesthetic will fill the ridge plots with color.\nQuestion 10 – Based on your plot, how different are the lengths of the Cutthroat trout between the different channel types and forest sections?"
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4: Simple Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lterdatasampler)"
  },
  {
    "objectID": "labs/lab-4.html#old-packages",
    "href": "labs/lab-4.html#old-packages",
    "title": "Lab 4: Simple Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lterdatasampler)"
  },
  {
    "objectID": "labs/lab-4.html#new-package",
    "href": "labs/lab-4.html#new-package",
    "title": "Lab 4: Simple Linear Regression",
    "section": "New Package!",
    "text": "New Package!\n\nlibrary(moderndive)"
  },
  {
    "objectID": "labs/lab-4.html#data-for-today",
    "href": "labs/lab-4.html#data-for-today",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Data for Today",
    "text": "Data for Today\nToday we’ll be working with data on lake ice duration for two lakes surrounding Madison, Wisconsin. This dataset contains information on the number of days of ice (ice duration) on each lake for years between 1851 and 2019. These data are stored in the ntl_icecover dataset, which lives in the lterdatsampler package.\nAccording to the EPA, lake ice duration can be an indicator of climate change. This is because lake ice is dependent on several environmental factors, so changes in these factors will influence the formation of ice on top of lakes. As a result, the study and analysis of lake ice formation can inform scientists about how quickly the climate is changing, and are critical to minimizing disruptions to lake ecosystems."
  },
  {
    "objectID": "labs/lab-4.html#inspecting-the-data",
    "href": "labs/lab-4.html#inspecting-the-data",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Inspecting the Data",
    "text": "Inspecting the Data\nQuestion 1 – How large is the ntl_icecover dataset? (i.e. How many rows and columns does it have?)\n\n# Code to answer question 1 goes here!"
  },
  {
    "objectID": "labs/lab-4.html#visualize-a-simple-linear-regression",
    "href": "labs/lab-4.html#visualize-a-simple-linear-regression",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Visualize a Simple Linear Regression",
    "text": "Visualize a Simple Linear Regression\nLet’s start with tools to visualize and summarize linear regression.\n\nTools\n\nVisualize the relationship between x & y – geom_point()\nVisualize the linear regression line – geom_smooth()\n\nWe will be investigating the relationship between the ice_duration of each lake and the year.\n\n\nStep 1\nQuestion 2 – Make a scatterplot of the relationship between the ice_duration (response) and the year (explanatory).\nBe sure to make the axis labels look nice, including any necessary units!\n\n# Code to answer question 2 goes here!\n\nQuestion 3 – Describe the relationship you see in the scatterplot. Be sure to address the four aspects we discussed in class: form, direction, strength, and unusual points.\nHint: You need to explicitly state where the unusual observations are!\n\n\nStep 2\nTo add a regression line on top of a scatterplot, you add (+) a geom_smooth() layer to your plot. However, if you add a “plain” geom_smooth() to the plot, it uses a wiggly line. You need to tell geom_smooth() what type of smoother line you want for it to use! We can get a straight line by including method = \"lm\" inside of geom_smooth().\nQuestion 4 – Add a linear regression line to the scatterplot you made in Question 3.\nNo code goes here, you need to modify your scatterplot from Question 3!"
  },
  {
    "objectID": "labs/lab-4.html#fit-a-simple-linear-regression-model",
    "href": "labs/lab-4.html#fit-a-simple-linear-regression-model",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Fit a Simple Linear Regression Model",
    "text": "Fit a Simple Linear Regression Model\nNext, we are going to summarize the relationship between ice_duration and year with a linear regression equation.\n\nTools\n\nCalculate the correlation between x & y – get_correlation()\nModel the relationship between x & y – lm()\nExplore coefficient estimates – get_regression_table()\n\n\n\nStep 1\nQuestion 5 – Calculate the correlation between these variables, using the get_correlation() function.\n\n# Code to answer question 5 goes here!\n\n\n\nStep 2\nNext, we will “fit” a linear regression with the lm() function. Remember, the “formula” for lm() is response_variable ~ explanatory_variable. Also recall that you need to tell lm() where the data live using data =!\nQuestion 6 – Fit a linear regression modeling the relationship between between ice_duration and year. Save your linear regression into an object named ice_lm (using the &lt;- assignment arrow) so you can use it later.\nRemember what order you need to put the response and explanatory variables!\n\n# Code to answer question 6 goes here!\n\n\n\nStep 3\nFinally, to get the regression equation, we need grab the coefficients out of the linear model object you made in Step 2. The get_regression_table() function is a handy tool to do just that!\nQuestion 7 – Use the get_regression_table() function to obtain the coefficient estimates for the ice_lm regression you fit in Question 6.\n\n# Code to answer question 7 goes here!\n\nQuestion 8 – Using the coefficient estimates above, write out the estimated regression equation.\nYour equation needs to be in the context of the variables, not in generic \\(x\\) and \\(y\\) statements!\nQuestion 9 – Interpret the value of the slope coefficient.\nYour interpretation needs to be in the context of the variables, not in generic \\(x\\) and \\(y\\) statements!\nQuestion 10 – What do you expect to happen to the duration of ice if the number of years is increased by 100?"
  },
  {
    "objectID": "labs/lab-4.html#a-preview-of-whats-to-come",
    "href": "labs/lab-4.html#a-preview-of-whats-to-come",
    "title": "Lab 4: Simple Linear Regression",
    "section": "A preview of what’s to come",
    "text": "A preview of what’s to come\nIn our analysis above, we only looked at the relationship between ice duration and year, not accounting for which lake the duration was for. That is another explanatory variable we could include in our regression model!\nQuestion 11 – Using the code you wrote for Question 2 (with the regression line added), add a color for the name of the lake (lakeid).\n\n# Code to answer question 11 goes here!"
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Predicting Professor Evaluation Scores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(moderndive)\n\n# This is needed to make sure everyone gets the SAME testing / training datasets!\nset.seed(1234)\n\nevals &lt;- evals |&gt; \n  mutate(large_class = if_else(cls_students &gt; 100, \n                               \"large class\", \n                               \"regular class\"), \n         eval_completion = cls_did_eval / cls_students \n         ) |&gt; \n  select(-cls_did_eval, \n         -cls_students)"
  },
  {
    "objectID": "labs/lab-6.html#your-challenge",
    "href": "labs/lab-6.html#your-challenge",
    "title": "Predicting Professor Evaluation Scores",
    "section": "Your Challenge",
    "text": "Your Challenge\nThis week you have learned about model selection. During class you worked on performing a backward selection process to determine the “best” model for penguin body mass.\nToday, you are going to use forward selection to determine the “best” model for professor’s evaluation score. There is an extra layer to this challenge, you are going to test how well your model predicts the evaluation scores of professors not in your dataset. The group(s) with the lowest prediction error will win two extra tokens.\nThis task will require you to fit tons of linear regressions. You must be able to show me exactly how you got to your top model. Meaning, I need to see a record of every model you fit and compared along the way.\n\nYour Datasets\nI’ve used the slice_sample() function to divide the evals dataset into two datasets:\n\nevals_train contains 80% of the observations in the original evals dataset – this is the dataset you will use for your models\nevals_test contains the other 20% of observations – this is the dataset you will use for your predictions\n\n\nevals_train &lt;- slice_sample(evals, prop = 0.8)\nevals_test &lt;- anti_join(evals, evals_train, by = \"ID\")"
  },
  {
    "objectID": "labs/lab-6.html#forward-selection",
    "href": "labs/lab-6.html#forward-selection",
    "title": "Predicting Professor Evaluation Scores",
    "section": "Forward Selection",
    "text": "Forward Selection\nThe forward selection process starts with a model with no predictor variables. That means, this model predicts the same mean evaluation score for every professor. I’ve fit this model for you below!\n\none_mean &lt;- lm(score ~ 1, data = evals_train)\n\nYou can pull out the adjusted \\(R^2\\) for this model using the get_regression_summaries() function.\n\nget_regression_summaries(one_mean)\n\nBased on this output, we are starting with a really low adjusted \\(R^2\\). So, things can only get better from here!\n\nStep 1\nRules: You can only add a variable to the model if it improves the adjusted \\(R^2\\) by at least 1% (0.01).\nAlright, so now we get cooking. The next step is to fit every model with one explanatory variable. I’ve provided a list of every explanatory variable you are allowed to consider!\n\nprof_ID – identification variable for each professor\nage – age of the professor\nbty_avg – average beauty rating of the professor\ngender – gender of the professor\nethnicity – ethnicity of the professor\nlanguage – language of school where professor received education\nrank – rank of professor\npic_outfit – outfit of professor in picture\npic_color – color of professor’s picture\nlarge_class – whether the class had over 100 students\neval_completion – proportion of students who completed the evaluation\ncls_level – class level\n\nWoof, that’s 12 different variables. That means, for this first round, you will need to compare the adjusted \\(R^2\\) for 12 different models to decide what variable should be added.\nEvery model you fit will have the same format:\nname_of_model &lt;- lm(score ~ &lt;variable&gt;, data = evals_train)\nBut, the name of the model will need to change. I’ve started the process for you, using the naming style of one_ followed by the variable name (e.g., one_id, one_bty, etc.).\n\none_id &lt;- lm(score ~ prof_ID, data = evals_train)\none_age &lt;- lm(score ~ age, data = evals_train)\none_bty &lt;- lm(score ~ bty_avg, data = evals_train)\none_gender &lt;- lm(score ~ gender, data = evals_train)\none_ethnicity &lt;- lm(score ~ ethnicity, data = evals_train)\none_language &lt;- lm(score ~ language, data = evals_train)\n\n## Now, you need to fit the other six models! \n\nAlright, now that you’ve fit the models, you need to inspect the adjusted \\(R^2\\) values to see which of these 12 models is the “top” model – the model with the highest adjusted \\(R^2\\)! Similar to before, I’ve provided you with some code to get you started, but you need to write the remaining code.\n\nget_regression_summaries(one_id)\nget_regression_summaries(one_age)\nget_regression_summaries(one_bty)\nget_regression_summaries(one_gender)\nget_regression_summaries(one_ethnicity)\nget_regression_summaries(one_language)\n\n## Now, you need to compare the other six models! \n\n1. What model was your top model? Specifically, which variable was selected to be included?\n\n\nStep 2\nAlright, you’ve added one variable, the next step is to decide if you should add a second variable. This process looks nearly identical to the previous step, with one major change: every model you fit needs to contain the variable you decided to add. So, if you decided to add the bty_avg variable, every model you fit would look like this:\nname_of_model &lt;- lm(score ~ bty_avg + &lt;variable&gt;, data = evals_train)\nAgain, the name of the model will need to change. This round, you are on your own – I’ve provided you with no code. Here are my recommendations:\n\nname each model two_ followed by the names of both variables included in the model (e.g., two_bty_id)\ngo through each variable step-by-step just like you did before\n\n\n## Code to fit all 11 models that add a second variable to your top model goes here!\n\nAlright, now you should have 11 more models to compare! Like before, you need to inspect the adjusted \\(R^2\\) values to see which of these 101models is the “top” model.\nRules: You can only add a variable to the model if it improves adjusted \\(R^2\\) by at least 1% (0.01) from the model you chose in Question 1.\n\n## Code to compare all 11 models you fit goes here!\n\n2. What model was your top model? You must state which variables are included in the model you chose!\n\n\nStep 3\nAs you might have expected, in this step we add a third variable to our top model from the previous step. This process should be getting familiar at this point!\nThis process of fitting 10-12 models at a time is getting rather tedious! So, I’ve written some code that will carry out this process for us in one pipeline! This is how what the code looks:\nevals_train %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\nWoah, that’s a lot. The only thing you need to change is:\n\nadd in the names of the variables you selected in Steps 1 & 2 in the ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train) step\nadd in the names of the variables you selected in Steps 1 & 2 in the select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) step\n\n\n## Code to fit all  models that add a third variable to your top model goes here!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1 & 2\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1 & 2\n\nevals_train %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 3 and see if it improved adjusted** \\(R^2\\) by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 2 is the “best” model!\n3. What model was your top model? You must state which variables are included in the model you chose!\n\n\nStep 4\nAs you might have expected, in this step we add a fourth variable to our top model from the previous step. We’re again going to use the code that allows for us to fit lots of regressions without having to type them all out.\n\n## Code to fit all  models that add a forth variable to your top model goes here!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1, 2, & 3\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1, 2 & 3\n\nevals_train %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 3 and see if it improved adjusted** \\(R^2\\) by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the “best” model!\n4. What model was your top model? You must state which variables are included in the model you chose!\n\n\nStep 5\nAs you might have expected, in this step we add a fifth variable to our top model from the previous step. We’re again going to use the code that allows for us to fit lots of regressions without having to type them all out.\n\n## Code to fit all  models that add a forth variable to your top model goes here!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1, 2, 3 & 4\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1, 2, 3 & 4\n\nevals_train %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 4 and see if it improved adjusted** \\(R^2\\) by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the “best” model!\n5. What model was your top model? You must state which variables are included in the model you chose!\n\n\nStep 6\nAs you might have expected, in this step we add a sixth variable to our top model from the previous step. We’re again going to use the code that allows for us to fit lots of regressions without having to type them all out.\n\n## Code to fit all  models that add a forth variable to your top model goes here!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1, 2, 3, 4 & 5\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1, 2, 3, 4 & 5\n\nevals_train %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 5 and see if it improved adjusted** \\(R^2\\) by at least 1% (0.01). If so, this variable should be added. If not, then your model from Step 3 is the “best” model!\n6. What model was your top model? You must state which variables are included in the model you chose!"
  },
  {
    "objectID": "labs/lab-6.html#testing-your-top-model",
    "href": "labs/lab-6.html#testing-your-top-model",
    "title": "Predicting Professor Evaluation Scores",
    "section": "Testing Your Top Model",
    "text": "Testing Your Top Model\nIf you choose to test the data on a different model than what you said in #6, you must justify why you are using a different model!\n\ntop_model &lt;- lm(score ~ &lt;ALL VARIABLES SELECTED&gt;, data = evals_train)\n\nevals_test %&gt;% \n  mutate(predictions = predict(top_model, newdata = evals_test), \n         residuals = score - predictions) %&gt;%\n  summarize(RMSE = sd(residuals))"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html",
    "href": "labs/grading-guides/lab-8-grading-guide.html",
    "title": "Lab 8 Grading Guide",
    "section": "",
    "text": "Q1:\nQ2:\nQ3:\nQ4:\nQ5:\nQ6:\nQ7:\nQ8:\nQ9:\nQ10:\nQ11:\nQ12:\nQ13:\nQ14:\nQ15:\nQ16:"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-1",
    "title": "Lab 8 Grading Guide",
    "section": "Question 1",
    "text": "Question 1\nTo earn a Success: Creates a scatterplot with the following qualities:\n\nlife expectancy on y-axis\nGDP per capita on the x-axis\naxis labels for both axes, including units\n\nIf they swap \\(x\\) and \\(y\\):\n\nCareful! What variable should be the response variable (located on the y-axis)?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-2",
    "title": "Lab 8 Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\n\ntakes log of GDP (using scale_x_log10())\nrenames x-axis label to indicate log GDP (or log $) is what is being plotted\n\nIf they log transform life expectancy (y):\n\nSimilar to Occam’s Razor, we want to transform as few variables as is necessary. Look back at the plot with just x log transformed. Is log transforming y and x notibly better?\n\nIf their x-axis doesn’t indicate the log was taken:\n\nCareful! You axis label needs to indicate how the variable was transformed!"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-3",
    "title": "Lab 8 Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success: Code should look like the following:\n\ngapminder_lm &lt;- lm(lifeExp ~ log(gdpPercap), data = gapminder)\n\nIf they swap lifeExp and gdpPercap:\n\nCareful! What variable comes first in the lm() function? The response or explanatory variable?\n\nIf they don’t use log(gdpPercap) when fitting their regression:\n\nWhat variable did you decide to transform in #2? How should that variable appear in the regression model you are fitting here?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-4",
    "title": "Lab 8 Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\n\nSays “no”, it is not reasonable to assume the observations are independent\nDescribes how observations are not independent citing at least one of the following:\n\neach country has repeated observations\nobservations for each country are related in time\ncountries of close geographical proximity may share information\n\n\nIf their justification doesn’t talk about the context of the data (e.g., temporal relationships between observations):\n\nYour justification needs to make direct reference to the context of the data.\n\nIf their reasoning doesn’t include any of the above justifications:\n\nHow many observations are their for each country? Are these observations related in some way? If so, how? Are the observations between countries related? If so, how?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-5",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-5",
    "title": "Lab 8 Grading Guide",
    "section": "Question 5",
    "text": "Question 5\nTo earn a Success:\n\n\n\nSays the condition is violated\nJustifies with the left skew of the distribution\n\n\n\n\n\nSays the condition is not violated\nJustifies with characteristics of the distribution\n\n\n\nIf their justification is insufficient:\n\nWhen evaluting conditions the choices are subjective, so it is necessary for you to justify WHY you made the decision you did. Your justification should make direct reference to characteristics of the distribution of residuals."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-6",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-6",
    "title": "Lab 8 Grading Guide",
    "section": "Question 6",
    "text": "Question 6\nTo earn a Success:\n\nSays the condition is violated\nJustifies with the decreased variability in larger values of log(GDP)\n\nIf they say the condition is not violated:\n\nEqual variance requires that the variability of the residuals (vertical distance) says the same for ALL values of the explanatory variable. Is that the case? Why or why not?\n\nIf they say the condition is violated but reference the spread around the line:\n\nEqual variance is not about having equal spread of points above and below the line – it is okay for there to be more residuals below the line compared to above the line. They key is that the spread of residuals (e.g., -20 to +20) stays the same for ALL values of the explanatory variable (across the x-axis). Is that the case? Why or why not?\n\nIf they say the condition is violated but have insufficient justification as to why:\n\nWhen evaluting conditions the choices are subjective, so it is necessary for you to justify WHY you made the decision you did. Your justification should make direct reference to characteristics of the distribution of residuals."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-7",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-7",
    "title": "Lab 8 Grading Guide",
    "section": "Question 7",
    "text": "Question 7\nTo earn a Success:\n\n\\(H_0\\) there is no linear relationship between log GDP per capita and life expectancy\n\\(H_A\\) there is a linear relationship between log GDP per capita and life expectancy\n\nIf they say GDP instead of log GDP:\n\nCareful! How did you transform your variable(s) in #2? What variables are you looking at the linear relationship between?\n\nIf they say there is a positive relationship in their alternative:\n\nThe standard hypothesis test for the slope uses a two-sided alternative hypothesis, unless we knew 100% going in that the relationship between \\(x\\) and \\(y\\) was positive. Did you know that the relationship was positive BEFORE you made your visualizations?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-8",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-8",
    "title": "Lab 8 Grading Guide",
    "section": "Question 8",
    "text": "Question 8\nTo earn a Success: Code should look like the following:\n\nobs_slope &lt;-  gapminder %&gt;%\n  specify(response = lifeExp, explanatory = log(gdpPercap)) %&gt;%\n  calculate(stat = \"slope\")\n\nIf they swap lifeExp and gdpPercap:\n\nCareful! What variable is your response variable?\n\nIf they use GDP instead of log(GDP) for their explanatory:\n\nCareful! You need to be consistent with the transformation you decided in #2."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-9",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-9",
    "title": "Lab 8 Grading Guide",
    "section": "Question 9",
    "text": "Question 9\nTo earn a Success: Code should look like the following:\n\nnull_dist &lt;- gapminder %&gt;%\n  specify(response = lifeExp, explanatory = log(gdpPercap)) %&gt;%\n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"slope\")\n\nIf they also don’t use the log in this step:\n\nUpdate your code to use the same variable transformation that you use in #8."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-10",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-10",
    "title": "Lab 8 Grading Guide",
    "section": "Question 10",
    "text": "Question 10\nTo earn a Success: Code should look like the following:\n\nvisualise(null_dist)  +\n  shade_p_value(obs_stat = obs_slope, \n                direction = \"two-sided\")\n\nIf they don’t use \"two-sided\" for their alternative:\n\nHow many tails are there in the hypotheses stated in #7?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-11",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-11",
    "title": "Lab 8 Grading Guide",
    "section": "Question 11",
    "text": "Question 11\nTo earn a Success: Code should look like the following:\n\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\")\n\nIf they don’t use \"two-sided\" for their alternative:\n\nHow many tails are there in the hypotheses stated in #7?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-12",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-12",
    "title": "Lab 8 Grading Guide",
    "section": "Question 12",
    "text": "Question 12\nTo earn a Success: States they reject \\(H_0\\) because the p-value is less than 0.1\nIf they don’t say why they rejected:\n\nCareful! Hypothesis test decisions can differ based on the significance threshold that was used. What threshold did you use?\n\nIf they do not make a decision:\n\nYou were asked to make a decision regarding the hypotheses, which has two possible options. Which option do you choose and why?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-13",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-13",
    "title": "Lab 8 Grading Guide",
    "section": "Question 13",
    "text": "Question 13\nTo earn a Success: Code should look like the following:\n\nget_regression_table(gapminder_lm, conf.level = 0.95)"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-14",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-14",
    "title": "Lab 8 Grading Guide",
    "section": "Question 14",
    "text": "Question 14\nTo earn a Success: States that the p-value is essentially the same"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-15",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-15",
    "title": "Lab 8 Grading Guide",
    "section": "Question 15",
    "text": "Question 15\nTo earn a Success:\n\nstates that the permutation distribution is approximately Normal\nstates that a t-distribution is a reasonable approximation\n\nIf they don’t reference the permutation distribution:\n\nTheory-based methods are an approximation for simulation-based methods. Based on the permutation distribution you created, do you believe a t-distribution is a good approximation for THIS permutation distribution? Why or why not?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-16",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-16",
    "title": "Lab 8 Grading Guide",
    "section": "Question 16",
    "text": "Question 16\nTo earn a Success: Answer must agree with what they said in #4-6\n\n\nIf they said equal variance and / or independence was violated, they must say that neither p-value is reliable.\n\n\n\nIf they said normality was violated but equal variance was not violated, they must say that the simulation-based p-value is more reliable.\n\n\n\nIf they said neither equal variance or normality were violated, they must say that both p-values are reliable.\n\n\nIf they choose the wrong method:\n\nLook back at the conditions required for each of these methods. Which conditions did you say were violated? What does that imply for the method(s) which give you the most reliable p-value?"
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html",
    "href": "labs/grading-guides/lab7_key.html",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "",
    "text": "Today we will explore the pie_crab dataset contained in the lterdatasampler R package. The data is from a study by Johnson et al. at the Plum Island Ecosystem Long Term Ecological Research site, studying the relationship between the size (carapace width) of a Fiddler Crab and the geographical location of its habitat. These data can be used to investigate if Bergmann’s Rule applies to Fidder Crabs, or specifically that the size of a crab increases as the distance from the equator increases.\n\n\nThe students who investigated this relationship for their midterm project found that when both latitude and water temperature are included as explanatory variables in the multiple regression model, the coefficient associated with water temperature doesn’t make sense. Namely, the model suggests warmer water temperatures are associated with larger crab sizes. However, we know that the water is warmer near the equator, which is where the crab sizes should be smaller. Rather perplexing!\nThe moral of the story is that water temperature and latitude are high correlated with each other, so including them both as explanatory variables leads to multicollinearity – something we do not want in our multiple linear regression.\n\n\n\nThe focus of this lab is on quantifying the relationship between water temperature (response) and latitude (explanatory)."
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html#data",
    "href": "labs/grading-guides/lab7_key.html#data",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "",
    "text": "Today we will explore the pie_crab dataset contained in the lterdatasampler R package. The data is from a study by Johnson et al. at the Plum Island Ecosystem Long Term Ecological Research site, studying the relationship between the size (carapace width) of a Fiddler Crab and the geographical location of its habitat. These data can be used to investigate if Bergmann’s Rule applies to Fidder Crabs, or specifically that the size of a crab increases as the distance from the equator increases.\n\n\nThe students who investigated this relationship for their midterm project found that when both latitude and water temperature are included as explanatory variables in the multiple regression model, the coefficient associated with water temperature doesn’t make sense. Namely, the model suggests warmer water temperatures are associated with larger crab sizes. However, we know that the water is warmer near the equator, which is where the crab sizes should be smaller. Rather perplexing!\nThe moral of the story is that water temperature and latitude are high correlated with each other, so including them both as explanatory variables leads to multicollinearity – something we do not want in our multiple linear regression.\n\n\n\nThe focus of this lab is on quantifying the relationship between water temperature (response) and latitude (explanatory)."
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html#cleaning-the-data",
    "href": "labs/grading-guides/lab7_key.html#cleaning-the-data",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nThe data contains information on at total of 392 Fiddler Crabs caught at 13 marshes on the Atlantic coast of the United States in summer 2016. However, at each marsh, there is only one recorded water temperature. Meaning, we need to collapse our dataset to have only one observation per latitude (marsh).\n1. Fill in the code below to create a new dataset called pie_crab_clean which has 13 observations – one per latitude / marsh.\n\npie_crab_clean &lt;- pie_crab %&gt;% \n  group_by(site) %&gt;% \n  slice_sample(n = 1) %&gt;% \n  ungroup()\n\nFrom this point forward, you should use the pie_crab_clean dataset for EVERY problem."
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html#visualizing-relationships",
    "href": "labs/grading-guides/lab7_key.html#visualizing-relationships",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Visualizing Relationships",
    "text": "Visualizing Relationships\n2. Create a scatterplot modeling the relationship between latitude (explanatory) and water temperature (response)for these 13 marshes.\nDon’t forget to add descriptive axis labels!\n\nggplot(data = pie_crab_clean, \n       mapping = aes(x = latitude, y = water_temp)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Latitude (degrees)\", \n       y = \"Water Temperature (Celcius)\")\n\n\n\n\n\n\n\n\n3. Describe the relationship you see in the scatterplot. Be sure to address the four aspects we discussed in class: form, direction, strength, and unusual points! Keep in mind that you are no longer analyzing data on crabs! The dataset you have is on marshes along the Atlantic coast!\n\nSummarizing the Relationships\nNow that you’ve visualized the relationship, let’s summarize this relationship with some statistics. As we discussed in Week 4, there are two statistics you can use to summarize a linear relationship:\n\nthe correlation\nthe slope\n\nLet’s calculate both!\n4. Use the get_correlation() function to calculate the correlation between the water temperature and latitude for these 13 marshes.\n\nget_correlation(data = pie_crab_clean, \n                formula = latitude ~ water_temp)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.956\n\n\n5. Fill in the code below to calculate the observed slope for the relationship between the water temperature (response) and latitude (explanatory) for these 13 marshes.\nNote: Nothing will be output when you run this code!\n\nobs_slope &lt;- pie_crab_clean %&gt;% \n  specify(response = water_temp, \n          explanatory = latitude) %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html#bootstrap-distribution",
    "href": "labs/grading-guides/lab7_key.html#bootstrap-distribution",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\nNow that we have the observed slope statistic, let’s see what variability we might get in the slope statistic for other samples (marshes) we might have gotten from the population (the eastern coast of the US).\nAs a refresher, when we use resampling to obtain our bootstrap distribution, our steps look like the following:\nStep 1: specify() the response and explanatory variables\nStep 2: generate() lots of bootstrap resamples\nStep 3: calculate() for each of the generated() samples, calculate the statistic you are interested in\nLet’s give this a try!\n6. Fill in the code to generate 500 bootstrap slope statistics (from 500 bootstrap resamples).\n\nbootstrap &lt;- pie_crab_clean %&gt;% \n  specify(response = water_temp, \n          explanatory = latitude) %&gt;% \n  generate(reps = 500, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n\nAlright, now that we have the bootstrap slope statistics, let’s see what it looks like! Let’s use the visualize() function (not ggplot()!) to make a quick visualization of the statistics you calculated above.\n7. Use the visualize() function to create a simple histogram of your 500 bootstrap statistics.\nIt would be nice to change the x-axis label to describe what statistic is being plotted!\n\n# Code to visualize bootstrap statistics\nvisualise(bootstrap) + \n  labs(x = \"Slope Statistic for Relationship Between Water Temperature and Latitude\")"
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html#confidence-interval",
    "href": "labs/grading-guides/lab7_key.html#confidence-interval",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nThe next step to obtain our confidence interval! First we need to determine what percentage of statistics we want to keep in the confidence interval. 90%? 95%? 99%? 80%?\nThis seems like a study where we care a bit less about our interval capturing the true value, at least compared to something like a medical study. So, I think this could be a great instance to use an 85% confidence interval.\n8. Use the get_confidence_interval() function to find the 85% confidence interval from your bootstrap distribution, using the percentile method!\n\n# Code to obtain a 85% PERCENTILE based confidence interval\n\nget_confidence_interval(bootstrap, \n                        level = 0.85, \n                        type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.870   -0.672\n\n\n9. Interpret the confidence interval you obtained in #8. Make sure to include the context of the data!\nWe are 95% confident that the slope of the relationship between water temperature and latitude for all marshes along the eastern coast of the US is between -0.864 and -0.669.\nJust for fun, let’s compare the confidence we obtained using a percentile method with an interval found using the SE method.\n10. Use the get_confidence_interval() function to find the 85% confidence interval from your bootstrap distribution, using the SE method! Remember – with the SE method, you need to specify the point estimate!\n\n# Code to obtain a 85% SE based confidence interval\n\nget_confidence_interval(bootstrap, \n                        level = 0.85, \n                        type = \"se\",\n                        point_estimate = obs_slope)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.874   -0.672\n\n\n11. How do your confidence intervals compare? Based on the shape of the bootstrap distribution, would you expect for these methods to yield similar results?\nHint: Think about the conditions for using the SE method to obtain a confidence interval!\nThey are nearly identical! This is because the bootstrap distribution is approximately normal, so an SE method yields similar results to the percentile method."
  },
  {
    "objectID": "labs/grading-guides/lab7_key.html#bootstrap-assumptions",
    "href": "labs/grading-guides/lab7_key.html#bootstrap-assumptions",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Bootstrap Assumptions",
    "text": "Bootstrap Assumptions\nA bootstrap distribution aims to simulate the variability we’d get from other samples from our population. However, the accuracy of these samples relies on the quality of our original sample.\n12. Based on the information given, how do you feel about the assumption a bootstrap distribution makes about the original sample? What issues do you believe might prevent this assumption being appropriate?)\nThe assumption is that the sample is random / representative of the population. I don’t honestly know enough about the representative-ness of the 13 marshes selected for this study, but I imagine there is a great deal of variability."
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html",
    "href": "labs/grading-guides/lab-3-grading-guide.html",
    "title": "Lab 3: Grading Guide",
    "section": "",
    "text": "Q1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\nQ9\nQ10\nQ11\nQ12\nQ13"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-6-distribution-of-trout-lengths",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-6-distribution-of-trout-lengths",
    "title": "Lab 3: Grading Guide",
    "section": "Question 6 – Distribution of trout lengths",
    "text": "Question 6 – Distribution of trout lengths\nSuccess: Acceptable geoms:\n\ngeom_histogram()\ngeom_density()\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Axis labels are not required! But, if they don’t change it, give the following feedback:\nFor Question 6, it is always nice to have professional looking axis labels!\n\n\nGrowing:\n\nUse geom_dotplot()\n\nFeedback: Careful! A dotplot doesn’t automatically resize the dots based on the number of observations you have. This makes it so your dots are running off the page! You can resize the dots using the dotsize argument, with a number smaller than 1 (e.g., 0.5).\n\nUses geom_boxplot()\n\nFeedback: For Question 6, we need to use a geom which allows for us to see the shape of the distribution. Boxplots hide distributions with multiple modes, so what type of plot would be better?"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-7-sources-of-variation",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-7-sources-of-variation",
    "title": "Lab 3: Grading Guide",
    "section": "Question 7 – Sources of variation",
    "text": "Question 7 – Sources of variation\nSuccess: Names three “reasonable” sources of variation in fish length\nGrowing: Names an unreasonable source of variation in fish lengths\nFeedback: For Question 7, we are interested in variables that, if changed, we would expect changes in the length of the Cutthroat trout."
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-8-ridge-plot",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-8-ridge-plot",
    "title": "Lab 3: Grading Guide",
    "section": "Question 8 – Ridge plot",
    "text": "Question 8 – Ridge plot\nSuccess: Code should look like the following\n\nggplot(data = fish, \n       mapping = aes(x = length_1_mm, y = unittype)) +\n  geom_density_ridges() +\n  labs(x = \"Length (mm)\", \n       y = \"Channel Section\")\n\nGrowing:\n\nDoesn’t change axis labels\n\nFeedback: For Question 8, you were asked to give your visualization nice axis labels. Note that means BOTH axes!\n\nDoesn’t include units (mm) in x-axis label\n\nFeedback: For Question 8, it is important for the axis label to contain the units of the variable. What units were the lengths measured in?\n\nY-axis should say something about the type of channel\n\nFeedback: For Question 8, it is important for the axis label to describe the variable that is being measured."
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-9-adding-another-categorical-variable",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-9-adding-another-categorical-variable",
    "title": "Lab 3: Grading Guide",
    "section": "Question 9 – Adding another categorical variable",
    "text": "Question 9 – Adding another categorical variable\nSuccess: Uses either color or facets to incorporate species\n\n## Option 1\nggplot(data = fish, \n       mapping = aes(x = length_1_mm, y = unittype)) +\n  geom_density_ridges() +\n  facet_wrap(~species)\n\n## Option 2\nggplot(data = fish, \n       mapping = aes(x = length_1_mm, y = unittype, color = species)) +\n  geom_density_ridges() \n\nGrowing: If doesn’t use facets or colors\n\n\n\n\n\n\nNote\n\n\n\nIf they got a growing on #8 for an axis label, they don’t get a growing here.\n\n\nQuestion 10 – Based on your plot, how different are the lengths of the Cutthroat trout between the different channel types and forest sections?"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-10-based-on-the-plot-how-different-are-the-lengths-between-the-channel-types-and-forest-sections",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-10-based-on-the-plot-how-different-are-the-lengths-between-the-channel-types-and-forest-sections",
    "title": "Lab 3: Grading Guide",
    "section": "Question 10 – Based on the plot, how different are the lengths between the channel types and forest sections?",
    "text": "Question 10 – Based on the plot, how different are the lengths between the channel types and forest sections?\nSuccess: Must have all of the following\n\nComparisons between forest sections (centers, spreads, shapes)\nComparisons between channel types\nState what channel types did not have both types of forest\n\nGrowing:\n\nComparisons are incomplete\n\nFeedback: Your comparison of these distributions should include similarities / differences in their centers and shapes.\n\nDoes not state what channel types did not have both types of forest\n\nFeedback: Careful! For Question 10 you should note if not every forest type was observed for every type of channel."
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-11-average-length-of-cutthroat-trout-between-channel-types",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-11-average-length-of-cutthroat-trout-between-channel-types",
    "title": "Lab 3: Grading Guide",
    "section": "Question 11 – Average length of Cutthroat trout between channel types",
    "text": "Question 11 – Average length of Cutthroat trout between channel types\nSuccess: Code should look similar to:\n\ntrout %&gt;%\n  group_by(unittype) %&gt;%\n  summarize(mean_length = mean(length_1_mm))\n\nIf they don’t name their summary statistics\nFeedback: For Question 11, the output of your summary statistics looks a lot nicer if you give them names! (like the example that was given)\nGrowing: If process is not correct"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-12-find-the-average-length-of-cutthroat-trout-between-channel",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-12-find-the-average-length-of-cutthroat-trout-between-channel",
    "title": "Lab 3: Grading Guide",
    "section": "Question 12 – Find the average length of Cutthroat trout between channel",
    "text": "Question 12 – Find the average length of Cutthroat trout between channel\ntypes and forest section\nSuccess: Code should look similar to:\n\ntrout %&gt;%\n  group_by(unittype, species) %&gt;%\n  summarize(mean_length = mean(length_1_mm))\n\nGrowing: If process is not correct"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-13-differences-in-averages-compared-to-plot",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-13-differences-in-averages-compared-to-plot",
    "title": "Lab 3: Grading Guide",
    "section": "Question 13 – Differences in averages compared to plot",
    "text": "Question 13 – Differences in averages compared to plot\nSuccess:\n\nStates that the averages are all fairly similar (comparing the centers)\nConnects with the skew seen in the visualizations\n\nGrowing: If all they say is that the differences in means are similar to the visualization\nFeedback: Note that in Question 13 you are specifically using the mean to summarize the center of the distribution. In Question 9 you saw the shape of each channel / forest section’s distribution. Based on what you saw, how do the shapes of the distributions influence the means you are seeing in Question 13?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html",
    "href": "labs/grading-guides/lab-2-grading-guide.html",
    "title": "Lab 2 - Grading Guide",
    "section": "",
    "text": "Q1 (a)\nQ1 (b)\nQ2\nQ3 (a)\nQ3 (b)\nQ4\nQ5\nQ6\nQ7\nQ8\nQ9\nQ10"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#a",
    "href": "labs/grading-guides/lab-2-grading-guide.html#a",
    "title": "Lab 2 - Grading Guide",
    "section": "1 (a)",
    "text": "1 (a)\nHow large is the nycflights dataset? (i.e. How many rows and columns does it have?) \nSuccess:\n\nUses glimpse() to obtain the size of the dataset\nSize of 32,735 rows and 16 columns\n\nGrowing:\n\nIf no code is present\nIf they flip the rows and columns\nIf they do not provide size of data\n\nFeedback for no code: For Question 1(a), you needed to write code to explore the size of the data and the types of variables.\nFeedback for not using glimpse(): For this course, we prefer you use the tools being taught in the textbook and course materials. What function have you learned in this class which give a preview of a dataset?\nFeedback for no / incorrect dimensions: For Question 1(a), look again at the output of the glimpse() function, how many rows and columns are included in the nycflights dataset?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#b",
    "href": "labs/grading-guides/lab-2-grading-guide.html#b",
    "title": "Lab 2 - Grading Guide",
    "section": "1 (b)",
    "text": "1 (b)\nAre there numerical variables in the dataset? If so, what are their names? \nSuccess:\n\nLists all variables with an int or dbl data type:\n\nyear\nmonth (many may miss)\nday\ndep_time\ndep_delay\narr_time\narr_delay\nflight (many may miss)\nair_time\ndistance\nhour\nminute\n\nPermitted to miss 1 variable\n\nGrowing:\n\nIf misses 2 or more variables\nIf includes categorical variables (labeled chr)\n\nFeedback: For Question 1(b), you need to be careful to include ALL of the variables that R believes are numerical, that includes variables which may behave more like a categorical variable."
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section",
    "title": "Lab 2 - Grading Guide",
    "section": "2",
    "text": "2\nCreate a histogram of the dep_delay variable from the nycflights data\nSuccess: Code should look like\n\nggplot(data = nycflights, mapping = aes(x = dep_delay)) + \n  geom_histogram()\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Axis labels are not required!\nNote: May choose their own binwidth, but that is not required!\n\n\nGrowing: If maps dep_delay to y-axis instead of x-axis\nFeedback: For Question 2, our histograms are always made with our quantitative variable on the x-axis!"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#a-1",
    "href": "labs/grading-guides/lab-2-grading-guide.html#a-1",
    "title": "Lab 2 - Grading Guide",
    "section": "3 (a)",
    "text": "3 (a)\nMake two other histograms, one with a binwidth of 15 and one with a binwidth of 150.\nSuccess: Code should look like:\n\nggplot(data = nycflights, \n       mapping = aes(x = dep_delay)) + \n  geom_histogram(binwidth = 15)\n\nggplot(data = nycflights, \n       mapping = aes(x = dep_delay)) + \n  geom_histogram(binwidth = 150)\n\nGrowing: If code does not use correct binwidths\nFeedback: For Question 3(a), pay attention to what binwidth you need to use within your geom_histogram()!"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#b-1",
    "href": "labs/grading-guides/lab-2-grading-guide.html#b-1",
    "title": "Lab 2 - Grading Guide",
    "section": "3 (b)",
    "text": "3 (b)\nHow do these three histograms compare? Are features revealed in one that are obscured in another? \nSuccess:\n\nDiscusses how wider bins make it hard to see where there are small differences and / or how smaller bins make it easier to see differences\n\nGrowing:\n\nIf description only references how the bins are larger or smaller, but doesn’t talk about shape the distribution\nOnly talks about how the y-axis scale changes\n\nFeedback: For Question 3(b), your discussion should address how the binwidth affects the shape of the distribution and what you are able to see."
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-1",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-1",
    "title": "Lab 2 - Grading Guide",
    "section": "4",
    "text": "4\nFill in the code to create a new dataframe named sfo_flights that is the result of filter()ing only the observations whose destination was San Francisco.\nSuccess: Code should look like:\n\nsfo_flights &lt;- filter(nycflights, \n                      dest == \"SFO\")\n\nGrowing: If they use the wrong destination in their filter()\nFeedback: For Question 4, you need to look at the description for how the airport names were coded!"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-2",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-2",
    "title": "Lab 2 - Grading Guide",
    "section": "5",
    "text": "5\nFill in the code below to find the number of flights flying into SFO in July that arrived early. What does the result tell you?\nSuccess: Code should look like:\n\nfilter(sfo_flights, \n       month == 7, \n       arr_delay &lt; 0) %&gt;% \n  dim()\n\nAND\nThey should state that 45 flights arrived to SFO early in July.\nGrowing:\n\nIf they don’t interpret the output\n\nFeedback: For Question 5, you were asked to interpret the output of the code.\n\nIf they don’t interpret 45 as the number of flights\n\nFeedback: For Question 5, what does the 45 refer to?\n\nIf they interpret 45 and 16 as flight numbers\n\nFeedback: For Question 5, the 45 and the 16 refer to the size of the filtered dataset. What does that tell you in terms of the number of flights that satisfied your criteria out of the sfo_flights dataset?\n\nIf they only say that the 45 goes with flights\n\n\n*FeedbacK:\n\nIf they don’t interpret the conditions of the filter (arrived early in July)\n\nFeedback: For Question 5, what were the conditions of these 45 flights?\n\nIf they don’t state the destination of these flights (SFO)\n\nFeedback: For Question 5, what flights did you filter? All NYC flights or a smaller dataset?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-3",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-3",
    "title": "Lab 2 - Grading Guide",
    "section": "6",
    "text": "6\nCalculate the following statistics for the arrival delays in the sfo_flights dataset:\n\nmean\nmedian\nmax\nmin\n\nSuccess: Code should look like:\n\nsummarise(sfo_flights, \n          mean_ad = mean(arr_delay), \n          median_ad = median(arr_delay), \n          max_ad = max(arr_delay),\n          min_ad = min(arr_delay)\n          )\n\n\nIf they don’t name their summary statistics\n\nFeedback: For Question 6, the output of your summary statistics looks a lot nicer if you give them names! (like the example that was given)\n\nI’d write a comment if they used _dd in their names, since it should be _ad for the arrival delays.\n\nFeedback: For Question 6, it would be more clear to use _ad at the end of your names, since you are summarizing the arrival delays (not the departure delays).\nGrowing:\n\nIf they miss some of the statistics\n\nFeedback: For Question 6, you were asked to provide four (4) statistics!\n\nIf they use the wrong dataset (nycflights instead of sfo_flights)\n\nFeedback: For Question 6, what dataset should you be using to calculate your summary statistics?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-4",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-4",
    "title": "Lab 2 - Grading Guide",
    "section": "7",
    "text": "7\nUsing the above summary statistics, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nSuccess\n\nMakes a statement about what I should expect (flight to be early / late)\nJustifies statement based on summary statistics\n\nGrowing\n\nDoes not justify their statement\n\nFeedback: In Statistics it is critical to back your claim with data! How did you decide what I should expect if I am flying from NYC to SFO?\n\nOnly discuss the statistics but don’t address the question\n\nFeedback: You point out the statistics and what they mean in terms of an arrival delay, but how do these statistics connect with my question about what I should expect?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-5",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-5",
    "title": "Lab 2 - Grading Guide",
    "section": "8",
    "text": "8\nNow, rather than calculating summary statistics, plot the distribution of arrival delays for the sfo_flights dataset. Choose the type of plot you believe is appropriate for visualizing the distribution of arrival delays. Be sure to give your visualization nice axis labels!\nYou choose the type of plot you believe is appropriate for visualizing the distribution of departure delays.\nSuccess: Code should look like:\n\nggplot(data = nycflights, \n       mapping = aes(x = arr_delay)) + \n  geom_histogram() + \n  labs(x = \"Arrival Delays (min)\")\n\n\nAcceptable geoms:\n\ngeom_histogram()\ngeom_dotplot() (though not great)\ngeom_density()\n\nIt’s okay for them not to change the binwidth\n\nGrowing:\n\nUses geom_boxplot()\n\nFeedback: For Question 8, we need to use a geom which allows for us to see the shape of the distribution. Boxplots hide distributions with multiple modes, so what type of plot would be better?\n\nDoesn’t change x-axis label\n\nFeedback: For Question 8, you were asked to give your visualization nice axis labels. It is also important for the axis label to contain the units of the variable! What units were the arrival delays measured in?\n\nDoesn’t include units (minutes) in axis label\n\nFeedback: For Question 8, it is important for the axis label to contain the units of the variable. What units were the arrival delays measured in?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-6",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-6",
    "title": "Lab 2 - Grading Guide",
    "section": "9",
    "text": "9\nUsing the plot above, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nSuccess\n\nMakes a statement about what I should expect (flight to be early / late)\nJustifies statement based on the plot\n\nGrowing\n\nDoes not justify their statement\n\nFeedback: In Statistics it is critical to back your claim with data! How did you decide what I should expect if I am flying from NYC to SFO?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-7",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-7",
    "title": "Lab 2 - Grading Guide",
    "section": "10",
    "text": "10\nHow did your answer change when using the plot versus using the summary statistics? i.e. What were you able to see in the plot that could could not “see” with the summary statistics?\nSuccess:\n\nStates if / how their answer did / did not change\nDiscusses what could be see in the visualizations that could not be seen in the statistics\n\nskew\nmode / peak\n\n\nGrowing:\n\nResponse does not discuss what could be seen in the visualizations that could not be seen in the statistics\n\nFeedback: For Question 10, you should address what you were able to SEE in the visualization that you could not “see” in the summary statistics.\n\nResponse doesn’t discuss if / how the answer to the question changed\n\nFeedback: Great job discussing how the shape of the distribution informs you choice of statistic! You did not, however, discuss if / how your answer to my question changed between looking at the plot versus looking at the statistics."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/tutorial/week-10.html",
    "href": "weeks/tutorial/week-10.html",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "A two-way ANOVA extends the one-way ANOVA to situations with two categorical explanatory variables. This new methods allows researchers to simultaneously study two variables that might explain variability in the responses and explore whether the impacts of one explanatory variable change depending on the level of the other explanatory variable.\nIn a clinical trials context, it is well known that certain factors can change the performance of certain drugs. For example, different dosages of a drug might have different benefits or side-effects on men, versus women or children or even for different age groups in adults. When the impact of one factor on the response changes depending on the level of another factor, we say that the two explanatory variables interact.\nIt is also possible for both explanatory variables to be related to differences in the mean responses and not interact. For example, suppose there are differences in how younger and older subjects respond to a drug, and there are differences in how all individuals respond to different dosages of a drug, but the effect of increasing the dosage is the same for both young and old subjects. This is an example of what is called an additive type of model.\nIn general, the world is more complicated than the single factor models we’ve considered, especially in observational studies, so these models allow us to start to handle more realistic situations."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#two-way-anova",
    "href": "weeks/tutorial/week-10.html#two-way-anova",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "A two-way ANOVA extends the one-way ANOVA to situations with two categorical explanatory variables. This new methods allows researchers to simultaneously study two variables that might explain variability in the responses and explore whether the impacts of one explanatory variable change depending on the level of the other explanatory variable.\nIn a clinical trials context, it is well known that certain factors can change the performance of certain drugs. For example, different dosages of a drug might have different benefits or side-effects on men, versus women or children or even for different age groups in adults. When the impact of one factor on the response changes depending on the level of another factor, we say that the two explanatory variables interact.\nIt is also possible for both explanatory variables to be related to differences in the mean responses and not interact. For example, suppose there are differences in how younger and older subjects respond to a drug, and there are differences in how all individuals respond to different dosages of a drug, but the effect of increasing the dosage is the same for both young and old subjects. This is an example of what is called an additive type of model.\nIn general, the world is more complicated than the single factor models we’ve considered, especially in observational studies, so these models allow us to start to handle more realistic situations."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#visualizing-a-two-way-anova",
    "href": "weeks/tutorial/week-10.html#visualizing-a-two-way-anova",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Visualizing a Two-Way ANOVA",
    "text": "Visualizing a Two-Way ANOVA\nThe visualizations we created for a one-way ANOVA are still relevant here, but we need to figure out how to add a second categorical explanatory variable to our plots.\nSimilar to the multivariate plots we’ve talked about previously, there are two main ways to add a second categorical variable to our plots:\n\ncolors\nfacets\n\nWe’ll explore both below!"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section",
    "href": "weeks/tutorial/week-10.html#section",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Below is a plot of the relationship between body mass and species for penguins in the Palmer Archipelago.\nThe year the data were collected was modified to a categorical variable, named year_cat. Change to code below to fill the density ridges with the year_cat variable.\n\n\npenguins %&gt;% \n  ggplot(aes(y = species, x = body_mass_g, fill = ___)) + \n  geom_density_ridges()\n\n\n\n\n\nggplot(aes(y = species, x = body_mass_g, fill = year_cat))"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-1",
    "href": "weeks/tutorial/week-10.html#section-1",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Now, take the same plot and instead of coloring by year_cat use facets to separate the different sampling years.\n\n\npenguins %&gt;% \n  ggplot(aes(y = species, x = body_mass_g)) + \n  geom_density_ridges() + \n  ___\n\n\n\n\nHint: Add ~ year_cat to facet_wrap() to create year facets.\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = species, x = body_mass_g)) + \n  geom_density_ridges() + \n  facet_wrap(~ year_cat)"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#additive-versus-interactive-models",
    "href": "weeks/tutorial/week-10.html#additive-versus-interactive-models",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Additive versus Interactive Models",
    "text": "Additive versus Interactive Models\nAs was mentioned in the Introduction, there are two different types of two-way ANOVA models. Similar to a multiple linear regression, the two explanatory variables could have their own impact on the response (similar to a parallel slopes regression model). Or, the relationship between one explanatory variable and the response could differ based on another explanatory variable (similar to a different slopes regression model).\nThese two types of models are called an additive two-way ANOVA model or an interaction two-way ANOVA model. Similar to how we decided which model to choose in a multiple linear regression, we will use visualizations to guide us."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-2",
    "href": "weeks/tutorial/week-10.html#section-2",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "When deciding if an interaction model is a good fit for the data, we look to see if the relationship between one categorical variable and the response differs based on the level of the other response variable.\nHere, we look at the relationship between year and body_mass_g and see if it differs based on the species of the penguin. To me, the easiest way to assess if this is the case is to fill the density plots with color for the different years:\n\n\n\n\n\n\n\n\n\nNow that I have the plot, I compare the “profile” of the density ridges (the combo of the pink, green, and blue) between the species. If the relationship between year and body_mass_g changed based on the species, then we would see very different profiles. Note, I’m not paying attention to where the profiles are located along the x-axis, I’m simply looking at the density ridges as individual pictures to be compared.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at the profiles above, removed from the context of the plot, does it seem that these profiles very similar? Or does it seem that these are different pictures? I didn’t think so, since it seems like in all three pictures there is about the same overlap between the three colors. However, we can use a two-way ANOVA interaction model to see if my intuition is right."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#conditions-of-a-two-way-anova-model",
    "href": "weeks/tutorial/week-10.html#conditions-of-a-two-way-anova-model",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Conditions of a Two-Way ANOVA Model",
    "text": "Conditions of a Two-Way ANOVA Model\nThe two-way ANOVA model has the same conditions as its one-way counterpart, however, we now have one more variable to construct our groups from.\nIndependence\nFor both categorical variables:\n\nobservations across groups need to be independent\nobservations within each group need to be independent\n\nEqual Variance: the variability of each group is similar to the others.\n\n\n\n\n\n\nIntersectional groups\n\n\n\nThis is an assumption about the groups at the intersection of each categorical variable (e.g. Gentoo penguins captured in 2007). This assumption is fairly robust, but large differences in variability will cause issues.\n\n\nNormal Distribution: the responses of each group need to be approximately Normal.\n\n\n\n\n\n\nIntersectional distributions\n\n\n\nThis is an assumption about the groups at the intersection of each categorical variable (e.g. Gentoo penguins captured in 2007). This assumption is also fairly robust, but influential outliers and the sample sizes of the groups should be noted when assessing."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-3",
    "href": "weeks/tutorial/week-10.html#section-3",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Your turn!"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#fitting-a-two-way-anova-model",
    "href": "weeks/tutorial/week-10.html#fitting-a-two-way-anova-model",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Fitting a Two-Way ANOVA Model",
    "text": "Fitting a Two-Way ANOVA Model\nThe R code to fit a two-way ANOVA model is very similar to a one-way ANOVA model. We use the aov() (analysis of variance) function, but now we will have two categorical explanatory variables.\nFirst, we’ll fit an interaction two-way ANOVA model to see if the relationship between year and body mass differs based on species of penguins.\n\nSimilar to different slopes model, * is the symbol we use to fit an interaction model.\n\n\naov(body_mass_g ~ species * year_cat, \n    data = penguins) %&gt;% \n  tidy()\n\n# A tibble: 4 × 6\n  term                df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species              2 146864214. 73432107.  340.      3.16e-81\n2 year_cat             2     26661.    13330.    0.0618  9.40e- 1\n3 species:year_cat     4    575055.   143764.    0.666   6.16e- 1\n4 Residuals          333  71841768.   215741.   NA      NA       \n\n\n\nWe notice that the interaction line (species:year_cat) has a small F-statistic 0.666 and a large p-value 0.616. This would lead for us to conclude that there is not an interaction between these two variables, or that the relationship between the mean body mass and year does not differ based on penguin species. As suspected!"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-4",
    "href": "weeks/tutorial/week-10.html#section-4",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "An additive model seems like a better choice. Modify the code from the interaction model to fit an additive model instead.\n\n\naov(body_mass_g ~ species * year_cat, data = penguins) %&gt;% \n  tidy()\n\n\n\n\nHint: We used a + sign to fit an additive model with regression!\n\n\n\naov(body_mass_g ~ species + year_cat, data = penguins) %&gt;% \n  tidy()\n\n\n\nBased on the ANOVA table above, what would you conclude for the relationship between body mass and species and the relationship between body mass and sampling year?"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#model-conclusions",
    "href": "weeks/tutorial/week-10.html#model-conclusions",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Model Conclusions",
    "text": "Model Conclusions\nIn this context, the ANOVA table allows for use to test two hypotheses:\n\nwhether the mean body mass for every year are equal\nwhether the mean body mass for every species are equal\n\nAgain, similar to a multiple regression, the interpretation of these tests is conditional on the other variable in the model."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-5",
    "href": "weeks/tutorial/week-10.html#section-5",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Based on the ANOVA table below, with a p-value of 0.94 at an \\(\\alpha\\) of 0.05, we would conclude that, after accounting for the species of penguin, there is insufficient evidence that at least one year of capture has a different mean body mass.\n\n\n# A tibble: 3 × 6\n  term         df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 146864214. 73432107.  342.      8.40e-82\n2 year_cat      2     26661.    13330.    0.0620  9.40e- 1\n3 Residuals   337  72416822.   214887.   NA      NA       \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit a screenshot of the final page of this tutorial to both the Week 10 Reading Guide and R tutorial assignments!"
  },
  {
    "objectID": "weeks/reading-guide/week1-answers.html",
    "href": "weeks/reading-guide/week1-answers.html",
    "title": "Answers to Questions on Week 1 Reading Guide",
    "section": "",
    "text": "In a data frame, rows correspond to: observations\nIn a data frame, columns correspond to: variables\n\nTrue or False: A pair of variables can be both associated AND independent.\nFalse – They can only be one or the other!\n\nTrue or False: Given a pair of variables, one will always be the explanatory variable and one the response variable.\nTechnically, this is false. For a variable to be the “explanatory variable” it needs to explain the changes in the “response variable”. You could have two variables that are not related at all.\n\nTrue or False: If a study does have an explanatory and a response variable, that means changes in the explanatory variable must cause changes in the response variable.\nFalse! For an explanatory variable to cause changes in a response variable, we need to have an experiment where the explanatory variable is randomly assigned.\n\nTrue or False: Observational studies can show a naturally occurring association between variables.\nTrue! Observational studies can show associations between variables, but they cannot show that one variable causes changes in another. This is because observational studies do not have random assignment."
  },
  {
    "objectID": "weeks/reading-guide/week1-answers.html#chapter-1-notes",
    "href": "weeks/reading-guide/week1-answers.html#chapter-1-notes",
    "title": "Answers to Questions on Week 1 Reading Guide",
    "section": "",
    "text": "In a data frame, rows correspond to: observations\nIn a data frame, columns correspond to: variables\n\nTrue or False: A pair of variables can be both associated AND independent.\nFalse – They can only be one or the other!\n\nTrue or False: Given a pair of variables, one will always be the explanatory variable and one the response variable.\nTechnically, this is false. For a variable to be the “explanatory variable” it needs to explain the changes in the “response variable”. You could have two variables that are not related at all.\n\nTrue or False: If a study does have an explanatory and a response variable, that means changes in the explanatory variable must cause changes in the response variable.\nFalse! For an explanatory variable to cause changes in a response variable, we need to have an experiment where the explanatory variable is randomly assigned.\n\nTrue or False: Observational studies can show a naturally occurring association between variables.\nTrue! Observational studies can show associations between variables, but they cannot show that one variable causes changes in another. This is because observational studies do not have random assignment."
  },
  {
    "objectID": "weeks/reading-guide/week1-answers.html#chapter-2-notes",
    "href": "weeks/reading-guide/week1-answers.html#chapter-2-notes",
    "title": "Answers to Questions on Week 1 Reading Guide",
    "section": "Chapter 2 Notes",
    "text": "Chapter 2 Notes\nTrue or False: Convenience sampling tends to result in non-response bias.\nFalse! Convenience sampling makes it so our sample is not representative of the population, or we have a biased sample. Non-response bias requires the people sampled to choose not to respond.\n\nTrue or False: Volunteer sampling tends to result in response bias.\n**True! If our sample is made up of volunteers, it is more likely to have people with strong views. This makes it so our \nTrue or False: Random sampling helps to resolve selection bias, but has no impact on non-response or response bias.\n\nTrue or False: Observational studies can show an association between two variables, but cannot determine a causal relationship.\nTrue! We need random assignment of our explanatory variable to determine if a relationship is causal.\n\nTrue or False: In order for an experiment to be valid, a placebo must be used.\nTrue! An experiment needs to have a “baseline” treatment to compare with. \nTrue or False: If random sampling of the target population is used, and no other types of bias is suspected, results from the sample can be generalized to the entire target population.\nTrue! Mathematically, we know that random sampling should make our sample look similar to the population, so we are not systematically including / excluding certain individuals more / less.\n\nTrue or False: If random sampling of the target population is used, and no other types of bias are suspected, results from the sample can be inferred as a causal relationship between the explanatory and response variables.\nFalse! Random sampling allows us to infer that our sample is representative of the population, it does not allow us to say anything about the relationship between the explanatory and response variables."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html",
    "href": "weeks/chapters/week7-reading2.html",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "",
    "text": "This week’s reading is a compilation of Chapter 8 from ModernDive (Kim et al. 2020), Chapter 24 from Introduction to Modern Statistics (Çetinkaya-Rundel and Hardin 2023), with a smattering of my own ideas."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#baby-birth-weights",
    "href": "weeks/chapters/week7-reading2.html#baby-birth-weights",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "1 Baby Birth Weights",
    "text": "1 Baby Birth Weights\nMedical researchers may be interested in the relationship between a baby’s birth weight and the age of the mother, so as to provide medical interventions for specific age groups if it is found that they are associated with lower birth weights.\nEvery year, the US releases to the public a large data set containing information on births recorded in the country. The births14 dataset is a random sample of 1,000 cases from one such dataset released in 2014.\n\n1.1 Observed data\nFigure 1 visualizes the relationship between mage and weight for this sample of 1,000 birth records.\n\nggplot(data = births14, \n       mapping = aes(x = mage, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight of Baby (lbs)\")\n\n\n\n\n\n\n\nFigure 1: Weight of baby at birth (in lbs) as explained by mother’s age.\n\n\n\n\n\nTable 1 displays the estimated regression coefficients for modeling the relationship between mage and weight for this sample of 1,000 birth records.\n\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n\nget_regression_table(births_lm)\n\n\n\n\n\nTable 1: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n6.793\n0.208\n32.651\n0.000\n6.385\n7.201\n\n\nmage\n0.014\n0.007\n1.987\n0.047\n0.000\n0.028\n\n\n\n\n\n\n\n\n\n\nBased on these coefficients, the estimated regression equation is:\n\\[ \\widehat{\\text{birth weight}} = -6.793 + 0.014 \\times \\text{mother's age}\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will let \\(\\beta_1\\) represent the slope of the relationship between baby’s birth weight and mother’s age for every baby born in the US in 2014. We will estimate \\(\\beta_1\\) using the births14 dataset, labeling the estimate \\(b_1\\) (just as we did in Week 4).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA parameter is the value of the statistic of interest for the entire population.\nWe typically estimate the parameter using a “point estimate” from a sample of data. The point estimate is also referred to as the statistic.\n\n\n\n\n1.2 Variability of the statistic\nThis sample of 1,000 births is only one of possibly tens of thousands of possible samples that could have been taken from the large dataset released in 2014. So, then we might wonder how different our regression equation would be if we had a different sample. There is no reason to believe that \\(\\beta_1\\) is 0.279, but there is also no reason to believe that \\(\\beta_1\\) is particularly far away from \\(b_1 =\\) 0.279.\nJust this week you read about how estimates, such as \\(b_1\\), are prone to sampling variation – the variability from sample to sample. For example, if we took a different sample of 1,000 births, would we obtain a slope of exactly 0.279? No, that seems fairly unlikely. We might obtain a slope of 0.29 or 0.26, or even 0.35!\nWhen we studied the effects of sampling variation, we took many samples, something that was easily done with a shovel and a bowl of red and white balls. In this case, however, how would we obtain another sample? Well, we would need to go to the source of the data—the large public dataset released in 2014—and take another random sample of 1,000 observations. Maybe we don’t have access to that original dataset of 2014 births, how could we study the effects of sampling variation using our single sample? We will do so using a technique known as bootstrap resampling with replacement, which we now illustrate.\n\n\n1.3 Resampling once\n\n\n\n\n\n\nFigure 2: Step 1: Write out mother’s ages and baby’s birth weights on 1,000 slips of paper representing one of the 1,000 births included in the original sample.\n\n\n\nStep 1: Print out 1,000 identically sized slips of paper (or post-it notes) representing the sample of 1,000 babies in our sample. On each piece of paper, write the mother’s age and the birth weight of the baby. Figure 2 displays six of these such papers.\nStep 2: Put the 1,000 slips of paper into a hat as seen in Figure 3.\n\n\n\n\n\n\nFigure 3: Step 2: Putting 1,000 slips of paper (post-its) in a hat.\n\n\n\nStep 3: Mix the hat’s contents and draw one slip of paper at random, as seen in Figure 4. Record the mother’s age and baby’s birth weight, as printed on the paper.\n\n\n\n\n\n\nFigure 4: Step 3: Drawing one slip of paper at random.\n\n\n\nStep 4: Put the slip of paper back in the hat! In other words, replace it as seen in Figure 5.\n\n\n\n\n\n\nFigure 5: Step 4: Replacing slip of paper.\n\n\n\nStep 5: Repeat Steps 3 and 4 a total of 999 more times, resulting in 1,000 recorded mother’s ages and baby birth weights.\nWhat we just performed was a resampling of the original sample of 1,000 birth records. We are not sampling 1,000 birth records from the population of all 2014 US births as we did for our original sample. Instead, we are mimicking this process by resampling 1,000 birth records from our original sample.\nNow ask yourselves, why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would end up with the same 1,000 birth records! In other words, replacing the slips of paper induces sampling variation.\nBeing more precise with our terminology, we just performed a resampling with replacement from the original sample of 1,000 birth records. Had we left the slip of paper out of the hat each time we performed Step 4, this would be resampling without replacement.\nLet’s study our sample of 1,000 resampled birth records. First, I’ve made a table of the number of times each observation (birth record) was resampled. Table 2 displays the birth records that were resampled the most often. Based on the table, it appears that record IDs 71 and 534 were resampled six times.\n\n\n\n\nTable 2: Frequencies of how often a given birth record (ID) was resampled.\n\n\n\n\n\n\n71\n534\n170\n561\n19\n142\n305\n309\n495\n573\n726\n727\n835\n838\n840\n864\n919\n924\n929\n953\n\n\n\n\n6\n6\n5\n5\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\nWhen sampling with replacement, not every observation is guaranteed to be sampled. So, some observations from the original sample may never appear in the resample, whereas others may appear multiple times.\n\n\nFigure 6 compares the relationship between mother’s age and baby’s birth weight from our resample with the relationship in our original sample.\n\nggplot(data = births14, \n       mapping = aes(x = mage, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight (lbs)\")\n\nggplot(data = resample1, \n       mapping = aes(x = mage, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight (lbs)\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Sample of 1,000 Birth Records\n\n\n\n\n\n\n\n\n\n\n\n(b) Resample of 1,000 Birth Records\n\n\n\n\n\n\n\nFigure 6: Comparing relationship between mage and weight in the resampled birth records compared to the relationship seen in the original sample of birth records.\n\n\n\n\nObserved in Figure 6 that while the general shape of both relationships are similar, they are not identical.\nRecall, from the previous section that the sample slope (\\(b_1\\)) from the original sample was . What about for our resample? Based on the scatterplot, what would your guess be? Larger than before? Smaller than before?\nLet’s look at the coefficient estimates for the resampled dataset. Table 3 displays the estimated regression coefficients for modeling the relationship between mage and weight for the resample of 1,000 birth records.\n\nresample_lm &lt;- lm(weight ~ mage, data = resample1)\n\nget_regression_table(resample_lm)\n\n\n\n\n\nTable 3: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age, for the resample of 1,000 birth records.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n6.981\n0.220\n31.738\n0.000\n6.549\n7.413\n\n\nmage\n0.005\n0.008\n0.684\n0.494\n-0.010\n0.020\n\n\n\n\n\n\n\n\n\n\nFor the resampled dataset, the relationship between mother’s age and baby birth weight is much weaker than in the original sample, with an estimated slope of \\(b_1 =\\) 0.005.\nWhat if we repeated this resampling exercise many times? Would we obtain the same slope each time? In other words, would our guess at the slope for the relationship between mother’s age and baby’s birth weight for all births in the US in 2014 exactly 0.005 every time?"
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#computer-simulation-and-resampling",
    "href": "weeks/chapters/week7-reading2.html#computer-simulation-and-resampling",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "2 Computer simulation and resampling",
    "text": "2 Computer simulation and resampling\nIt should be very clear that tactile resampling with a dataset with 1,000 observations would be extremely time consuming, nothing we would want to ask our friends to do. A computer, however, would be happy to do this process for us!\n\n2.1 Virtually resampling once\nFirst, let’s perform the virtual analog of resampling once. Recall that the births14 dataset included in the openintro package contains the 1,000 birth records from the original study. Furthermore, recall in the last chapter that we used the rep_sample_n() function as a virtual shovel to sample balls from our virtual bowl of 2400 balls as follows:\nLet’s modify this code to perform the resampling with replacement from the 1,000 birth records in the original sample:\n\nvirtual_resample &lt;- rep_sample_n(births14, \n                                 size = 1000,\n                                 replace = TRUE, \n                                 reps = 1)\n\nObserve how we explicitly set the replace argument to TRUE in order to tell rep_sample_n() that we would like to sample birth records with replacement. Had we not kept replace = FALSE, we would have done resampling without replacement. Additionally, we changed the sample size, as we need to create a sample with the same size as the original sample which had 1,000 observations.\nLet’s look at only the first 10 out of 1,000 rows of virtual_resample:\n\nvirtual_resample\n\n# A tibble: 1,000 × 15\n# Groups:   replicate [1]\n   replicate    ID  mage weight  fage mature      weeks premie    visits gained\n       &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1         1   333    29   7.4     31 younger mom    39 full term     13     50\n 2         1   595    29   0.75    NA younger mom    21 premie         4     NA\n 3         1   338    31   6.94    NA younger mom    39 full term      9     20\n 4         1   531    33   6.94    37 younger mom    39 full term     14     20\n 5         1   289    35   7.98    35 mature mom     38 full term     10     40\n 6         1   461    32   7.15    33 younger mom    39 full term     13     NA\n 7         1   137    26   7.69    36 younger mom    38 full term     NA     25\n 8         1   828    36   4.54    65 mature mom     36 premie        11     10\n 9         1   118    26   8.31    24 younger mom    37 full term     12     25\n10         1    95    38   6.04    37 mature mom     36 premie        10     32\n# ℹ 990 more rows\n# ℹ 5 more variables: lowbirthweight &lt;chr&gt;, sex &lt;chr&gt;, habit &lt;chr&gt;,\n#   marital &lt;chr&gt;, whitemom &lt;chr&gt;\n\n\nThe replicate variable only takes on the value of 1 corresponding to us only having reps = 1, the ID variable indicates which of the 1,000 birth records was resampled, and mage and weight denote mother’s age and the baby’s birth weight, respectively.\nLet’s now compute the slope for the relationship between mother’s age and baby’s birth weight for this resample:\n\nvirtual_resample_lm &lt;- lm(weight ~ mage, data = virtual_resample)\n\nget_regression_table(virtual_resample_lm)\n\n\n\n\n\nTable 4: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age, for the virtual resample of 1,000 birth records.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n7.156\n0.224\n31.974\n0.000\n6.716\n7.595\n\n\nmage\n0.002\n0.008\n0.210\n0.834\n-0.014\n0.017\n\n\n\n\n\n\n\n\n\n\nAs we saw when we did our tactile resampling exercise, Table 4 shows that the estimated slope is different from the slope of our original sample of 0.005.\n\n\n2.2 infer package workflow\nUnfortunately, our process of virtual resampling relies on us fitting a linear regression for each replicate of our virtual_resample dataset. This gets a bit tricky coding wise, as we would need to fit 35 different linear regressions if we had 35 different resamples of our data.\nEnter, infer, an R package for statistical inference. infer makes efficient use of the %&gt;% pipe operator we learned in Week 3 to spell out the sequence of steps necessary to perform statistical inference in a “tidy” and transparent fashion. Furthermore, just as the dplyr package provides functions with verb-like names to perform data wrangling, the infer package provides functions with intuitive verb-like names to perform statistical inference.\nLet’s go back to our original slope. Previously, we computed the value of the sample slope \\(b_1\\) using the lm() function:\n\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n\nget_regression_table(births_lm)\n\nWe’ll see that we can also do this using infer functions specify() and calculate():\n\nbirths14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  calculate(stat = \"slope\")\n\nYou might be asking yourself: “Isn’t the infer code longer? Why would I use that code?”. While not immediately apparent, you’ll see that there are three chief benefits to the infer workflow as opposed to the lm() function workflow we had previously.\nFirst, the infer verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests. We’ll see flowchart diagrams of this framework in the upcoming Figure 12.\nSecond, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent next week, when we also use infer to conduct hypothesis tests for the slope statistic.\nThird, the infer workflow is much simpler for conducting inference when you have more than one variable, meaning we can extend what we’ve learned for simple linear regression to multiple linear regression models.\nLet’s now illustrate the sequence of verbs necessary to construct a confidence interval for \\(\\b_1\\), the slope of the relationship between mother’s age and baby’s birth weight for all US births in 2014.\n\n1. specify() variables\n\n\n\n\n\n\nFigure 7: “Diagram of specify()ing variables.”\n\n\n\nAs shown in Figure 7, the specify() function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by specifying the response argument. For example, in our births14 data frame, the response variable of interest is weight and the explanatory variable is mage:\n\nbirths14 %&gt;% \n  specify(response = weight, \n          explanatory = mage)\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 1,000 × 2\n   weight  mage\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   6.96    34\n 2   8.86    31\n 3   7.51    36\n 4   6.19    16\n 5   6.75    31\n 6   6.69    26\n 7   6.13    36\n 8   6.74    24\n 9   8.94    32\n10   9.12    26\n# ℹ 990 more rows\n\n\nNotice how the dataset got smaller, now there are only two columns where before there were 14. You should also notice the messages above the dataset (Response: weight (numeric) and Explanatory: mage). These are meta-data about the grouping structure of the dataset, declaring which variable has been assigned to the explanatory / response.\n\n\n\n\n\n\nNote\n\n\n\nThis is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Week 3.\n\n\n\n\n2. generate() replicates\n\n\n\n\n\n\nFigure 8: “Diagram of generate() replicates.”\n\n\n\nAfter we specify() the variables of interest, we pipe the results into the generate() function to generate replicates. Figure 8 shows how this is combined with specify() to start the pipeline. In other words, repeat the resampling process a large number of times, similar to how we collected 35 different samples of balls from the bowl.\nThe generate() function’s first argument is reps, which sets the number of replicates we would like to generate. Suppose we were interested in obtaining 50 different resamples (each of 1,000 birth records). Then, we would we set reps = 50, telling infer that we are interested in obtaining 50 different resamples, each with 1,000 observations.\nThe second argument type determines the type of computer simulation we’d like to perform. We set this to type = \"bootstrap\" indicating that we want to perform bootstrap resampling, meaning the resampling should be done with replacement. You’ll see different options for type when we learn about hypothesis testing next week.\n\nbirths14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 50, \n           type = \"bootstrap\")\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 50,000 × 3\n# Groups:   replicate [50]\n   replicate weight  mage\n       &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1         1  10.6     31\n 2         1   4.31    31\n 3         1   4.94    33\n 4         1   5.65    36\n 5         1   8       23\n 6         1   7.63    37\n 7         1   8       19\n 8         1   3.22    30\n 9         1   8.56    32\n10         1   4.32    21\n# ℹ 49,990 more rows\n\n\nObserve that the resulting data frame has 50,000 rows. This is because we performed resampling of 1000 birth records with replacement 50 times and 1000 \\(\\times\\) 50 = 50,000.\nThe variable replicate indicates which resample each row belongs to. So it has the value 1 1000 times, the value 2 1000 times, all the way through to the value 50 1000 times.\nComparing with original workflow: Note that the steps of the infer workflow so far produce the same results as the original workflow using the rep_sample_n() function we saw earlier. In other words, the following two code chunks produce similar results:\n\n\ninfer workflow\n\nbirths14 %&gt;%\n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 50, \n           type = \"bootstrap\") \n\n\n\n\nOriginal workflow\n\nrep_sample_n(births14, \n             size = 1000, \n             replace = TRUE,\n             reps = 50)\n\n\n\n\n\n3. calculate() summary statistics\n\n\n\n\n\n\nFigure 9: Diagram of calculate()d summary statistics.\n\n\n\nAfter we generate() many replicates of bootstrap resampling with replacement, we next want to summarize each of the 50 resamples of size 1000 to a single sample statistic value. As seen in Figure 9, the calculate() function does this.\nIn our case, we want to calculate the slope between mother’s age and baby’s birth weight for each bootstrap resample of size 1000. To do so, we set the stat argument to \"slope\".\n\n\n\n\n\n\nTip\n\n\n\nYou can also set the stat argument to a variety of other common summary statistics, like \"median\", \"sum\", \"sd\" (standard deviation), and \"prop\" (proportion). To see a list of all possible summary statistics you can use, type ?calculate and read the help file.\n\n\nLet’s save the result in a data frame called bootstrap_distribution and explore its contents:\n\nbootstrap_distribution &lt;- births14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 50) %&gt;% \n  calculate(stat = \"slope\")\n\nbootstrap_distribution\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 50 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1 0.0328 \n 2         2 0.0181 \n 3         3 0.0123 \n 4         4 0.0106 \n 5         5 0.0183 \n 6         6 0.0190 \n 7         7 0.0145 \n 8         8 0.00824\n 9         9 0.0155 \n10        10 0.0160 \n# ℹ 40 more rows\n\n\nObserve that the resulting data frame has 50 rows and two columns. The replicate column corresponds to the 50 replicates and the stat column corresponds to the estimated slope for each resample.\n\n\n4. visualize() the results\n\n\n\n\n\n\nFigure 10: Diagram of closing the entire process by visualize()ing the results.\n\n\n\nThe visualize()verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical stat variable’s values. The pipeline of the main infer verbs used for exploring bootstrap distribution results is shown in Figure 11.\n\nvisualize(bootstrap_distribution)\n\n\n\n\n\n\n\n\n\nFigure 11: Bootstrap distribution of slope statistics from 50 bootstrap resamples.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe visualize() function can take many other arguments which we’ll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values.\n\n\nComparing with original workflow: In fact, visualize() is a wrapper function for the ggplot() function that uses a geom_histogram() layer. That’s a lot of fancy language which means that the visualize() function does the same thing as we did previously with ggplot(), just with fewer steps.\n\n\ninfer workflow\n\nvisualize(bootstrap_distribution)\n\n\n\n\nOriginal workflow\n\nggplot(data = bootstrap_distribution,\n       mapping = aes(x = stat)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCareful! It might sound tempting to ditch the ggplot() code altogether now that you know of a simpler approach. The visualize() function only works for a specific case – a data frame containing a distribution of calculate()d statistics.\n\n\nLet’s recap the steps of the infer workflow for constructing a bootstrap distribution and then visualizing it in Figure 12.\n\n\n\n\n\n\nFigure 12: infer package workflow for resampling\n\n\n\n\n\n\n2.3 Virtually resampling 1,000 times\nTo change from resampling 50 times to resampling 1,000 times, only one line of code needs to change—the number of reps. In the code process below, we’ve chained together the entire infer pipeline, connecting the specify, generate, calculate, and visualize steps.\n\nbootstrap_1000 &lt;- births14 |&gt; \n  specify(response = weight, \n          explanatory = mage) |&gt; \n  generate(reps = 1000, \n           type = \"bootstrap\") |&gt; \n  calculate(stat = \"slope\") \n\nvisualize(bootstrap_1000) + \n  labs(x = \"Bootstrap Slope Statistic\", \n       y = \"Bootstrap Distribution of 1,000 Resamples\")\n\n\n\n\n\n\n\nFigure 13: Bootstrap distribution with 1,000 replicates.\n\n\n\n\n\n\n\n\n\n\n\nChanging axis labels\n\n\n\nNotice you can still use the labs() function to change your axis labels. Notice that you still need to connect the axis labels with the plot using a + sign.\n\n\nWhat do you notice about this distribution? What distribution does it resemble? Why is that the case?\n\n\n2.4 Connection to sampling distributions\nThe histogram of sample slopes in Figure 13 is called the bootstrap distribution. The bootstrap distribution is an approximation to the sampling distribution of sample slopes.\nIf you recall back to the last chapter, a sampling distribution was a distribution of statistics from repeatedly sampling from the population. However, a bootstrap distribution is a distribution of statistics from repeatedly resampling from the sample. This may seem rather strange, how can a distribution based entirely on the sample approximate a distribution based on the population? That’s a great question!\nThe bootstrapping process hinges on the belief that the sample is representative of the population. Meaning, there are not systematic differences between the sample and the population. For example, if there were no young mothers in the sample of 1,000 birth records in the births14 dataset. If we have a random sample, however, then on average our sample should look very similar to our population. Meaning, the individuals in the sample can “stand in” for individuals in the population with similar characteristics. So, we can think of repeatedly resampling from our sample as a similar process as sampling from the population.\n\n\n\n\n\n\nNo guaranteed representation\n\n\n\n\n\nThe process of randomly sampling does not guarantee that smaller groups of observations will always appear in the sample. Rather, by randomly sampling from the population, on average the representation of individuals in the sample should reflect the proportion of individuals in the population.\nTo me, this feels rather uncomfortable as a random sample assumes that you can generalize from the sample onto the population from which it was drawn. This means you are generalizing the observations of a few individuals onto the entire population of similar individuals. For example, if you were to collect a random sample of Cal Poly students, it is likely your sample would include very few Black students (as Cal Poly is a predominantly white institution). But, if your sample was random, statistically you could infer from your small sample of Black students onto the entire population of Black students at Cal Poly. That seems kind of iffy to me."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#sec-understanding",
    "href": "weeks/chapters/week7-reading2.html#sec-understanding",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "3 Understanding confidence intervals",
    "text": "3 Understanding confidence intervals\nLet’s start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish!\nIn the births14 investigation, we are trying to estimate the population slope for the relationship between mother’s age and baby’s birth weight (\\(\\beta_1\\)) for all babies born in the US in 2014. Think of the value of \\(\\beta_1\\) as a fish.\nOn the one hand, we could use the appropriate point estimate/sample statistic to estimate \\(\\beta_1\\), which we saw in Table 1 is the sample slope \\(b_1\\). Based on our sample of 1000 birth records, the sample slope was 0.014. Think of using this value as “fishing with a spear.”\nWhat would “fishing with a net” correspond to? Look at the bootstrap distribution in Figure 13 once more. Between which values would you say that “most” sample slopes lie? While this question is somewhat subjective, saying that most sample slopes lie between 0 and 0.03 would not be unreasonable. Think of this interval as the “net.”\nWhat we’ve just illustrated is the concept of a confidence interval, which I’ll abbreviate with “CI” throughout this chapter. As opposed to a point estimate / sample statistic that estimates the value of an unknown population parameter with a single value, a confidence interval gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates / sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets.\n\n\n\n\n\n\nFigure 14: Analogy of difference between point estimates and confidence intervals.\n\n\n\nOur proposed interval of 0 to 0.03 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the percentile method and the standard error method.\nBoth methods for confidence interval construction share some commonalities. First, they are both constructed from a bootstrap distribution, as you constructed in the previous section and visualized in Figure 13.\nSecond, they both require you to specify the confidence level. Commonly used confidence levels include 90%, 95%, and 99%. All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals.\n\n3.1 Percentile method\nOne method to construct a 95% confidence interval is to use the middle 95% of values of the bootstrap distribution. We can do this by computing the 2.5th and 97.5th percentiles, which are 0.0002 and 0.0281, respectively. This is known as the percentile method for constructing confidence intervals.\nFor now, let’s focus only on the concepts behind a percentile method constructed confidence interval; we’ll show you the code that computes these values in the next section.\nLet’s mark these percentiles on the bootstrap distribution with vertical lines in Figure 15. About 95% of the slope variable values in bootstrap_1000 fall between 0.0002 and 0.0281, with 2.5% to the left of the leftmost line and 2.5% to the right of the rightmost line.\n\n\n\n\n\n\n\n\nFigure 15: Percentile method 95% confidence interval. Interval endpoints marked by vertical lines.\n\n\n\n\n\n\n\n3.2 Standard error method\nRecall in the last chapter we saw that if a numerical variable follows a normal distribution, or, in other words, the histogram of this variable is bell-shaped, then roughly 95% of values fall between \\(\\pm\\) 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure 13 is normally shaped, let’s use this fact about normal distributions to construct a confidence interval in a different way.\nFirst, recall the bootstrap distribution has a mean equal to 0.014. This value almost coincides exactly with the value of the sample slope \\(b_1\\) of our original 1000 birth records 0.014. Second, let’s compute the standard deviation of the bootstrap distribution using the bootstrap slope statistics stored in the stat column of the bootstrap_1000 data frame:\n\nbootstrap_1000 %&gt;% \n  summarize(SE = sd(stat))\n\n# A tibble: 1 × 1\n       SE\n    &lt;dbl&gt;\n1 0.00722\n\n\nWhat is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution. Thus, the variability of the sampling distribution may be approximated by the variability of the resampling distribution. Recall also that the standard deviation of a sampling distribution has a special name: the standard error. Putting these two facts together, we can say that 0.00722 is an approximation of the standard error of \\(b_1\\).\nTraditional theory-based methodologies for inference also have formulas for standard errors, assuming some conditions are not violated. In this method, we are not using a formula to get our standard error, but using the standard error of the bootstrap distribution. Thus, using our 95% rule of thumb about normal distributions, we can use the following formula to determine the lower and upper endpoints of a 95% confidence interval for \\(\\beta_1\\):\n\\[\nb_1 \\pm 1.96 \\times SE = (b_1 - 1.96 \\times SE, b_1 + 1.96 \\times SE)\n\\]\n\\[\n= (0.014 - 1.96 \\times 0.007, 0.014 + 1.96 \\times 0.007)\n\\]\n\\[\n= (0.00028, 0.02772)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nTo use the bootstrap standard error in this formula the bootstrap distribution must be bell shaped and symmetric. This is often the case with bootstrap distributions, especially those in which the original distribution of the sample is not highly skewed, however it is not always the case. Next week, we’ll go deeper into the explorations of model conditions.\n\n\nLet’s now add the SE method confidence interval with dashed lines in Figure 16.\n\n\n\n\n\n\n\n\nFigure 16: Comparing two 95% confidence interval methods.\n\n\n\n\n\nWe see that both methods produce nearly identical 95% confidence intervals for \\(\\beta_1\\) with the percentile method yielding \\((0.0002, 0.0281)\\) while the standard error method produces \\((0.0001, 0.0284)\\).\nNow that we’ve introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let’s explore the code that allows us to construct them."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#constructing-confidence-intervals",
    "href": "weeks/chapters/week7-reading2.html#constructing-confidence-intervals",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "4 Constructing confidence intervals",
    "text": "4 Constructing confidence intervals\nNow that we’ve covered two methods for obtaining confidence intervals, let’s now check out the infer package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the infer package!\n\nPercentile method with infer\nRecall the percentile method for constructing 95% confidence intervals we introduced in Section 3.1. This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean in the bootstrap distribution.\nWe can compute the 95% confidence interval using the get_confidence_interval() function function from the infer package. This function takes three arguments (inputs):\n\na data frame containing calculate()d statistics (e.g., slope statistics)\na confidence level (e.g., 0.95 or 0.99)\nthe type of interval to be made – \"percentile\" or \"se\"\n\n\npercentile_ci &lt;- get_confidence_interval(bootstrap_distribution, \n                                         level = 0.95, \n                                         type = \"percentile\")\npercentile_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.00185   0.0285\n\n\nAlternatively, we can visualize this interval by adding a shade_confidence_interval() layer to the visualize() step. This step requires the endpoints of the interval to be specified, so the previous step must come before the following step.\n\nvisualize(bootstrap_1000) + \n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\nFigure 17: Percentile method 95% confidence interval shaded corresponding to potential values.\n\n\n\n\n\nObserve in Figure 17 that 95% of the sample slopes stored in the stat variable in bootstrap_1000 fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right.\n\n\n\n\n\n\nTip\n\n\n\nThe default colors for shade_confidence_interval() are color = \"mediumaquamarine\" and fill = \"turquoise\". You can, however, change these colors if you wish!\n\nvisualize(bootstrap_distribution) + \n  shade_ci(endpoints = percentile_ci, color = \"hotpink\", fill = \"khaki\")\n\n\n\n\n\nStandard error method with infer\nRecall the standard error method for constructing 95% confidence intervals we introduced in Section 3.2. From properties of the Normal Distribution, we know that for any distribution that is bell shaped and symmetric, roughly 95% of the values lie within two standard deviations of the mean. Recall that in the case of the sampling distribution, the standard deviation has a special name: the standard error. Moreover, remember that the bootstrap distribution provides us with a robust estimate of the standard error of the sampling distribution.\nSo in our case, 95% of values of the bootstrap distribution will lie within \\(\\pm 1.96\\) standard errors of \\(b_1\\). Thus, a 95% confidence interval is\n\\[\nb_a \\pm 1.96 \\times SE = (b_1 - 1.96 \\times SE, b_1 + 1.96 \\times SE)\n\\]\nComputation of the 95% confidence interval can once again be done by inputting the the bootstrap_1000 data frame we created into the get_confidence_interval() function, using roughly the same code as before. There are two components, however, that must change:\n\nwe need to set the type argument to be \"se\"\nwe need to specify where the confidence interval should be centered – the point_estimate\n\nTo calculate the point estimate, we need one additional step before calculating our confidence interval. Rather than using the output from the lm() function like we did in Table 1, let’s obtain our observed slope statistic using the infer pipeline! This process should look familiar, in fact the only part that has changed is we no longer have a generate() step. We are not generating any resamples, so that step is no longer needed. All we need to do is specify() our variables and calculate() the \"slope\" statistic.\n\nobs_slope &lt;- births14 |&gt; \n  specify(response = weight, \n          explanatory = mage) |&gt; \n  calculate(stat = \"slope\")\n\nstandard_error_ci &lt;- get_confidence_interval(bootstrap_1000, \n                                             type = \"se\", \n                                             point_estimate = obs_slope, \n                                             level = 0.95)\nstandard_error_ci\n\n# A tibble: 1 × 2\n   lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;\n1 0.0000947   0.0284\n\n\nIf we would like to visualize this interval, we can once again add a shade_confidence_interval() layer to our plot. However, when visualizing an interval constructed using the SE method, we need to declare the endpoints of the interval, as seen in Figure 18.\n\nvisualize(bootstrap_1000) + \n  shade_confidence_interval(endpoints = standard_error_ci)\n\n\n\n\n\n\n\n\n\nFigure 18: Standard-error-method 95% confidence interval.\n\n\n\n\n\nAs noted in Section 3.2, both methods produce similar confidence intervals:\n\n\n\n\nTable 5: Comparison of 95% confidence intervals between percentile and SE methods.\n\n\n\n\n\n\ntype\n2.5%\n97.5%\n\n\n\n\nPercentile\n-0.0018499\n0.0285359\n\n\nSE\n0.0000947\n0.0283909"
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#interpreting-confidence-intervals",
    "href": "weeks/chapters/week7-reading2.html#interpreting-confidence-intervals",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "5 Interpreting confidence intervals",
    "text": "5 Interpreting confidence intervals\nNow that we’ve shown you how to construct confidence intervals using a sample drawn from a population, let’s now focus on how to interpret their effectiveness. The effectiveness of a confidence interval is judged by whether or not it contains the true value of the population parameter. Going back to our fishing analogy in Section 3, this is like asking, “Did our net capture the fish?”.\nSo, for example, does our percentile-based confidence interval of (-0.0018, 0.0285) “capture” the slope of the relationship between mother’s age and baby’s birth weight (\\(\\beta_1\\)) for all babies born in the US in 2014? Alas, we’ll never know, because we don’t know what the true value of \\(\\beta_1\\) is. After all, we’re sampling to estimate it!\nIn order to interpret a confidence interval’s effectiveness, we need to know what the value of the population parameter is. That way we can say whether or not a confidence interval “captured” this value. In the case of births14 dataset, we have the ability to go find the original dataset, housing the birth records of every baby born in the US in 2014. I did so, and found that the slope for the relationship between mother’s age and baby’s birth weight (\\(\\beta_1\\)) for all babies born in the US in 2014 was \\(\\beta_1 = 0.0007\\).\n\n5.1 Did our interval(s) capture \\(\\beta_1\\)?\nNow that we know the value of \\(\\beta_1\\), we can see if the intervals we constructed previously do in fact contain this parameter. The percentile interval of (-0.0018, 0.0285) contains \\(\\beta_1 = 0.0007\\) and the SE interval of 0.0001, 0.0284) also contains \\(\\beta_1 = 0.0007\\). Will our 95% confidence intervals always contain \\(\\beta_1\\)? Let’s see!\n\n\n5.2 Constructing more intervals\nLet’s construct 50 more confidence intervals for \\(\\beta_1\\) using the infer workflow we learned before. We can then compare how many of these confidence intervals “captured” the true value of \\(\\beta_1\\), which we know to be 0.0007. That is to say, “Did the net capture the fish?”.\nIn the code below, we create a second bootstrap distribution, also with 1,000 bootstrap resamples. Then, using this second bootstrap distribution, we use the percentile method to calculate a 95% confidence interval.\n\n\n\n\n\n\nChoice of method\n\n\n\nI’ve chosen to use the percentile method as we saw that the intervals had similar results and the percentile method has fewer steps. So, I’m choosing to be efficient! However, since we saw the bootstrap distribution was bell shaped and symmetric, either method would work.\n\n\n\nbootstrap_1000_v2 &lt;- births14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 1000, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n  \npercentile_ci_2 &lt;- get_confidence_interval(bootstrap_1000_v2, \n                                         level = 0.95)\npercentile_ci_2\n\n# A tibble: 1 × 2\n    lower_ci upper_ci\n       &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.0000326   0.0285\n\n\nNotice this interval, also contains \\(\\beta_1\\). Let’s do this again…\n\nbootstrap_1000_v3 &lt;- births14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 1000, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n  \npercentile_ci_3 &lt;- get_confidence_interval(bootstrap_1000_v3, \n                                         level = 0.95)\npercentile_ci_3\n\n# A tibble: 1 × 2\n   lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.000133   0.0285\n\n\nAgain, our interval contains \\(\\beta_1\\). Let’s scale this up and look at 50 different intervals constructed using this method. Figure 19 visualizes these 50 intervals, where:\n\nthe true value of \\(\\beta_1 = 0.0007\\) is marked with a red, vertical line\neach of the intervals is marked with a horizontal line\n\nIf the interval contained \\(\\beta_1 = 0.0007\\), the line is colored grey. If the interval did not contain \\(\\beta_1 = 0.0007\\), the line is colored black.\n\n\n\n\n\n\n\n\nFigure 19: 50 percentile-based 95% confidence intervals for the true slope for the relationship between mother’s age and baby’s birth weight, for all babies born in the US in 2014.\n\n\n\n\n\nOf the 50 95% confidence intervals, 46 of them captured the true value of \\(\\beta_1 = 0.0007\\), whereas 4 of them didn’t. In other words, 46 of our nets caught the fish, and 4 of our nets didn’t.\nThis is where the “95% confidence level” comes into play: for every 50 95% confidence intervals constructed, we expect that about 95% of them will capture \\(\\beta_1\\) and 5% of them won’t. That’s a bit tricky for 50, as 95% of 50 is 47.5. Technically, we can only have either 47 or 48 intervals contain \\(\\beta_1\\), but this should reinforce that the process is not exact. We will not always get 95% of the intervals containing \\(\\beta_1\\), but on average we expect that 95% of them should.\n\n\n5.3 Connection to the empiracle rule\nYou might wonder, why do only 5% of the 95% confidence intervals miss the parameter they are estimating. This reasoning goes back to what you read on the empiracle rule for bell-shaped and symmetric curves. Namely, the rule of thumb that 95% of values will lie within approximately 2 standard deviations of the center of the distribution. The center of a sampling distribution is the value of the parameter. For our example, this value is \\(\\beta_1 = 0.0007\\), the value of the slope for the relationship between mother’s age and baby’s birth weight for all babies born in the US in 2014.\nFigure 20 displays the sampling distribution of \\(b_1\\), centered at \\(\\beta_1 = 0.0007\\). Because our sampling distribution is bell-shaped and symmetric, we know that 95% of the slope statistics should fall within 2 standard errors of the center, as noted by the two dashed lines. That means, only 5% of the slope statistics should fall outside this interval. Our sample slope statistic (\\(b_1\\)) is always the center of our confidence interval. So, if our sample slope statistic is greater than 2 standard deviations from the center of the sampling distribution, its confidence interval will not contain \\(\\beta_1\\).\n\n\n\n\n\n\n\n\nFigure 20: Rules of thumb about areas under normal curves.\n\n\n\n\n\n\n\n5.4 Precise and shorthand interpretation\nLet’s return our attention to 95% confidence intervals. The precise and mathematically correct interpretation of a 95% confidence interval is a little long-winded:\n\nPrecise interpretation: If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter.\n\nThis is what we observed in Figure 19. Our confidence interval construction procedure is 95% reliable. That is to say, we can expect our confidence intervals to include the true population parameter about 95% of the time.\nA common but incorrect interpretation is: “There is a 95% probability that the confidence interval contains \\(\\beta_1\\).” Looking at Figure 19, each of the confidence intervals either does or doesn’t contain \\(\\beta_1\\). In other words, the probability is either 100% or 0%.\nSo if the 95% confidence level only relates to the reliability of the confidence interval construction procedure and not to a given confidence interval itself, what insight can be derived from a given confidence interval? Loosely speaking, we can think of these intervals as our “best guess” of a plausible range of values for the slope of the relationship between mother’s age and baby’s birth weight \\(\\beta_1\\) of all babies born in the US in 2014. It is typical to use the following shorthand summary of the precise interpretation:\n\nWe are 95% “confident” that a 95% confidence interval captures the value of the population parameter. We use quotation marks around “confident” to emphasize that while 95% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. In other words, it’s our best net.\n\nSo returning to the first confidence interval we calculated, we would interpret this interval as “We are 95% ‘confident’ that the slope of the relationship between mother’s age and baby’s birth weight \\(\\beta_1\\) of all babies born in the US in 2014 is between -0.0018 and 0.0285.\n\n\n5.5 Width of confidence intervals\nNow that we know how to interpret confidence intervals, let’s go over some factors that determine their width.\n\nImpact of confidence level\nOne factor that determines confidence interval widths is the pre-specified confidence level. For example, we used 95% confidence intervals for this reading, but we could have equally chosen 85% intervals. A 95% confidence interval created using the percentile method contains 95% of the values in the bootstrap distribution. An 85% confidence interval would only contain 85% of these values, so it would be a narrower confidence interval. The quantification of the confidence level should match what many expect of the word “confident.” In order to be more confident in our best guess of a range of values, we need to widen the range of values.\nTo elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul’s temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50°F - 95°F (10°C - 35°C). However, if we wanted a temperature range we were absolutely confident about, we would need to widen it.\nWe need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32°F - 110°F (0°C - 43°C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70°F - 85°F (21°C - 30°C).\nSo in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to be more confident, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level.\n\nThe moral of the story is: Higher confidence levels tend to produce wider confidence intervals.\n\n\n\nImpact of sample size\nPulling from what we learned about the Central Limit Theorem at the end of the last reading, we know that variability decreases when sample size increases. We quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the standard error. So as the sample size increases, the standard error decreases.\nConnecting to what you learned in this reading, if we have a large sample size, the spread of the bootstrap distribution will be smaller, which in turn leads to smaller estimated standard errors, and narrower confidence intervals. In Section 3.2 you saw directly how the standard error is used when calculating a confidence interval."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Model Selection",
    "section": "",
    "text": "Download the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#reading-guide-due-tuesday-by-noon",
    "href": "weeks/week-6.html#reading-guide-due-tuesday-by-noon",
    "title": "Model Selection",
    "section": "",
    "text": "Download the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#model-selection",
    "href": "weeks/week-6.html#model-selection",
    "title": "Model Selection",
    "section": "1.1 Model Selection",
    "text": "1.1 Model Selection\nThe best model is not always the most complicated. Sometimes including variables that are not evidently important can actually reduce the accuracy of predictions. This week, we will learn about model selection strategies – strategies which can help us eliminate variables from the model that are found to be less important. It’s common (and hip, at least in the statistical world) to refer to models that have undergone such variable pruning as parsimonious.\nIn practice, the model that includes all available predictors is often referred to as the full model. The full model may not be the best model, and if it isn’t, we want to identify a smaller model that is preferable.\nIn Section 6.3.1 of ModernDive, you used visualizations to compare an interaction model (different slopes) with a parallel slopes model. In the figures below, both the parallel slope and the interaction model attempt to explain \\(y =\\) the average math SAT score for various high schools in Massachusetts.\nWhen comparing the left and right-hand plots below, we observed that the three lines corresponding to small, medium, and large high schools were not that different. Given this similarity, we stated it could be argued that the “simpler” parallel slopes model should be favored.\n# Interaction model\nggplot(MA_schools, \n       aes(x = perc_disadvan, y = average_sat_math, color = size)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Percent economically disadvantaged\", y = \"Math SAT Score\", \n       color = \"School size\", title = \"Interaction model\")\n\n# Parallel slopes model\nggplot(MA_schools, \n       aes(x = perc_disadvan, y = average_sat_math, color = size)) +\n  geom_point(alpha = 0.25) +\n  geom_parallel_slopes(se = FALSE) +\n  labs(x = \"Percent economically disadvantaged\", y = \"Math SAT Score\", \n       color = \"School size\", title = \"Parallel slopes model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Table 1 and Table 2, we observed that the interaction model was “more complex” in that the regression table had six (6) rows versus the four (4) rows of the parallel slopes model.\n\nsat_interaction &lt;- lm(average_sat_math ~ perc_disadvan * size, \n                          data = MA_schools)\nget_regression_table(sat_interaction)\n\n\n\n\n\nTable 1: Interaction model coefficient estimates\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n594.327\n13.288\n44.726\n0.000\n568.186\n620.469\n\n\nperc_disadvan\n-2.932\n0.294\n-9.961\n0.000\n-3.511\n-2.353\n\n\nsize: medium\n-17.764\n15.827\n-1.122\n0.263\n-48.899\n13.371\n\n\nsize: large\n-13.293\n13.813\n-0.962\n0.337\n-40.466\n13.880\n\n\nperc_disadvan:sizemedium\n0.146\n0.371\n0.393\n0.694\n-0.585\n0.877\n\n\nperc_disadvan:sizelarge\n0.189\n0.323\n0.586\n0.559\n-0.446\n0.824\n\n\n\n\n\n\n\n\n\n\n\nsat_parallel_slopes &lt;- lm(average_sat_math ~ perc_disadvan + size, \n                              data = MA_schools)\nget_regression_table(sat_parallel_slopes)\n\n\n\n\n\nTable 2: Parallel slopes model coefficient estimates\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n588.19\n7.607\n77.325\n0.000\n573.23\n603.15\n\n\nperc_disadvan\n-2.78\n0.106\n-26.120\n0.000\n-2.99\n-2.57\n\n\nsize: medium\n-11.91\n7.535\n-1.581\n0.115\n-26.74\n2.91\n\n\nsize: large\n-6.36\n6.923\n-0.919\n0.359\n-19.98\n7.26\n\n\n\n\n\n\n\n\n\n\nIn this reading, we’ll mimic the model selection we just performed using the qualitative “eyeball test,” but this time using a numerical and quantitative approach. Specifically, we’ll use the \\(R^2\\) summary statistic (pronounced “R-squared”), also called the “coefficient of determination.” But first, we must introduce one new concept: the variance of a numerical variable.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#r-squared-r2",
    "href": "weeks/week-6.html#r-squared-r2",
    "title": "Model Selection",
    "section": "1.2 R-squared (\\(R^2\\))",
    "text": "1.2 R-squared (\\(R^2\\))\nWe’ve previously studied two summary statistics of the spread (or variation) of a numerical variable: the standard deviation and the interquartile range (IQR). We now introduce a third summary statistic of spread: the variance. The variance is merely the standard deviation squared and it can be computed in R using the var() summary function within summarize()1.\nLet’s investigate the variability of the SAT math scores, using the MA_schools dataset. In the first plot, we visualize the overall spread / variability in these scores, using a histogram. In the second plot, we investigate the relationship between SAT math scores and the percent of students who are economically disadvantaged.\nggplot(MA_schools, \n       aes(x = average_sat_math)) +\n  geom_histogram(binwidth = 30) +\n  labs(x = \"Math SAT Score\")\n\nggplot(MA_schools, \n       aes(x = perc_disadvan, y = average_sat_math)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Percent economically disadvantaged\", y = \"Math SAT Score\")\n\n\n\n\n\n\nExploring the variability of SAT math scores\n\n\n\n\n\n\n\nExploring the variability of SAT math scores\n\n\n\n\n\nAs evidenced by the plot, there appears to be a moderately strong relationship between SAT math scores and the percent of students who are economically disadvantaged. Next, let’s fit the linear regression for modeling the relationship between SAT math scores and the percent of students who are economically disadvantaged.\nRecall, we can use the get_regression_points() function2 to our saved linear model (fit with the lm() function) to get:\n\nthe observed values (\\(y\\))\nthe fitted values (\\(\\hat{y}\\))\nthe residuals (\\(y - \\hat{y}\\))\n\n\nget_regression_points(sat_simple) \n#&gt; # A tibble: 332 × 5\n#&gt;      ID average_sat_math perc_disadvan average_sat_math_hat residual\n#&gt;   &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1              516          21.5                 522.    -5.52\n#&gt; 2     2              514          22.7                 518.    -4.18\n#&gt; 3     3              534          14.6                 541.    -6.70\n#&gt; 4     4              581           6.3                 564.    17.2 \n#&gt; 5     5              592          10.3                 553.    39.4 \n#&gt; 6     6              576          10.3                 553.    23.4 \n#&gt; # ℹ 326 more rows\n\nLet’s now use the var() summary function within a summarize() to compute the variance of these three terms:\n\nget_regression_points(sat_simple) %&gt;% \n  summarize(var_y = var(average_sat_math), \n                      var_y_hat = var(average_sat_math_hat), \n                      var_residual = var(residual))\n#&gt; # A tibble: 1 × 3\n#&gt;   var_y var_y_hat var_residual\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 3691.     2570.        1121.\n\nObserve that the variance of \\(y\\) is equal to the variance of \\(\\hat{y}\\) plus the variance of the residuals. But what do these three terms tell us individually?\nFirst, the variance of \\(y\\) (denoted as \\(\\text{var}(y)\\)) tells us how much do Massachusetts high schools differ in average math SAT scores. The goal of regression modeling is to fit a model that hopefully explains this variation. In other words, we want to understand what factors explain why certain schools have high math SAT scores, while others have low scores. This is independent of the model; this is just data. In other words, whether we fit an interaction or parallel slopes model, \\(\\text{var}(y)\\) remains the same.\nSecond, the variance of \\(\\hat{y}\\) (denoted as \\(\\text{var}(\\hat{y})\\)) tells us how much the fitted values from our interaction model vary. That is to say, after accounting for (1) the percentage of the study body that is socioeconomically disadvantaged and (2) school size in an interaction model, how much do our model’s explanations of average math SAT scores vary?\nThird, the variance of the residuals tells us how much do “the left-overs” from the model vary. Observe how the points in the interaction plot above (on the left) scatter around the three lines. Say instead all the points fell exactly on one of the three lines. Then all residuals would be zero and hence the variance of the residuals would be zero.\nWe’re now ready to introduce \\(R^2\\)!\n\\[R^2 = 1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\]\n\\(R^2\\) is the proportion of the spread / variation of the outcome variable \\(y\\) that is explained by our model, where our model’s explanatory power is embedded in the fitted values \\(\\hat{y}\\). Furthermore, since it can be mathematically proven that \\(0 \\leq \\text{var}(y) \\leq \\text{var}(\\hat{y})\\) (a fact we leave for an advanced class on regression), we are guaranteed that:\n\\[0 \\leq R^2 \\leq 1\\]\nThus, \\(R^2\\) can be interpreted as follows:\n\n\\(R^2\\) values of 0 tell us that our model explains 0% of the variation in \\(y\\). Say we fit a model to the Massachusetts high school data and obtained \\(R^2 = 0\\). This would be telling us that the combination of explanatory variables (\\(x\\)) we used and model form we chose (interaction or parallel slopes) explains 0% of the variation in the SAT math scores. Or, in other words, the combination of explanatory variables (\\(x\\)) we used and model form we chose tell us nothing about average math SAT scores. The model is a poor fit.\n\\(R^2\\) values of 1 tell us that our model explains 100% of the variation in \\(y\\). Say we fit a model to the Massachusetts high school data and obtained \\(R^2 = 1\\). This would be telling us that the combination of explanatory variables (\\(x\\)) we used and model form we chose (interaction or parallel slopes) explains 0% of the variation in the SAT math scores. Or, in other words, the combination of explanatory variables (\\(x\\)) we used and model form we chose tell us everything we need to know about average math SAT scores.\n\nIn practice however, \\(R^2\\) values of 1 almost never occur. Think about it in the context of Massachusetts high schools. There are an infinite number of factors that influence why certain high schools perform well on SAT’s on average while others don’t perform well. The idea that a human-designed statistical model can capture all the heterogeneity of all high school students in Massachusetts is bordering on hubris. However, even if such models are not perfect, they may still prove useful in determining educational policy. A general principle of modeling we should keep in mind is a famous quote by eminent statistician George Box: “All models are wrong, but some are useful.”\n\nModel selection using R-squared\nWhen we learned about simple / basic regression, we discussed how, given a set of basic regressions, you could choose the “best” basic regression as the one where the explanatory variable has the strongest relationship with the response. We discussed this in terms of correlation, but for simple linear regression, \\(R^2\\) is the square of the correlation.\nThe ncbirths dataset contains a variety of variables on the habits and practices of expectant mothers and the birth of their children. Suppose, we were interested in choosing a simple linear regression with the variable that has the strongest relationship with weight (the birth weight of the baby). For explanatory variables, we have:\n\nweeks – length of pregnancy\nmage – mother’s age\nvisits – number of hospital visits during pregnancy\ngained – weight gained during pregnancy\n\nLet’s explore each of these basic regression models!\n\n\n\n\nTable 3: Comparison of four different simple regressions on birth weight of baby\n\n\n\n\n\n\n\ncor\nvar_y\nvar_y_hat\nvar_residual\nr_sq\n\n\n\n\nWeeks\n0.670\n2.27\n1.019\n1.25\n0.449\n\n\nGained\n0.154\n2.22\n0.053\n2.17\n0.024\n\n\nVisits\n0.135\n2.21\n0.040\n2.17\n0.018\n\n\nMother's Age\n0.055\n2.28\n0.007\n2.27\n0.003\n\n\n\n\n\n\n\n\n\n\nFrom Table 3, we can see that weeks has by far the strongest relationship with the birth weight of the baby, with a correlation of 0.670. Notice how we could get the \\(R^2\\) for this regression model (weight ~ weeks) two ways:\n\nWe could square the correlation: \\(0.670^2 =\\) 0.449\nWe could find \\(1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)} =\\) 0.449\n\nBoth methods produce an \\(R^2\\) of 0.449. This value tells us that the simple linear regression with weeks as the explanatory variable is able to explain 44.9% of the variability in the birth weight of a baby.\n\n\nAdding more variables\nYou might say, well why not add more than one variable into the model. That makes sense, given what we’ve learned about multiple linear regression. However, when we have a regression with more than one explanatory variable, \\(R^2\\) is no longer a good summary measure of how much variability in the response we’ve explained with our model. The problem lies in the formula for \\(R^2\\)!\nThis strategy for estimating \\(R^2\\) is acceptable when there is just a single explanatory variable. However, it becomes less helpful when there are many variables. The regular \\(R^2\\) is a biased estimate of the amount of variability explained by the model when applied to model with more than one predictor. This is because any variable when added to the model will decrease the residuals, even just by a little bit.\nDecreasing the residuals leads to:\n\na smaller value of \\(\\text{var}(\\text{residuals})\\)\na smaller fraction of \\(\\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\)\na larger number of \\(1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\)\n\nThus, adding any variable will always result in a larger value of \\(R^2\\).\nFor example, I’ve added a new column to the ncbirths dataset, one that is entirely simulated noise and has no relationship with the birth weight of the baby. Here is a plot of my variable, just to confirm!\n\n\n\n\n\n\n\n\n\nAlright, now I’m including this noise variable as an explanatory variable in the multiple linear regression, where weeks is also an explanatory variable.\n\nnoise_lm &lt;- lm(weight ~ weeks + noise, data = ncbirths_noise)\n\nget_regression_points(noise_lm) |&gt; \n    summarize(var_y = var(weight), \n              var_y_hat = var(weight_hat), \n              var_residual = var(residual), \n              r_sq = 1 - (var_residual / var_y)\n              )\n#&gt; # A tibble: 1 × 4\n#&gt;   var_y var_y_hat var_residual  r_sq\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2.27      1.02         1.25 0.450\n\nFrom the table above, we can see that the \\(R^2\\) value has increased. Not by much, but still! We went from an \\(R^2\\) of 0.449 when weeks was the only explanatory variable, to an \\(R^2\\) of 0.45 when noise was added as an explanatory variable. This may seem like a small change, but it may not necessarily always be small! Thus, this behavior was concerning to Statisticians, and they invented an alternative.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#adjusted-r2",
    "href": "weeks/week-6.html#adjusted-r2",
    "title": "Model Selection",
    "section": "1.3 Adjusted \\(R^2\\)",
    "text": "1.3 Adjusted \\(R^2\\)\nTo get a better estimate, we use the adjusted \\(R^2\\)!\n\n\n\n\n\n\nAdjusted R-squared as a tool for model assessment.\n\n\n\nThe adjusted R-squared is computed as:\n\\[R^2_{adj} = 1 - \\frac{\\text{var(residuals)} / (n - k - 1)}{\\text{var}(y) / (n - 1)}\\] \\[R^2_{adj} = 1 - \\frac{\\text{var(residuals)}}{\\text{var}(y)} \\times \\frac{(n - 1)}{(n - k - 1)}\\] where \\(n\\) is the number of observations used to fit the model and \\(k\\) is the number of predictor variables in the model. Remember that a categorical predictor with \\(p\\) levels will contribute \\(p - 1\\) to the number of variables in the model because we need \\(p - 1\\) offsets!\n\n\nBecause \\(k\\) is never negative, the adjusted \\(R^2\\) will be smaller – often times just a little smaller – than the unadjusted \\(R^2\\). The reasoning behind the adjusted \\(R^2\\) lies in the degrees of freedom associated with each variance, which is equal to \\(n - k - 1\\) in the multiple regression context. If we were to make predictions for new data using our current model, we would find that the unadjusted \\(R^2\\) would tend to be slightly overly optimistic, while the adjusted \\(R^2\\) formula helps correct this bias.\n\n\n\n\n\n\nCaution\n\n\n\nAdjusted \\(R^2\\) could also have been used instead of unadjusted \\(R^2\\) for basic regressions. However, when there is only \\(k = 1\\) explanatory variables, adjusted \\(R^2\\) is very close to regular \\(R^2\\), so this nuance isn’t typically important when the model has only one predictor.\n\n\nComing back to the multiple linear regression where I included a second noise variable, we can compare the unadjusted \\(R^2\\) to the adjusted \\(R^2\\). Rather than calculating all of these summary statistics “by hand”, we can use the get_regression_summaries() function to obtain interesting summary statistics about the model:\n\nget_regression_summaries(noise_lm)\n\n\n\n\n\nTable 4: Summary statistics from multiple linear regression with noise and weeks as predictor variables\n\n\n\n\n\n\nr_squared\nadj_r_squared\nmse\nrmse\n\n\n\n\n0.45\n0.449\n1.25\n1.12\n\n\n\n\n\n\n\n\n\n\nNotice, Table 4 shows that adjusted \\(R^2\\) is lower than unadjusted \\(R^2\\). This is exactly what we want! We want our \\(R^2\\) to increase only if the extra variable we added was “worth it.” Here, the adjusted \\(R^2\\) suggests that adding a second explanatory variable (noise) is not worth the additional model complexity.\n\nModel selection using adjusted \\(R^2\\)\nAlright, let’s apply the concept of adjusted \\(R^2\\) to a multiple linear regression we are actually interested in. At the beginning of this reading, you were asked to recall the “eyeball” selection method we used to determine that the interaction model and parallel slopes model were unnecessarily more complicated than the simple linear regression model.\nIn Table 5 below, I’ve calculated the adjusted \\(R^2\\) for both the interaction model and parallel slopes model, and compared them to the simple linear regression model:\n\n\n\n\nTable 5: Comparing R-squared values for three different models for SAT math scores\n\n\n\n\n\n\n\nr_squared\nadj_r_squared\nmse\nrmse\n\n\n\n\nInteraction Model\n0.699\n0.694\n1107\n33.3\n\n\nParallel Slopes Model\n0.699\n0.696\n1109\n33.3\n\n\nSimple Linear Regression Model\n0.696\n0.695\n1118\n33.4\n\n\n\n\n\n\n\n\n\n\nObserve how the adjusted \\(R^2\\) values for the interaction model and parallel slopes model are nearly identical to the unadjusted \\(R^2\\) value from the simple linear regression model. These summaries suggest, there is no justification for added model complexity. The simple linear regression model with percent of students economically disadvantaged is able to explain the same amount of variance in the SAT math scores as the more complex models which include the size of the school.\n\nExtending to evals data\nNow let’s repeat this \\(R^2\\) comparison between interaction and parallel slopes model for our models of \\(y\\) = teaching score for UT Austin professors. In the ModernDive textbook, you visually compared using parallel slopes or different slopes to model the relationship between age and teaching score for male and female professors.\n\n\n\n\nTable 6: Comparing R-squared values from interaction and parallel slope models for UT Austin evaluation scores data\n\n\n\n\n\n\n\nr_squared\nadj_r_squared\nmse\nrmse\n\n\n\n\nInteraction Model\n0.051\n0.045\n0.280\n0.529\n\n\nParallel Slopes Model\n0.039\n0.035\n0.284\n0.533\n\n\n\n\n\n\n\n\n\n\nObserve how the adjusted \\(R^2\\) values in Table 6 are now very different! The adjusted \\(R^2\\) for the interaction model is 0.045 (or 4.5%) and the adjusted \\(R^2\\) for the parallel slopes model is 0.035 (or 3.5%). This may seem like a small change (of 1%), but if we calculate the percent change (\\(\\frac{0.045 - 0.035}{0.035}\\)), we see that by allowing the slopes to be different, we improved our adjusted \\(R^2\\) by almost 30%! Thus, it could be argued that the additional complexity is warranted.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#step-wise-selection",
    "href": "weeks/week-6.html#step-wise-selection",
    "title": "Model Selection",
    "section": "1.4 Step-wise Selection",
    "text": "1.4 Step-wise Selection\nAnother method for model selection is called “stepwise selection.” This method considers every possible explanatory variable in the dataset and decides if each variable should be included in the model, using a stepwise approach.\n\nData Context\nFor our investigation of this method, we will consider data about loans from the peer-to-peer lender, Lending Club. The loan data includes terms of the loan as well as information about the borrower. The outcome variable we would like to better understand is the interest rate assigned to the loan. For instance, all other characteristics held constant, does it matter how much debt someone already has? Does it matter if their income has been verified? Multiple regression will help us answer these and other questions.\nThe dataset includes results from 10,000 loans, and we’ll be looking at a subset of the available variables. The first six observations in the dataset are shown in Table 7 below, and descriptions for each variable are provided in Table 8. Notice that the past bankruptcy variable (bankruptcy) is an indicator variable, where it takes the value 1 if the borrower had a past bankruptcy in their record and 0 if not. Using an indicator variable in place of a category name allows for these variables to be directly used in regression. Two of the other variables are categorical (verified_income and issue_month), each of which can take one of a few different non-numerical values.\n\nThe loans_full_schema data can be found in the openintro R package. Based on the data in this dataset we have created two new variables: credit_util which is calculated as the total credit utilized divided by the total credit limit and bankruptcy which turns the number of bankruptcies to an indicator variable (0 for no bankruptcies and 1 for at least 1 bankruptcy). We will refer to this modified dataset as loans.\n\n\n\n\n\nTable 7: First six rows of the loans dataset.\n\n\n\n\n\n\ninterest_rate\nverified_income\ndebt_to_income\ncredit_util\nbankruptcy\nterm\ncredit_checks\nissue_month\n\n\n\n\n14.07\nVerified\n18.01\n0.548\n0\n60\n6\nMar-2018\n\n\n12.61\nNot Verified\n5.04\n0.150\n1\n36\n1\nFeb-2018\n\n\n17.09\nSource Verified\n21.15\n0.661\n0\n36\n4\nFeb-2018\n\n\n6.72\nNot Verified\n10.16\n0.197\n0\n36\n0\nJan-2018\n\n\n14.07\nVerified\n57.96\n0.755\n0\n36\n7\nMar-2018\n\n\n6.72\nNot Verified\n6.46\n0.093\n0\n36\n6\nJan-2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Variables and their descriptions for the loans dataset.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ninterest_rate\nInterest rate on the loan, in an annual percentage.\n\n\nverified_income\nCategorical variable describing whether the borrower's income source and amount have been verified, with levels `Verified`, `Source Verified`, and `Not Verified`.\n\n\ndebt_to_income\nDebt-to-income ratio, which is the percentage of total debt of the borrower divided by their total income.\n\n\ncredit_util\nOf all the credit available to the borrower, what fraction are they utilizing. For example, the credit utilization on a credit card would be the card's balance divided by the card's credit limit.\n\n\nbankruptcy\nAn indicator variable for whether the borrower has a past bankruptcy in their record. This variable takes a value of `1` if the answer is *yes* and `0` if the answer is *no*.\n\n\nterm\nThe length of the loan, in months.\n\n\nissue_month\nThe month and year the loan was issued, which for these loans is always during the first quarter of 2018.\n\n\ncredit_checks\nNumber of credit checks in the last 12 months. For example, when filing an application for a credit card, it is common for the company receiving the application to run a credit check.\n\n\n\n\n\n\n\n\n\n\n\n\nStepwise selection\nTwo common strategies for adding or removing variables in a multiple regression model are called backward elimination and forward selection. These techniques are often referred to as stepwise selection strategies, because they add or delete one variable at a time as they “step” through the candidate predictors.\nBackward elimination starts with the full model (the model that includes every potential predictor variable). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\nAn important consideration in implementing either of these stepwise selection strategies is the criterion used to decide whether to eliminate or add a variable. One commonly used decision criterion is adjusted \\(R^2\\). When using adjusted \\(R^2\\) as the decision criterion, we seek to eliminate or add variables depending on whether they lead to the largest improvement in adjusted \\(R^2\\) and we stop when adding or elimination of another variable does not lead to further improvement in adjusted \\(R^2\\).\n\nBackward Selection\nLet’s consider two models, which are shown in Table 9 and Table 10 below. The first table summarizes the full model since it includes all predictors, while the second does not include the issue_month variable.\n\n\n\n\nTable 9: The fit for the full regression model, including the adjusted R-squared.\n\n\n\n\nThe fit for the full regression model, including the adjusted $R^2$.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.89\n0.21\n9.01\n&lt;0.0001\n\n\nverified_incomeSource Verified\n1.00\n0.10\n10.06\n&lt;0.0001\n\n\nverified_incomeVerified\n2.56\n0.12\n21.87\n&lt;0.0001\n\n\ndebt_to_income\n0.02\n0.00\n7.43\n&lt;0.0001\n\n\ncredit_util\n4.90\n0.16\n30.25\n&lt;0.0001\n\n\nbankruptcy1\n0.39\n0.13\n2.96\n0.0031\n\n\nterm\n0.15\n0.00\n38.89\n&lt;0.0001\n\n\ncredit_checks\n0.23\n0.02\n12.52\n&lt;0.0001\n\n\nissue_monthJan-2018\n0.05\n0.11\n0.42\n0.6736\n\n\nissue_monthMar-2018\n-0.04\n0.11\n-0.39\n0.696\n\n\n\n\n\nAdjusted R-sq = 0.2597\n\n\n\n\n\n\ndf = 9964\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10: he fit for the regression model after dropping issue month, including the adjusted R-squared.\n\n\n\n\nThe fit for the regression model after dropping issue month, including the adjusted $R^2$.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.90\n0.20\n9.56\n&lt;0.0001\n\n\nverified_incomeSource Verified\n1.00\n0.10\n10.05\n&lt;0.0001\n\n\nverified_incomeVerified\n2.56\n0.12\n21.86\n&lt;0.0001\n\n\ndebt_to_income\n0.02\n0.00\n7.44\n&lt;0.0001\n\n\ncredit_util\n4.90\n0.16\n30.25\n&lt;0.0001\n\n\nbankruptcy1\n0.39\n0.13\n2.96\n0.0031\n\n\nterm\n0.15\n0.00\n38.89\n&lt;0.0001\n\n\ncredit_checks\n0.23\n0.02\n12.52\n&lt;0.0001\n\n\n\n\n\nAdjusted R-sq = 0.2598\n\n\n\n\n\n\ndf = 9966\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhich of the two models is better?\nWe compare the adjusted \\(R^2\\) of each model to determine which to choose. Since the second model has a higher \\(R^2_{adj}\\) compared to the first model, we prefer the second model to the first.\n\n\nWill the model without issue_month be better than the model with issue_month? We cannot know for sure, but based on the adjusted \\(R^2\\), this is our best assessment.\nResults corresponding to the full model for the loans data are shown in Table 9. How should we proceed under the backward elimination strategy?\nOur baseline adjusted \\(R^2\\) from the full model is 0.2597, and we need to determine whether dropping a predictor will improve the adjusted \\(R^2\\). To check, we fit models that each drop a different predictor, and we record the adjusted \\(R^2\\) for each model:\n\nExcluding verified_income: 0.2238\nExcluding debt_to_income: 0.2557\nExcluding credit_util: 0.1916\nExcluding bankruptcy: 0.2589\nExcluding term: 0.1468\nExcluding credit_checks: 0.2484\nExcluding issue_month: 0.2598\n\nThe model without issue_month has the highest adjusted \\(R^2\\) of 0.2598, higher than the adjusted \\(R^2\\) for the full model. Because eliminating issue_month leads to a model with a higher adjusted \\(R^2\\), we drop issue_month from the model.\nSince we eliminated a predictor from the model in the first step, we see whether we should eliminate any additional predictors. Our baseline adjusted \\(R^2\\) is now \\(R^2_{adj} = 0.2598\\) – the adjusted \\(R^2\\) from the model without issue_month. We now fit new models, which consider eliminating issue_month and each of the remaining predictors:\n\nExcluding issue_month and verified_income: 0.22395\nExcluding issue_month and debt_to_income: 0.25579\nExcluding issue_month and credit_util: 0.19174\nExcluding issue_month and bankruptcy: 0.25898\nExcluding issue_month and term: 0.14692\nExcluding issue_month and credit_checks: 0.24801\n\nNone of these models lead to an improvement in adjusted \\(R^2\\), so we do not eliminate any of the remaining predictors. That is, after backward elimination, we are left with the model that keeps all predictors except issue_month, which we can summarize using the coefficients from Table 10.\n\n\n\n\n\n\n\nForward Selection\nNext, let’s use forward selection to construct a model for predicting interest_rate from the loans data.\nWe start with the model that includes no predictors. Then we fit each of the possible models with just one predictor. Then we examine the \\(R^2\\) for each of these models3:\n\nIncluding verified_income: 0.05926\nIncluding debt_to_income: 0.01946\nIncluding credit_util: 0.06452\nIncluding bankruptcy: 0.00222\nIncluding term: 0.12855\nIncluding credit_checks: -0.0001\nIncluding issue_month: 0.01711\n\nIn this first step, we compare the unadjusted \\(R^2\\) against a baseline model that has no predictors. The no-predictors model always has \\(R^2 = 0\\). The model with one predictor that has the largest \\(R^2\\) is the model with the term predictor, and because this \\(R^2\\) is larger than the \\(R^2\\) from the model with no predictors (\\(R^2 = 0\\)), we will add this variable to our model.\nWe repeat the process again, this time considering the adjusted \\(R^2\\)4 for all 2-predictor models where one of the predictors is term and with a new baseline of \\(R^2 = 0.12855:\\)\n\nIncluding term and verified_income: 0.16851\nIncluding term and debt_to_income: 0.14368\nIncluding term and credit_util: 0.20046\nIncluding term and bankruptcy: 0.13070\nIncluding term and credit_checks: 0.12840\nIncluding term and issue_month: 0.14294\n\nAdding credit_util yields the highest increase in adjusted \\(R^2\\) and has a higher adjusted \\(R^2\\) (0.20046) than the baseline (0.12855). Thus, we will also add credit_util to the model as a predictor.\nSince we have again added a predictor to the model, we again have a new baseline adjusted \\(R^2\\) of 0.20046. We can continue on and see whether it would be beneficial to add a third predictor:\n\nIncluding term, credit_util, and verified_income: 0.24183\nIncluding term, credit_util, and debt_to_income: 0.20810\nIncluding term, credit_util, and bankruptcy: 0.20169\nIncluding term, credit_util, and credit_checks: 0.20031\nIncluding term, credit_util, and issue_month: 0.21629\n\nThe model including verified_income has the largest increase in adjusted \\(R^2\\) (0.24183) from the baseline (0.20046), so we add verified_income to the model as a predictor as well.\nWe continue on in this way, next adding debt_to_income, then credit_checks, and bankruptcy. At this point, we come again to the issue_month variable: adding this as a predictor leads to \\(R_{adj}^2 = 0.25843\\), while keeping all the other predictors but excluding issue_month has a higher \\(R_{adj}^2 = 0.25854\\). This means we do not add issue_month to the model as a predictor. In this example, we have arrived at the same model that we identified from backward elimination.\n\n\n\nA recap\nBackward elimination begins with the model having the largest number of predictors and eliminates variables one-by-one until we are satisfied that all remaining variables are important to the model. Forward selection starts with no variables included in the model, then it adds in variables according to their importance until no other important variables are found. Notice that, for both methods, we have always chosen to retain the model with the largest adjusted \\(R^2\\) value, even if the difference is less than half a percent (e.g., 0.2597 versus 0.2598). One could argue that the difference between these two models is negligible, as they both explain nearly the same amount of variability in the interest_rate. These negligible differences are an important aspect to model selection. It is highly advised that before you begin the model selection process, you decide what a “meaningful” difference in adjusted \\(R^2\\) is for the context of your data. Maybe this difference is 1% or maybe it is 5%. This “threshold” is what you will then use to decide if one model is “better” than another model. Using meaningful thresholds in model selection requires more critical thinking about what the adjusted \\(R^2\\) values mean.\nAdditionally, backward elimination and forward selection sometimes arrive at different final models. This is because the decision for whether to include a given variable or not depends on the other variables that are already in the model. With forward selection, you start with a model that includes no variables and add variables one at a time. In backward elimination, you start with a model that includes all of the variables and remove variables one at a time. How much a given variable changes the percentage of the variability in the outcome that is explained by the model depends on what other variables are in the model. This is especially important if the predictor variables are correlated with each other.\nThere is no “one size fits all” model selection strategy, which is why there are so many different model selection methods. We hope you walk away from this exploration understanding how stepwise selection is carried out and the considerations that should be made when using stepwise selection with regression models.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#other-model-selection-strategies",
    "href": "weeks/week-6.html#other-model-selection-strategies",
    "title": "Model Selection",
    "section": "1.5 Other model selection strategies",
    "text": "1.5 Other model selection strategies\nStepwise selection using adjusted \\(R^2\\) as the decision criteria is one of many commonly used model selection strategies. Stepwise selection can also be carried out with decision criteria other than adjusted \\(R^2\\), such as p-values, which you’ll learn about in later weeks, or AIC (Akaike information criterion) or BIC (Bayesian information criterion), which you might learn about in more advanced statistics courses or your disciplinary courses.\nAlternatively, one could choose to include or exclude variables from a model based on expert opinion or due to research focus. In fact, many statisticians discourage the use of stepwise regression alone for model selection and advocate, instead, for a more thoughtful approach that carefully considers the research focus and features of the data.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#footnotes",
    "href": "weeks/week-6.html#footnotes",
    "title": "Model Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe formula for variance is the square of the standard deviation, or \\(\\text{variance} = \\text{sd}^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}\\)↩︎\nTechnically, the get_regression_points() function does the same thing as the augment() function you learned in the R tutorials last week!↩︎\nRemember, for a basic regression with only one variable, we use (unadjusted) \\(R^2\\)!↩︎\nWe’ve moved to adjusted \\(R^2\\) because we now have more than one predictor variable!↩︎",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week Five: Diving Deeper into Regression (Multiple Linear Regression)",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to build off the concepts we learned last week and delve deeper into linear regression. We are going to explore multiple linear regression, a statistical model where we have multiple explanatory variables and a single numerical response. We are going to refresh how to visualize these types of models, practice fitting these types of models in R, and the learn how to interpret these types of models.",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#learning-outcomes",
    "href": "weeks/week-5.html#learning-outcomes",
    "title": "Week Five: Diving Deeper into Regression (Multiple Linear Regression)",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe to someone what a multiple linear regression is\noutline how a categorical explanatory variable can be included in a simple linear regression\nvisualize multiple linear regression models with one numerical and one categorical explanatory variable\ncalculate the simple linear regression line for each group of a categorical variable\noutline how a second numerical explanatory variable can be included in a simple linear regression\nvisualize multiple linear regression models with two numerical explanatory variables\ninterpret the coefficient of each explanatory variable included in the regression model\nrecite different methods that can be used to decide what multiple linear regression model is “best”\ndescribe the benefits and costs of using “model selection” for deciding on a multiple linear regression model",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#textbook-reading",
    "href": "weeks/week-5.html#textbook-reading",
    "title": "Week Five: Diving Deeper into Regression (Multiple Linear Regression)",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Chapter 6 (https://moderndive.com/6-multiple-regression.html)\n\n\n\nReading Guide – Due Tuesday by noon\nDownload the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#concept-quiz-due-tuesday-by-noon",
    "href": "weeks/week-5.html#concept-quiz-due-tuesday-by-noon",
    "title": "Week Five: Diving Deeper into Regression (Multiple Linear Regression)",
    "section": "1.2 Concept Quiz – Due Tuesday by noon",
    "text": "1.2 Concept Quiz – Due Tuesday by noon\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1 – Based on the visualizations above, I believe the [interaction / parallel slopes] model is more appropriate because the slopes between the groups are [very different / very similar].\nQuestion 2 – What type of model does the following code obtain?\n\nminority_lm &lt;- lm(score ~ age * ethnicity, data = evals)\n\n\ninteraction model\nparallel slopes model\nsimple linear regression model\n\n\nThe following is the output from the above regression model (from Question 2):\n\n\n# A tibble: 4 × 7\n  term                    estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept                  2.61      0.518      5.04   0        1.59     3.63 \n2 age                        0.032     0.011      2.84   0.005    0.01     0.054\n3 ethnicity: not minority    2.00      0.534      3.74   0        0.945    3.04 \n4 age:ethnicitynot minor…   -0.04      0.012     -3.51   0       -0.063   -0.018\n\n\n\nQuestion 3 – The intercept line represents the [evaluation score / mean evaluation score] for [male / female / minority / non-minority] faculty.\n\nQuestion 4 – The age line represents the relationship between age and evaluation scores for [male / female / minority / non-minority] faculty.\n\nQuestion 5 – The ethnicity:not minority line represents the [mean / adjustment to the mean] evaluation score for [male / female / minority / non-minority] faculty.\n\nQuestion 6 – The age:ethnicitynot minority line represents the [slope / adjustment to the slope] for the relationship between age and evaluation scores for [male / female / minority / non-minority] faculty.\n\nQuestion 7 – The value of the age:ethnicitynot minority line (-0.004) [does / does not ] match the decision I made in Question 1 as there is [little difference / substantial difference] in the slopes between the minority and non-minority faculty.\n\nQuestion 8 – What type of model does the following code obtain?\n\nbty_lm &lt;- lm(score ~ age + bty_avg, data = evals)\n\n\nmultiple linear regression with two numerical predictors\ninteraction model\nparallel slopes\n\n\nThe following is the output from the above regression model (from Question 8):\n\n\n# A tibble: 3 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    4.06      0.17      23.9    0        3.72     4.39 \n2 age         -0.003     0.003     -1.15   0.251   -0.008    0.002\n3 bty_avg      0.061     0.017      3.55   0        0.027    0.094\n\n\n\nQuestion 9 – The intercept represents the [course evaluation score / mean course evaluation score] for professors whose age is __ and who have a average beauty score of ___.\n\nQuestion 10 – We interpret the value of -0.003 by age as:\nFor every [1 day / 1 year / 1 evaluation] increase in professor’s [evaluation score / age / average beauty] we expect the [course evaluation score / mean course evaluation score] to [increase / decrease] by __, after accounting for [ethnicity / gender / average beauty scores].",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#r-tutorial-due-thursday-by-noon",
    "href": "weeks/week-5.html#r-tutorial-due-thursday-by-noon",
    "title": "Week Five: Diving Deeper into Regression (Multiple Linear Regression)",
    "section": "1.3 R Tutorial – Due Thursday by noon",
    "text": "1.3 R Tutorial – Due Thursday by noon\n\nRegression modeling: Parallel Slopes\nRegression modeling: Evaluating and extending parallel slopes model",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-0.html",
    "href": "weeks/week-0.html",
    "title": "Course Setup and Structure",
    "section": "",
    "text": "Welcome!\nIn this coursework, you’ll get set up with the Class Discord, learn about what is expected of you each week, and hear some tips from me about how to succeed."
  },
  {
    "objectID": "weeks/week-0.html#join-the-correct-server",
    "href": "weeks/week-0.html#join-the-correct-server",
    "title": "Course Setup and Structure",
    "section": "1.1 Join the correct server",
    "text": "1.1 Join the correct server\nIf you are enrolled in STAT 313 – join the Stat 313 Discord Server (https://discord.gg/abF4wHkQRy)\nIf you are enrolled in STAT 513 – join the Stat 513 Discord Server (https://discord.gg/ZKDtpkBVwr)\nWhen you join the server, you will be given some suggestions to get started.\n\nI recommend you click through these - and in particular, it is probably a good idea to download the desktop version of Discord, and perhaps to install it on your phone if you wish."
  },
  {
    "objectID": "weeks/week-0.html#set-up-your-account",
    "href": "weeks/week-0.html#set-up-your-account",
    "title": "Course Setup and Structure",
    "section": "1.2 Set up your account",
    "text": "1.2 Set up your account\n\nVerify your email\nTo use this Discord server, you must have a verified email.\nNobody (including your professors) will be able to see this email, and it does not have to be your Cal Poly email. This is simply to keep the server from being overrun by temporary accounts.\n\n\nCreate your identity\nThe first thing you should do is decide what name and picture you would like to use.\n\nI would like to strongly encourage you to use your real name and picture, so that everyone can get to know you. However, if you prefer to remain anonymous, you are free to do so.\n\n(Please do not be like Regina and use the name of another student, however!\nThis kind of impersonation will result in a permanent ban from the server.)\n\n\nDecide about privacy and notifications\nThe default settings on the channel are probably just fine for you.\nFeel free to make any changes that work for you, though.\nYou can change your message notifications:\n\nYou can edit your privacy settings, although most things are already private:\n\n\n\nConnect other apps\nYou can connect other apps to Discord, either for productivity or just for fun."
  },
  {
    "objectID": "weeks/week-0.html#using-the-shannels",
    "href": "weeks/week-0.html#using-the-shannels",
    "title": "Course Setup and Structure",
    "section": "1.3 Using the shannels",
    "text": "1.3 Using the shannels\nThe server is made up of many channels. Some are text chatrooms, while some are “Voice Channels” that connect you via audio to everyone else in the channel.\n\nText shannels\nUse the #general channel for anything and everything:\n\nIf your question is about course logistics, rather than the material itself, consider using the #class-logistics channel:\n\nYou can use the specific weekly channels to ask questions about the material…\n\n… or the specific lab assignment.\n\nNotice that you can use tick marks (```), like in Quarto, to make your code appear in a formatted code box.\n\n\nVoice shannels\nTo join a voice channel, simply click it! Make sure you are careful about when you are muted or unmuted.\n\nThe extra “Side Chat” channels are limited to 4 or 8 people, if you would like to start an impromptu study conversation without being heard by me and / or the rest of the class. (I’ll only drop in if you invite us!)\nVoice channels can also be used for people to “Go Live”, and share their screen with everyone else.\n\nWhile this will usually be something professors use to demonstrate code, you can go live, too! But you may need to download the desktop version of Discord to do so.\n\n\nPrivate messages\nIt is also easy to send private messages, to your professor or to each other. These private messages can also easily be used to launch a private video chat and / or screen sharing."
  },
  {
    "objectID": "weeks/week-0.html#see-you-at-the-party",
    "href": "weeks/week-0.html#see-you-at-the-party",
    "title": "Course Setup and Structure",
    "section": "1.4 See you at the Party!",
    "text": "1.4 See you at the Party!"
  },
  {
    "objectID": "weeks/week-0.html#how-your-typical-week-will-look",
    "href": "weeks/week-0.html#how-your-typical-week-will-look",
    "title": "Course Setup and Structure",
    "section": "2.1 How your typical week will look",
    "text": "2.1 How your typical week will look\n\nReadings and videos (every week)\nI favor a “flipped classroom,” as it gives you more time to clarify and solidify statistical concepts through hands-on exercises. Each week, you will read the required chapter(s), completing a required reading guide to walk you through the central concepts for each week.\nDue Tuesday by the start of class.\n\n\nConcept quizzes (every week)\nEach week there will be a short (~10 questions) quiz over the reading and videos from the week. These quizzes are intended to ensure that you grasped the key concepts from the week’s readings. The quizzes are not timed, so you can feel free to check your answers with the textbook and / or videos if you so wish. The quizzes are marked on completion as complete or incomplete.\nDue Tuesday by the start of class.\n\n\nTutorials (every week)\nYou can think of the tutorials as an “interactive” textbook, as they interweave statistical ideas alongside examples of how to work with data in R and hands-on exercises writing the R code necessary to complete a given task. Each exercise has hints available if you get stuck!\nThe tutorials are work at your own pace, so you can complete them all at once or slowly throughout the week. The lab assignments will require for you to put the skills you learned in the tutorials to work, so you are required to complete each week’s tutorial before Thursday’s lab.\nThe tutorials are marked on completion as complete or incomplete. You will submit a screenshot of the completion page at the end of the tutorial to confirm that you completed the tutorial for the week.\nDue Thursday by the start of class.\n\n\nStatistical critiques (every 4-weeks)\nThese assignments are case studies in which you will evaluate a data visualization or statistical analysis, determining how well-performed and presented the analysis was and making recommendations for improving or using the analysis. Critiques are due roughly 1-week after they are assigned and should take 1-2 hours. You will receive feedback and a mark of Success or Growing (elaborated more on later), and you will be able to revise based on that feedback. There will be two total critiques.\n\n\nLab Assignments (every week)\nLabs will be assigned on Thursday every week, providing the opportunity to explore the course concepts in the context of real data. Lab assignments will require for you to work through the tutorial for the week, thus the tutorials are due before the start of class on Thursday.\nYou will complete the lab assignments in the same teams you collaborate with in class. You will access the lab assignment through Posit Cloud, which you will be walked through during the first lab. Your group will be expected to submit your completed lab on Canvas. You will need to submit only the HTML document.\nDue Mondays at 5pm.\n\n\nMidterm & Final Projects (Week 6 & Week 10)\nThere will be two projects throughout the quarter, where you will be asked to synthesize the statistical concepts you have learned in a formal statistical report. Your critiques will help guide you toward how you do / don’t want your report to look. Each project will be done independently, and requires you to submit a project proposal and draft report before the final deadline. You are encouraged to use the feedback received on these assignments to improve your final report. The final reports will be graded as Excellent, Satisfactory, Progressing, or No Credit based on a rubric that will be shared with the initial assignment."
  },
  {
    "objectID": "weeks/week-0.html#week-1-concept-quiz-course-set-up",
    "href": "weeks/week-0.html#week-1-concept-quiz-course-set-up",
    "title": "Course Setup and Structure",
    "section": "2.2 ✅ Week 1 Concept Quiz: Course Set-up",
    "text": "2.2 ✅ Week 1 Concept Quiz: Course Set-up\nQuestion 1: Where are student hours held?\n\nIn person\nDiscord\nZoom by appointment\nCanvas\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the student hours section of the course syllabus!\n\n\nQuestion 2: What materials and technology are required for this course? Select all that apply!\n\nIntroduction to Modern Statistics\nRStudio Cloud – the application for working in R\nCanvas\nDiscord\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the required materials section of the course syllabus!\n\n\nQuestion 3: In this course, Reading Guides are due by ____, Concept Quizzes are due by ____, and Tutorials are due by ____.\nQuestion 4: If you have a question about the course content, what is the best first step?\n\nPost your question on Discord in the appropriate channel\nSend Dr. Theobold an email\nGo to Dr. Theobold’s office hours on Tuesday\nGoogle it\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the getting help section of the course syllabus!\n\n\nQuestion 5: You are permitted to submit up to ___ assignments late.\n\n\n\n\n\n\nTip\n\n\n\nLook at the late work policy section of the course syllabus!\n\n\nQuestion 6: If you need to submit an assignment late, you must do which of the following?\n\nemail Dr. Theobold to request a deadline extension\nsubmit a deadline extension request to the Google Form linked in the syllabus (and on Canvas)\nrequest an extension before the deadline\ncomplete the assignment no later than 3-days after the original deadline\n\nQuestion 7: If you receive a “Growing” on a Lab Assignment or a Critique, what should you do?\n\nread the comments and complete revisions on the problems receiving a “Growing”\nread the comments and complete revisions on the entire assignment\ncomplete revisions on the problems receiving a “Growing”\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the lab section of the course syllabus!\n\n\nQuestion 8: For revisions on Lab Assignments and Critiques to be considered that week they need to be turned in with:\n\nreflections on how your learning progressed from your initial attempt\nyour new answers\nyour original answers\nnothing\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the revisions section of the course syllabus!\n\n\nQuestion 9: You and a friend have been working on Critique 1 together. You finish up and want to go to bed, but they are still a little confused. You email them your file, and say, “Don’t copy this, just look how I did it so you can figure it out.” Have you violated Academic Honesty policies?\nQuestion 10: You have been working on making a visualization for your Midterm Project for what feels like forever and it seems like you are making little to no progress. You find an example analysis on the internet with the visualization you were hoping to make. You copy-and-paste the code used to make the visualization into your Midterm Project and do not reference that you used an outside source. You have violated the Academic Honesty policy.\nQuestion 11: Upload a picture of you introducing yourself in the “Introductions” channel of the STAT 313 (or 513) Discord Server."
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to dive deeper into exploring data, by incorporating categorical variables. Specifically, we will explore how we can incorporate categorical variables into our visualizations through the use of colors and facets. We will also explore how we can obtain summary statistics for different groups of a categorical variable, making our summaries more specific."
  },
  {
    "objectID": "weeks/week-3.html#learning-outcomes",
    "href": "weeks/week-3.html#learning-outcomes",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe what a categorical variable is and how these types of variables are stored in R\ndetermine whether a variable in a dataset is categorical\nincorporate categorical variables into visualizations of two and three variables\nobtain summary statistics for each level of a categorical variable"
  },
  {
    "objectID": "weeks/week-3.html#reading-guide-one-reading-guide-for-both-readings",
    "href": "weeks/week-3.html#reading-guide-one-reading-guide-for-both-readings",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.1 Reading Guide – One Reading Guide for Both Readings!",
    "text": "1.1 Reading Guide – One Reading Guide for Both Readings!\nDownload the Word Document\n\n\n\n\n\n\nAnswers\n\n\n\nPlease include your answers as a different color! You can pick whatever color you like, but please use a color other than black.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#textbook-reading-part-1",
    "href": "weeks/week-3.html#textbook-reading-part-1",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.2 Textbook Reading – Part 1",
    "text": "1.2 Textbook Reading – Part 1\n\n\n\n\n\n\n\nRequired Reading: Incorporating Categorical Variables into Visualizations"
  },
  {
    "objectID": "weeks/week-3.html#textbook-reading-part-2",
    "href": "weeks/week-3.html#textbook-reading-part-2",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.3 Textbook Reading – Part 2",
    "text": "1.3 Textbook Reading – Part 2\n\n\n\n\n\n\n\nRequired Reading: Incorporating Categorical Variables into Summary Statistics"
  },
  {
    "objectID": "weeks/week-3.html#concept-quiz-due-tuesday-by-noon",
    "href": "weeks/week-3.html#concept-quiz-due-tuesday-by-noon",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.4 Concept Quiz – Due Tuesday by noon",
    "text": "1.4 Concept Quiz – Due Tuesday by noon\n1. Suppose you’ve run the following code to make side-by-side boxplots of life expectancy:\nggplot(data = health, mapping = aes(x = country, y = lifeExp)) + \n    geom_boxplot()\nYour resulting plot looks like this:\n\nWhat should you do?\n\nMove country to the y-axis.\nRemove some countries so the names don’t overlap.\nNothing, it looks great!\n\n2. If you didn’t want to stack your boxplots side-by-side, how else could you separate the groups?\n\n\n\n\n\n\nTip\n\n\n\nThis does not go inside the aes() function!\n\n\n3. What functions are necessary to calculate summary statistics for different groups of a categorical variable?\n\nsummarize()\nmutate()\narrange()\ngroup_by()\nfilter()\n\n4. A categorical variable could be correctly stored as what data types?\n\ninteger\ndouble\ncharacter\nfactor\n\n5. In Lab 2, you worked with the nycflights dataset, which contained information on flights departing from NYC airports. If we wanted to know the average departure delay for every airline in the dataset (e.g., Delta, Allegiant, United), what steps would we need to use? Match the code to its corresponding step.\n\n\nFirst Step\nSecond Step\nThird Step\n\n\n\nnycflights\ngroup_by(airline)\nfilter(dest == \"SFO\")\nsummarize(mean_dd = mean(dep_delay))\n\n\n6. What function did you learn that is used for converting a numerical variable to a categorical variable (a process called discretizing)?\n\nmutate()\nfilter()\nif_else()\ngroup_by()\n\n7. The arrange() function sorts or reorder a data frame’s rows according to the values of the specified variable. By default, what ordering does it use for the rows?\n\nascending (smallest to largest)\ndescending (largest to smallest)",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#r-tutorial-due-thursday-by-noon",
    "href": "weeks/week-3.html#r-tutorial-due-thursday-by-noon",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.5 R Tutorial – Due Thursday by noon",
    "text": "1.5 R Tutorial – Due Thursday by noon\nRequired Tutorial: Derive Information with dplyr\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit a screenshot of the completion page for each tutorial to the Canvas assignment portal!",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10: Two-Way ANOVA",
    "section": "",
    "text": "Welcome!\nThis week we explore one final area of ANOVA, a situation where we have multiple explanatory variables. This is called a two-way ANOVA. A two-way ANOVA extends the one-way ANOVA to situations with two categorical explanatory variables. This new methods allows researchers to simultaneously study two variables that might explain variability in the responses and explore whether the impacts of one explanatory variable change depending on the level of the other explanatory variable.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#textbook-reading",
    "href": "weeks/week-10.html#textbook-reading",
    "title": "Week 10: Two-Way ANOVA",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\nAs two-way ANOVA is an extension of one-way ANOVA, this week’s reading is rather short. I’ve written a guided tutorial to walk you through the components of a two-way ANOVA. The tutorial also has some coding exercises embedded in it, so it also counts as your R tutorial for the week.\nClick here to access the textbook reading and R tutorial\n\nReading Guide – Due Tuesday by noon\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit a screenshot of the completion page at the end of the reading.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#concept-quiz-due-tuesday-by-noon",
    "href": "weeks/week-10.html#concept-quiz-due-tuesday-by-noon",
    "title": "Week 10: Two-Way ANOVA",
    "section": "1.2 Concept Quiz – Due Tuesday by noon",
    "text": "1.2 Concept Quiz – Due Tuesday by noon\n1. Match each two-way ANOVA model to its correct description.\n\n\n\nadditive model\n  \ninteraction model\n\nThe relationship between one factor and the response changes depending on the level of another factor.\n\nThe relationship between one factor and the response remains the same across all levels of another factor.\n\nThere is a significant relationship between each factor and the response.\n\n\n\n2. Match each two-way ANOVA model to its multiple linear regression counterpart.\n\n\nadditive model\ninteraction model\n\n\n\ndifferent slopes\nparallel slopes\n\n\n\n3. If there is evidence of an interaction when visualizing a two-way ANOVA model we will see…\n\ndifferences in the centers of the distributions\ndifferences in the spreads of the distributions\nsimilar profiles of the distributions across the facets\ndifferences in the profiles of the distributions across the facets\n\n\n4. Which of the following are conditions of a two-way ANOVA model?\n\nindependence of observations between the groups\nindependence of observations within each group\nindependence of variables\nnormally distributed responses within each group\nnormally distributed residuals\nlinear relationship between the explanatory and response variables\nequal variance of residuals\nequal variance of responses within each group\n\n\n5. Which of the following is important to consider when assessing independence?\n\nIf the same observational unit (e.g., person, penguin, tree) could be sampled multiple times\nIf observations could be biologically related (e.g., parents, siblings, etc.)\nIf observations could be spatially related (e.g., countries that border each other)\nIf there is a relationship between the observations and the response variable\n\n\n6. An [additive / interaction] model is fit using a * symbol between the explanatory variables.\n\n7. When an additive model is fit, the interpretation of the p-value associated with each explanatory variable is [conditional / independent] on the other explanatory variable(s) in the model.\n\n8. If the null hypothesis is rejected in an additive two-way ANOVA model, what model should be fit instead?\n\nseparate one-way ANOVA models\na one-way ANOVA model containing the variable with the smallest p-value (from the additive two-way ANOVA)\na mean only model\nan interaction two-way ANOVA model\na simple linear regression",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#r-tutorial-due-thursday-by-noon",
    "href": "weeks/week-10.html#r-tutorial-due-thursday-by-noon",
    "title": "Week 10: Two-Way ANOVA",
    "section": "1.3 R Tutorial – Due Thursday by noon",
    "text": "1.3 R Tutorial – Due Thursday by noon\nThe tutorial this week is embedded in the reading.\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit a screenshot of the completion page at the end of the reading. Yes, this can be the same screenshot as your reading guide submission.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8 – Hypothesis Tests for Slope Coefficients & Conditions for Inference",
    "section": "",
    "text": "1 Textbook Reading\nRequired Reading: Hypothesis Test for Slope & Inference Conditions\n\n\n\n\n\n\nReading Guide – Due Tuesday by noon\n\n\n\nDownload the Word Document\n\n\n\n\n2 Concept Quiz – Due Tuesday by noon\n1. Which of the following are always true for hypothesis statements?\n\nThey are about sample statistics.\nThey are about relationships between variables.\nThey are about population parameters.\nThey are about differences in groups.\n\n2. Which of the following are true about a null distribution? (select all that apply)\n\nIt is a distribution of statistics.\nThe values on the distribution represent what might have happened if the null hypothesis was true.\nThe values on the distribution represent what might have happened if the alternative hypothesis was true.\nIt is a distribution of samples.\n\n3. Which of the following are true about a p-value? (select all that apply)\n\nIt is calculated assuming the null hypothesis is true.\nIt is the probability the null hypothesis is true.\nIt quantifies how “surprising” our data are.\nIt compares the observed statistic to a distribution of values that could have happened if the null was true.\nIt is calculated assuming the alternative hypothesis is true.\nIt is a probability.\n\n4. Which of the following is true about a small p-value?\n\nThe sample statistic is unlikely to have happened by chance.\nThe sample size was large.\nThe sample statistic is unlikely to have happened if the null hypothesis was true.\nThe sample statistic was large.\n\n5. If you obtain a large p-value, what can you conclude about your hypotheses?\n\nWe cannot say the alternative hypothesis is false.\nWe cannot say the null hypothesis is false.\nThe null hypothesis is true.\nThe alternative hypothesis is true.\n\n6. Match each procedure to the question it addresses.\n\n\nconfidence intervals\nhypothesis tests\n\n\n\n\nWhat are plausible values for the population parameter?\nWhat are plausible values for the sample statistic?\nIs the population parameter different from 0?\nIs the value of the parameter different from a specified quantity?\n\n\n\n7. If the probability of a Type I error goes down, what can you say about the probability of a Type II error?\n\nThe probability of a Type II error goes down.\nThe probability of a Type II error stays the same.\nThe probability of a Type II error goes up.\n\n8. If you obtained a small p-value (e.g., 0.02), what could you say about your confidence interval?\n\nIt would contain the null hypothesized value.\nIt would not contain the null hypothesized value.\nIt would contain the sample statistic.\nIt would contain the true population parameter.\n\n9. In a regression table, what is the “statistic” value associated with the slope?\n\na bootstrap statistic\na z-statistic\nthe sample slope statistic\na t-statistic\n\n10. In a regression table, how is the p-value calculated?\n\nUsing a permutation distribution with 1000 resamples\nUsing a bootstrap distribution with 1000 samples\nUsing a Normal distribution\nUsing a t-distribution\n\n11. What are the required conditions for linear regression? (select all that apply)\n\na random sample was taken\nequal variance of residuals\na large sample was collected\nlinear relationship between x and y\nindependence of observations\nindependence of variables\nnormality of residuals\nnormality of observations\n\n12. Which of the following would violate the condition of independence? (select all that apply)\n\ncollecting a non-random sample\nobservations related geographically (spatially)\nobservations that are related in time (temporally)\nrepeated observations on the same person\nobservations related biologically\n\n\n\n3 R Tutorial – Due Thursday by noon\nRequired Tutorial: Randomization test for the slope\nRequired Tutorial: Evaluating the technical conditions in linear regression",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "activity/week-5-sample-selection.html",
    "href": "activity/week-5-sample-selection.html",
    "title": "Sample Selection & Scope of Inference",
    "section": "",
    "text": "This activity is designed to help you critically evaluate how the data you are using in your Midterm Project were collected. You only need to complete one section of the activity, the section related to your dataset!"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#inclusion-criteria",
    "href": "activity/week-5-sample-selection.html#inclusion-criteria",
    "title": "Sample Selection & Scope of Inference",
    "section": "Inclusion Criteria",
    "text": "Inclusion Criteria\nThis activity will lead you in thinking about inclusion criteria–criteria which needed to be met in order for an observation to be included in a sample. Inclusion criteria determine which members of a population can or cannot be included as a possible observation in a study.\nInclusion criteria are used for experiments and observational studies! For example, a clinical trial for a new treatment for individuals with chronic heart failure might require potential subjects to:\n\nbe between 18 to 80 years of age\nhave been diagnosed with chronic heart failure within at least 6 months\nbe currently taking stable doses of heart failure therapies\nbe willing to return for required follow-up (post-test) visits\n\nAnyone meeting the inclusion criteria would be eligible to participate in the study. This does not mean that everyone meeting these criteria would participate in the study! The researchers would then decide how to sample from these eligible participants to obtain the sample for their study.\n\nAlright, let’s give this a go with the dataset you chose for your Midterm Project!"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#and_vertebrates",
    "href": "activity/week-5-sample-selection.html#and_vertebrates",
    "title": "Sample Selection & Scope of Inference",
    "section": "and_vertebrates",
    "text": "and_vertebrates\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#ntl_icecover",
    "href": "activity/week-5-sample-selection.html#ntl_icecover",
    "title": "Sample Selection & Scope of Inference",
    "section": "ntl_icecover",
    "text": "ntl_icecover\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#hbr_maples",
    "href": "activity/week-5-sample-selection.html#hbr_maples",
    "title": "Sample Selection & Scope of Inference",
    "section": "hbr_maples",
    "text": "hbr_maples\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#pie_crab",
    "href": "activity/week-5-sample-selection.html#pie_crab",
    "title": "Sample Selection & Scope of Inference",
    "section": "pie_crab",
    "text": "pie_crab\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#ncbirths",
    "href": "activity/week-5-sample-selection.html#ncbirths",
    "title": "Sample Selection & Scope of Inference",
    "section": "ncbirths",
    "text": "ncbirths\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#possum",
    "href": "activity/week-5-sample-selection.html#possum",
    "title": "Sample Selection & Scope of Inference",
    "section": "possum",
    "text": "possum\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#evals",
    "href": "activity/week-5-sample-selection.html#evals",
    "title": "Sample Selection & Scope of Inference",
    "section": "evals",
    "text": "evals\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#gapminder",
    "href": "activity/week-5-sample-selection.html#gapminder",
    "title": "Sample Selection & Scope of Inference",
    "section": "gapminder",
    "text": "gapminder\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "course_resources.html",
    "href": "course_resources.html",
    "title": "R Resources",
    "section": "",
    "text": "Tip\n\n\n\nClick on the link related to a specific week to explore R skills learned that week!",
    "crumbs": [
      "R Resources",
      "R Resources"
    ]
  },
  {
    "objectID": "course_resources.html#bonus",
    "href": "course_resources.html#bonus",
    "title": "R Resources",
    "section": "Bonus",
    "text": "Bonus\n\nggplot Customizations\n\n\nPivoted Summary Tables",
    "crumbs": [
      "R Resources",
      "R Resources"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "",
    "text": "A deep dive into linear models for analyzing multivariate data sets. In this course, you will learn techniques for checking the appropriateness of proposed models, such as residual analyses and case influence diagnostics, and techniques for selecting models. Gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course Instructor",
    "section": "",
    "text": "Dr. Allison Theobold (she / they) is an Assistant Professor of Statistics at Cal Poly in beautiful San Luis Obispo, California. Allison received her doctorate in Statistics, with an emphasis in Statistics Education, from Montana State University. Allison’s work focuses on innovation in statistics and data science education, with an emphasis on equitable pedagogy and learning trajectories. Allison is also interested in exploring pedagogical approaches for enhancing retention of under-represented students in STEM, including creating inclusive discoursive spaces and equitable group collaborations.",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "slides/week2-day1.html#warm-up",
    "href": "slides/week2-day1.html#warm-up",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Warm-up",
    "text": "Warm-up\n\n\nWhat are the aesthetics in this plot?\nWhat geometric object is being plotted?"
  },
  {
    "objectID": "slides/week2-day1.html#section",
    "href": "slides/week2-day1.html#section",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "15-minutes\n\n\n\nReview Lab 1 comments\nAsk questions\nStart revisions"
  },
  {
    "objectID": "slides/week2-day1.html#histogram",
    "href": "slides/week2-day1.html#histogram",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Histogram",
    "text": "Histogram\n\n\n\nIs count a variable in the dataset?\nHow did ggplot decide how tall each bar should be?"
  },
  {
    "objectID": "slides/week2-day1.html#section-1",
    "href": "slides/week2-day1.html#section-1",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Univariate (One Variable) Visualizations – For Numerical Data\n\n\n\n\nHistogram (or Dotplot)\nBoxplot\nDensity Plot"
  },
  {
    "objectID": "slides/week2-day1.html#boxplot",
    "href": "slides/week2-day1.html#boxplot",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Boxplot",
    "text": "Boxplot\n\n\n\n\n\n\n\n\n\n\n\nWhat calculations are necessary for creating a boxplot?"
  },
  {
    "objectID": "slides/week2-day1.html#density-plot",
    "href": "slides/week2-day1.html#density-plot",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Density Plot",
    "text": "Density Plot\n\n\n\nA smooth approximation to a variable’s distribution\nPlots density (as a proportion) on the y-axis"
  },
  {
    "objectID": "slides/week2-day1.html#section-3",
    "href": "slides/week2-day1.html#section-3",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Pros\n\nInspect shape of a distribution (skewed or symmetric)\nIdentify modes (most common values)\n\n\nCons\n\nDo not plot raw data, plot summaries (counts) of the data!\nSensitive to the width of the bins (binwidth)"
  },
  {
    "objectID": "slides/week2-day1.html#scatterplots",
    "href": "slides/week2-day1.html#scatterplots",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nWhat are the geometric objects being plotted in a scatterplot?"
  },
  {
    "objectID": "slides/week2-day1.html#section-4",
    "href": "slides/week2-day1.html#section-4",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Recreate my plot!\n\n\nhttps://bit.ly/week-2-code-along"
  },
  {
    "objectID": "slides/week2-day1.html#colors-in-scatterplots-categorical-variable",
    "href": "slides/week2-day1.html#colors-in-scatterplots-categorical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Colors in Scatterplots – Categorical Variable",
    "text": "Colors in Scatterplots – Categorical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#colors-in-scatterplots-categorical-variable-output",
    "href": "slides/week2-day1.html#colors-in-scatterplots-categorical-variable-output",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Colors in Scatterplots – Categorical Variable",
    "text": "Colors in Scatterplots – Categorical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#colors-in-scatterplots-numerical-variable",
    "href": "slides/week2-day1.html#colors-in-scatterplots-numerical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Colors in Scatterplots – Numerical Variable",
    "text": "Colors in Scatterplots – Numerical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#colors-in-scatterplots-numerical-variable-output",
    "href": "slides/week2-day1.html#colors-in-scatterplots-numerical-variable-output",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Colors in Scatterplots – Numerical Variable",
    "text": "Colors in Scatterplots – Numerical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#facets-in-scatterplots-categorical-variable",
    "href": "slides/week2-day1.html#facets-in-scatterplots-categorical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Facets in Scatterplots – Categorical Variable",
    "text": "Facets in Scatterplots – Categorical Variable\n\nggplot(data = penguins,\n       mapping = aes(y = bill_length_mm,\n                     x = bill_depth_mm)) +\n  geom_point() +\n  facet_wrap(~ species) + \n  labs(x = \"Bill Depth (mm)\", \n       y = \"Bill Length (mm)\")"
  },
  {
    "objectID": "slides/week2-day1.html#facets-in-scatterplots-categorical-variable-output",
    "href": "slides/week2-day1.html#facets-in-scatterplots-categorical-variable-output",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Facets in Scatterplots – Categorical Variable",
    "text": "Facets in Scatterplots – Categorical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#facets-in-scatterplots-numerical-variable",
    "href": "slides/week2-day1.html#facets-in-scatterplots-numerical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Facets in Scatterplots – Numerical Variable 🫤",
    "text": "Facets in Scatterplots – Numerical Variable 🫤\n\nggplot(data = penguins,\n       mapping = aes(y = bill_length_mm,\n                     x = bill_depth_mm)) +\n  geom_point() +\n  facet_wrap(~ body_mass_g) + \n  labs(x = \"Bill Depth (mm)\", \n       y = \"Bill Length (mm)\")"
  },
  {
    "objectID": "slides/week2-day1.html#facets-in-scatterplots-numerical-variable-output",
    "href": "slides/week2-day1.html#facets-in-scatterplots-numerical-variable-output",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Facets in Scatterplots – Numerical Variable 🫤",
    "text": "Facets in Scatterplots – Numerical Variable 🫤"
  },
  {
    "objectID": "slides/week2-day1.html#section-5",
    "href": "slides/week2-day1.html#section-5",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are strengths of a boxplot?\n\n\nEasy to flag unusual observations\nEasy to see the median\n\n\n\nWhat are weaknesses of a boxplot?\n\n\nDon’t plot raw data\nOnly plot summary statistics\nHide multiple modes"
  },
  {
    "objectID": "slides/week2-day1.html#section-6",
    "href": "slides/week2-day1.html#section-6",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are strengths of a density plot?\n\n\nInspect shape of a distribution (skewed or symmetric)\nIdentify modes (most common values)\nLess jagged than a histogram\n\n\n\nWhat are weaknesses of a density plot?\n\n\nDo not plot raw data, plot summaries (proportions) of the data!\ny-axis is difficult to interpret\nCan over smooth and hide interesting shapes"
  },
  {
    "objectID": "slides/week2-day1.html#section-7",
    "href": "slides/week2-day1.html#section-7",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are strengths of a density plot?\n\n\nInspect shape of a distribution (skewed or symmetric)\nIdentify modes (most common values)\nLess jagged than a histogram\n\n\n\nWhat are weaknesses of a density plot?\n\n\nDo not plot raw data, plot summaries (proportions) of the data!\ny-axis is difficult to interpret\nCan over smooth and hide interesting shapes"
  },
  {
    "objectID": "slides/week2-day1.html#for-right-skewed-data",
    "href": "slides/week2-day1.html#for-right-skewed-data",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "For right skewed data…",
    "text": "For right skewed data…"
  },
  {
    "objectID": "slides/week2-day1.html#for-symmetric-and-bimodal-data",
    "href": "slides/week2-day1.html#for-symmetric-and-bimodal-data",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "For symmetric (and bimodal) data…",
    "text": "For symmetric (and bimodal) data…"
  },
  {
    "objectID": "slides/week2-day1.html#point-estimates-parameters",
    "href": "slides/week2-day1.html#point-estimates-parameters",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Point Estimates & Parameters",
    "text": "Point Estimates & Parameters\n\nParameter: True value of the statistic for the population of interest\n\n\nPoint Estimate: provides our best guess for the value of the parameter\n\n\n\n\nEstimates based on larger samples tend to be more accurate than those based on smaller samples."
  },
  {
    "objectID": "slides/week2-day1.html#before-thursday",
    "href": "slides/week2-day1.html#before-thursday",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Before Thursday…",
    "text": "Before Thursday…\nComplete the two R tutorials\n\nVisualizing Numerical Variables\nSummarizing Numerical Variables\n\nLinked in the Week 2 coursework!"
  },
  {
    "objectID": "slides/week2-day1.html#meeting-your-team",
    "href": "slides/week2-day1.html#meeting-your-team",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Meeting your team!",
    "text": "Meeting your team!\n\nGo to the STAT 313 Canvas page\nGo to the “People” tab\nClick on the Weeks 2 - 4 groups\nFind your group number\n\n\nThese are the individuals you will be working with for the next three weeks! Exchange contact information!"
  },
  {
    "objectID": "slides/week8-day1.html#section",
    "href": "slides/week8-day1.html#section",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "you…\n\n\n\nunderstand the importance of sampling variability\nknow about using confidence intervals to estimate a range of plausible values for the population parameter\nwant to know how p-values fit in"
  },
  {
    "objectID": "slides/week8-day1.html#section-1",
    "href": "slides/week8-day1.html#section-1",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "What if I want to know if the population parameter differs from a specific value?\n\n\n\n\nHypothesis test!"
  },
  {
    "objectID": "slides/week8-day1.html#section-2",
    "href": "slides/week8-day1.html#section-2",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Goal:\n\n\nAssess how different what we saw in our data is from what could have happened if the null hypothesis was true"
  },
  {
    "objectID": "slides/week8-day1.html#section-3",
    "href": "slides/week8-day1.html#section-3",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Permutation Distribution\n\n\n\nUses the original sample to generate new samples that might have occurred if the null hypothesis was true.\n\n\n\n\nWe can use the statistics from these samples to approximate the true sampling distribution under the null!"
  },
  {
    "objectID": "slides/week8-day1.html#section-4",
    "href": "slides/week8-day1.html#section-4",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "What could have happened if the null was true?\n\n\n\nLike before, we are interested in knowing how a statistic varies from sample to sample.\n\n\n\n\nAgain, knowing a statistic’s behavior helps us make better / more informed decisions!\n\n\n\n\nThis helps us know what statistics are more or less likely to occur if the null hypothesis is true."
  },
  {
    "objectID": "slides/week8-day1.html#section-5",
    "href": "slides/week8-day1.html#section-5",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "p-values\n\n\n\nQuantify how “surprising” what we saw in our data is, if the null hypothesis was true"
  },
  {
    "objectID": "slides/week8-day1.html#section-6",
    "href": "slides/week8-day1.html#section-6",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Permutation Resamples\n\n\nFrom your original sample, separate the \\(x\\) values from the \\(y\\) values.\n\n\nCreate new ordered pairs by randomly pairing \\(x\\) values with \\(y\\) values (permuting the labels).\n\n\n\n\nThis is your permuted resample!"
  },
  {
    "objectID": "slides/week8-day1.html#section-7",
    "href": "slides/week8-day1.html#section-7",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Permuted Statistics\n\nRepeat this process many, many times.\n\nCalculate a numerical summary (e.g., mean, median) for each permuted resample.\n\n\n\n\nThese are your permuted statistics!"
  },
  {
    "objectID": "slides/week8-day1.html#section-8",
    "href": "slides/week8-day1.html#section-8",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Permutation Distribution\n\n\n\ndefinition: a distribution of the permuted statistics from every permuted resample\n\n\n\nDisplays the variability in the statistic that could have happened with repeated sampling, if the null hypothesis was true.\n\n\n\nApproximates the true sampling distribution under the null!"
  },
  {
    "objectID": "slides/week8-day1.html#section-9",
    "href": "slides/week8-day1.html#section-9",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "How do I get my p-value?\n\n\n\nWe compare the observed statistic with the statistics produced assuming the null hypothesis was true.\n\n\n\n\n\nA p-value summarizes the probability of obtaining a sample statistic as or more extreme than what we observed, if the null hypothesis was true."
  },
  {
    "objectID": "slides/week8-day1.html#section-10",
    "href": "slides/week8-day1.html#section-10",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Your turn!\n\n\nWhat is one similarity and one difference between\n\n\n\n\na permutation distribution\n\n\n\n\n\na bootstrap distribution"
  },
  {
    "objectID": "slides/week8-day1.html#section-11",
    "href": "slides/week8-day1.html#section-11",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Exploring the hbr_maples dataset!\n\n\n\n\n\n\n\nstem_length: a number denoting the height of the seedling in millimeters\nstem_dry_mass: a number denoting the dry mass of the stem in grams"
  },
  {
    "objectID": "slides/week8-day1.html#section-12",
    "href": "slides/week8-day1.html#section-12",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "What condition do we need to be worried about?"
  },
  {
    "objectID": "slides/week8-day1.html#section-13",
    "href": "slides/week8-day1.html#section-13",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Modeling the relationship between stem length and stem dry mass\n\n\nlength_mass_lm &lt;- lm(stem_dry_mass ~ stem_length, data = hbr_maples)\n\nget_regression_table(length_mass_lm)\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n-0.043\n0.008\n-5.470\n0\n-0.058\n-0.027\n\n\nstem_length\n0.001\n0.000\n11.425\n0\n0.001\n0.001"
  },
  {
    "objectID": "slides/week8-day1.html#section-14",
    "href": "slides/week8-day1.html#section-14",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "How do I know what is likely to happen if the null is true?\n\n\n\n\nPermuting!"
  },
  {
    "objectID": "slides/week8-day1.html#section-15",
    "href": "slides/week8-day1.html#section-15",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Generating a permuted resample\n\n\nStep 1: specify() your response and explanatory variables\n\n\nStep 2: hypothesize() what would happen under the null\n\n\nStep 3: generate() permuted resamples\n\n\nStep 4: calculate() the statistic of interest"
  },
  {
    "objectID": "slides/week8-day1.html#section-16",
    "href": "slides/week8-day1.html#section-16",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Declare your variables!\n\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length)"
  },
  {
    "objectID": "slides/week8-day1.html#section-17",
    "href": "slides/week8-day1.html#section-17",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "State your hypothesis!\n\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  hypothesize(null = \"independence\")\n\n\n\"independence\" – the assumed relationship between the explanatory and response variables under the null hypothesis\n\n\n\n\n\n\n\n\nIndependence of variables\n\n\nNote! This is different from assuming your observations are independent!"
  },
  {
    "objectID": "slides/week8-day1.html#section-18",
    "href": "slides/week8-day1.html#section-18",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Generate your resamples!\n\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")\n\n\n\n\n\nreps – the number of resamples you want to generate\n\n\n\n\"permute\" – the method that should be used to generate the new samples"
  },
  {
    "objectID": "slides/week8-day1.html#section-19",
    "href": "slides/week8-day1.html#section-19",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "infer will let you know if you missed something!\n\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  generate(reps = 1000, type = \"permute\")\n\n\nError: Permuting should be done only when doing independence hypothesis test. See `hypothesize()`.\nIn addition: Warning message:\nYou have given `type = \"permute\"`, but `type` is expected to be `\"bootstrap\"`.\nThis workflow is untested and the results may not mean what you think they mean."
  },
  {
    "objectID": "slides/week8-day1.html#section-20",
    "href": "slides/week8-day1.html#section-20",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Calculate your statistics!\n\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "slides/week8-day1.html#section-21",
    "href": "slides/week8-day1.html#section-21",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Your turn!\n\n\nWhy is the hypothesize() function used to make a null distribution but not for a bootstrap distribution?\nWhat does the null = \"independence\" input in hypothesize() mean? What is it assuming about the variables declared in the specify() step?"
  },
  {
    "objectID": "slides/week8-day1.html#section-22",
    "href": "slides/week8-day1.html#section-22",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "The final product\n\n\nvisualise(null_dist) + \n  labs(x = \"Permuted Slope Statistic for Linear Relationship between \\nStem Length and Stem Dry Mass\")"
  },
  {
    "objectID": "slides/week8-day1.html#section-23",
    "href": "slides/week8-day1.html#section-23",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Is our observed statistic unlikely under the null?\n\n\n\n\nStep 1: Calculate observed statistic\nStep 2: Find observed statistic on permutation distribution"
  },
  {
    "objectID": "slides/week8-day1.html#section-24",
    "href": "slides/week8-day1.html#section-24",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Finding the observed statistic\n\n\n\nobs_slope &lt;- hbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "slides/week8-day1.html#section-25",
    "href": "slides/week8-day1.html#section-25",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "visualise(null_dist) +\n  shade_p_value(obs_stat = obs_slope, direction = \"two-sided\") +\n  labs(x = \"Permuted Slope Statistic for Linear Relationship between \\nStem Length and Stem Dry Mass\")\n\n\n\n\nIs our observed statistic unlikely to have happened if the null hypothesis was true?"
  },
  {
    "objectID": "slides/week8-day1.html#section-26",
    "href": "slides/week8-day1.html#section-26",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "The p-value is…\n\n\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\") \n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n\n\nWhy did we get a warning?"
  },
  {
    "objectID": "slides/week8-day1.html#section-27",
    "href": "slides/week8-day1.html#section-27",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "How do we interpret a p-value?\n\n\nNeed:\n\nprobability of what we saw in the data\nassuming the null hypothesis is true\n\n\n\n\n“The probability of observing a slope statistic (for the relationship between stem length and stem dry mass) as or more extreme than what was observed is less than 1 in 1000, if there was no relationship between a sugar maple’s stem length and stem dry mass."
  },
  {
    "objectID": "slides/week8-day1.html#section-28",
    "href": "slides/week8-day1.html#section-28",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Classic interpretation mistakes\n\n\n\n“The probability that the null hypothesis is true is about 0%.”\n\n\n\n\n“The probability that the data were produced by random chance alone is about 0%.”"
  },
  {
    "objectID": "slides/week8-day1.html#section-29",
    "href": "slides/week8-day1.html#section-29",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Exit ticket\n\n\nSuppose we had obtained a p-value of 0.6 for the relationship between stem length and stem dry mass.\n\n\n\nHow would you interpret this value?"
  },
  {
    "objectID": "slides/week3-day2.html#section",
    "href": "slides/week3-day2.html#section",
    "title": "Categorical Variables for Whom?",
    "section": "",
    "text": "Data Feminism\n\n\n\n\n\n\n\n\n\nData science by whom?\nData science for whom?\nData sets about whom?\nData science with whose values?"
  },
  {
    "objectID": "slides/week3-day2.html#section-2",
    "href": "slides/week3-day2.html#section-2",
    "title": "Categorical Variables for Whom?",
    "section": "",
    "text": "Rethink binaries\n\n\n\nHow would you redesign the survey question about student’s gender identity?"
  },
  {
    "objectID": "slides/week3-day2.html#section-3",
    "href": "slides/week3-day2.html#section-3",
    "title": "Categorical Variables for Whom?",
    "section": "",
    "text": "Challenge power"
  },
  {
    "objectID": "slides/week3-day2.html#section-4",
    "href": "slides/week3-day2.html#section-4",
    "title": "Categorical Variables for Whom?",
    "section": "",
    "text": "An aura objectivity\n\n\n\n\n\n\n\n\n\n\n\n\n“We focus on four conventions which imbue visualizations with a sense of objectivity, transparency and facticity. These include: (a) two-dimensional viewpoints, (b) clean layouts, (c) geometric shapes and lines, (d) the inclusion of data sources.”\nThe work that visualization communications do"
  },
  {
    "objectID": "slides/week3-day2.html#section-5",
    "href": "slides/week3-day2.html#section-5",
    "title": "Categorical Variables for Whom?",
    "section": "",
    "text": "Elevate emotion\n\nhttps://guns.periscopic.com/"
  },
  {
    "objectID": "slides/week10-day1.html#youve-seen-a-one-way-anova-model-before",
    "href": "slides/week10-day1.html#youve-seen-a-one-way-anova-model-before",
    "title": "One-Way ANOVA Review",
    "section": "You’ve seen a one-way ANOVA model before!",
    "text": "You’ve seen a one-way ANOVA model before!\n\ntrout_lm &lt;- lm(weight_g ~ unittype, data = trout)\n\nget_regression_table(trout_lm)\n\n# A tibble: 7 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       8.07      0.113    71.6     0         7.85    8.29 \n2 unittype: I     1.74      1.98      0.878   0.38     -2.14    5.62 \n3 unittype: IP   -6.68      1        -6.68    0        -8.64   -4.72 \n4 unittype: P     3.79      0.208    18.2     0         3.38    4.20 \n5 unittype: R    -0.991     0.537    -1.85    0.065    -2.04    0.061\n6 unittype: S    -3.73      3.58     -1.04    0.298   -10.8     3.29 \n7 unittype: SC   -3.06      0.266   -11.5     0        -3.58   -2.54 \n\n\n\n\n\nWhat do the estimates from each of these lines (e.g., unittype:IP) represent?"
  },
  {
    "objectID": "slides/week10-day1.html#one-way-anova-testing-if-at-least-one-chanel-type-has-a-different-mean-weight",
    "href": "slides/week10-day1.html#one-way-anova-testing-if-at-least-one-chanel-type-has-a-different-mean-weight",
    "title": "One-Way ANOVA Review",
    "section": "One-Way ANOVA – Testing if at least one chanel type has a different mean weight",
    "text": "One-Way ANOVA – Testing if at least one chanel type has a different mean weight\n\naov(weight_g ~ unittype, data = trout) %&gt;% \n  tidy()\n\n# A tibble: 2 × 6\n  term         df    sumsq meansq statistic    p.value\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 unittype      6   58579. 9763.       109.  5.65e-134\n2 Residuals 11977 1075510.   89.8       NA  NA        \n\n\n\n\n\nWhat would you decide?\n\nWhat conditions need to be checked to know if this p-value is reliable?"
  },
  {
    "objectID": "slides/week10-day1.html#checking-conditions",
    "href": "slides/week10-day1.html#checking-conditions",
    "title": "One-Way ANOVA Review",
    "section": "Checking Conditions",
    "text": "Checking Conditions\n\nHow do you feel about the normality condition?\n\nHow do you feel about the equal variance condition?"
  },
  {
    "objectID": "slides/week10-day1.html#comparing-variances",
    "href": "slides/week10-day1.html#comparing-variances",
    "title": "One-Way ANOVA Review",
    "section": "Comparing Variances",
    "text": "Comparing Variances\n\n\nUn-transformed Variances\n\n\n\n\n\n\n\n\nunittype\nvar\n\n\n\n\nC\n84.826493\n\n\nI\n80.783024\n\n\nIP\n5.729663\n\n\nP\n129.138547\n\n\nR\n59.096925\n\n\nS\n49.280157\n\n\nSC\n49.923399\n\n\nNA\n112.284569\n\n\n\n\n\n\n\n\n\nLog Transformed Variances\n\n\n\n\n\n\n\n\nunittype\nvar\n\n\n\n\nC\n1.6978424\n\n\nI\n0.8788463\n\n\nIP\n0.8990321\n\n\nP\n1.5659500\n\n\nR\n1.3621461\n\n\nS\n2.1770549\n\n\nSC\n1.6514917\n\n\nNA\n0.7591942\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think?"
  },
  {
    "objectID": "slides/week10-day1.html#section",
    "href": "slides/week10-day1.html#section",
    "title": "One-Way ANOVA Review",
    "section": "",
    "text": "Two-way ANOVA\n\n\n\nGoal:\n\nAssess if multiple categorical variables have a relationship with the response."
  },
  {
    "objectID": "slides/week10-day1.html#section-1",
    "href": "slides/week10-day1.html#section-1",
    "title": "One-Way ANOVA Review",
    "section": "",
    "text": "Modeling Options\n\n\n\n\nAdditive Model\n\n\nAssess if each explanatory variable has a meaningful relationship with the response, conditional on the variable(s) included in the model.\n\n\n\nInteraction Model\n\n\nAssess if the relationship between one categorical explanatory variable and the response differs based on the values of another categorical variable."
  },
  {
    "objectID": "slides/week10-day1.html#section-2",
    "href": "slides/week10-day1.html#section-2",
    "title": "One-Way ANOVA Review",
    "section": "",
    "text": "What are we looking for?"
  },
  {
    "objectID": "slides/week10-day1.html#another-way-to-think-about-it",
    "href": "slides/week10-day1.html#another-way-to-think-about-it",
    "title": "One-Way ANOVA Review",
    "section": "Another way to think about it…",
    "text": "Another way to think about it…"
  },
  {
    "objectID": "slides/week10-day1.html#hypothesis-test-steps",
    "href": "slides/week10-day1.html#hypothesis-test-steps",
    "title": "One-Way ANOVA Review",
    "section": "Hypothesis Test Steps",
    "text": "Hypothesis Test Steps\n\n\n\nStep 1: Fit a one-way ANOVA model for each categorical variable\n\n\n\n\n\n\nStep 2: Decide if each explanatory variable has a meaningful relationship with the response variable\n\n\n\n\nIf yes, then go to step 3!\nIf no, then report which variable (if any) has the strongest relationship with the response."
  },
  {
    "objectID": "slides/week10-day1.html#additive-two-way-anova",
    "href": "slides/week10-day1.html#additive-two-way-anova",
    "title": "One-Way ANOVA Review",
    "section": "Additive Two-Way ANOVA",
    "text": "Additive Two-Way ANOVA\n\n\naov(rating ~ era + genre, data = movies) %&gt;% \n  tidy()\n\nWhy did I use a + when fitting the model?\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nera\n3\n4.585917\n1.528639\n0.6801973\n0.5682209\n\n\ngenre\n1\n4.190670\n4.190670\n1.8647194\n0.1780739\n\n\nResiduals\n51\n114.614663\n2.247346\nNA\nNA\n\n\n\n\n\n\n\n\nWhat is the era line testing?\n\nWhat would you decide?"
  },
  {
    "objectID": "slides/week10-day1.html#one-way-anova-research-question",
    "href": "slides/week10-day1.html#one-way-anova-research-question",
    "title": "One-Way ANOVA Review",
    "section": "One-way ANOVA Research Question",
    "text": "One-way ANOVA Research Question\n90-seconds\n \nHow would you phrase a research question for the one-way ANOVA model we’ve been investigating?\nExplanatory Variable: genre\nResponse Variable: rating"
  },
  {
    "objectID": "slides/week10-day1.html#two-way-anova-research-question",
    "href": "slides/week10-day1.html#two-way-anova-research-question",
    "title": "One-Way ANOVA Review",
    "section": "Two-way ANOVA Research Question",
    "text": "Two-way ANOVA Research Question\n90-seconds\n \nHow would you phrase a research question for the two-way ANOVA model we’ve been investigating?\nExplanatory Variables: genre and era\nResponse Variable: rating"
  },
  {
    "objectID": "slides/week10-day1.html#what-about-two-way-anova-interaction-models",
    "href": "slides/week10-day1.html#what-about-two-way-anova-interaction-models",
    "title": "One-Way ANOVA Review",
    "section": "What about two-way ANOVA interaction models?",
    "text": "What about two-way ANOVA interaction models?\n\n\nAn interaction model is similar to a different slopes multiple linear regression\nIt assesses if the relationship between two variables differs when a third variable is considered\nDue to time, we we are not fitting interaction models for the Final Project"
  },
  {
    "objectID": "slides/week10-day2.html#section",
    "href": "slides/week10-day2.html#section",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "Some Closing Thoughts…"
  },
  {
    "objectID": "slides/week10-day2.html#section-1",
    "href": "slides/week10-day2.html#section-1",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "I hope you leave this class understanding…\n\n\nReproducibility is a foundational aspect to scientific research.\nData visualizations tell you a story, where statistical tests only tell you a summary.\nMultiple regression and ANOVA are powerful tools to explore multivariate relationships.\nA well thought out study is more powerful than any statistical analysis."
  },
  {
    "objectID": "slides/week10-day2.html#section-2",
    "href": "slides/week10-day2.html#section-2",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "The Discipline of Statistics\n\n\nThe field of Statistics was developed to evaluate evidence obtained from data. Over the last century, the use of statistics has become embedded as a component of the scientific process for many disciplines."
  },
  {
    "objectID": "slides/week10-day2.html#section-3",
    "href": "slides/week10-day2.html#section-3",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "Foundational ideas taught in statistics courses were invented by:\n\n\nFrancis Galton\nKarl Pearson\nRonald Fisher"
  },
  {
    "objectID": "slides/week10-day2.html#section-4",
    "href": "slides/week10-day2.html#section-4",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "Eugenics & Statistics\n\n\n\nIdeally, statisticians would like to divorce these tools from the lives and times of the people who created them. It would be convenient if statistics existed outside of history, but that’s not the case. Statistics, as a lens through which scientists investigate real-world questions, has always been smudged by the fingerprints of the people holding the lens. Statistical thinking and eugenicist thinking are, in fact, deeply intertwined, and many of the theoretical problems with methods like significance testing—first developed to identify racial differences—are remnants of their original purpose, to support eugenics.\n\n\nHow Eugenics Shaped Statistics"
  },
  {
    "objectID": "slides/week10-day2.html#section-5",
    "href": "slides/week10-day2.html#section-5",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "Statistical Significance – The New “S” Word\n\n\n\n“Significance, the new s-word, is overused and underdefined in the realm of connecting statistical results to the underlying science.”\n\n\n\n\n\n“When used in statistics, significant does not mean important or meaningful, as it does in everyday speech.”\n\n\n\n\n\n\n“I advocate a simple solution: Replace the s-word with words describing what you actually mean by it.”"
  },
  {
    "objectID": "slides/week10-day2.html#section-6",
    "href": "slides/week10-day2.html#section-6",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "Remember to give yourself praise!"
  },
  {
    "objectID": "slides/week4-day1.html#section",
    "href": "slides/week4-day1.html#section",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Relationships Between Variables\n\n\n\nIn a statistical model, we generally have one variable that is the output and one or more variables that are the inputs.\n\n\n\n\n\n\n\n\nResponse variable\n\na.k.a. \\(y\\), dependent\nThe quantity you want to understand\nIn this class – always numerical\n\n\n\n\n\n\n\n\nExplanatory variable\n\na.k.a. \\(x\\), independent, explanatory, predictor\nSomething you think might be related to the response\nEither numerical or categorical"
  },
  {
    "objectID": "slides/week4-day1.html#section-1",
    "href": "slides/week4-day1.html#section-1",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Visualizing Linear Regression\n\n\n\n\n\n\n\nThe scatterplot has been called the most “generally useful invention in the history of statistical graphics.”\n\n\n\n\n\nIt is a simple two-dimensional plot in which the two coordinates of each dot represent the values of two variables measured on a single observation."
  },
  {
    "objectID": "slides/week4-day1.html#section-2",
    "href": "slides/week4-day1.html#section-2",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Characterizing Relationships\n\n\n\n\nForm (e.g. linear, quadratic, non-linear)\nDirection (e.g. positive, negative)\nStrength (how much scatter/noise?)\nUnusual observations (do points not fit the overall pattern?)"
  },
  {
    "objectID": "slides/week4-day1.html#section-3",
    "href": "slides/week4-day1.html#section-3",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Your Turn!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would your characterize this relationship?\n\nshape\ndirection\nstrength\noutliers"
  },
  {
    "objectID": "slides/week4-day1.html#section-4",
    "href": "slides/week4-day1.html#section-4",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "What if you added another variable?"
  },
  {
    "objectID": "slides/week4-day1.html#section-5",
    "href": "slides/week4-day1.html#section-5",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Summarizing a Linear Relationship\n\n\n\n Correlation:\n\n\nstrength and direction of a linear relationship between two quantitative variables\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between -1 and 1\nSign of the correlations shows direction\nMagnitude of the correlation shows strength\n\n\n\n\n\n\nbirths_post26 &lt;- ncbirths %&gt;% \n  drop_na(weight, weeks) %&gt;% \n  filter(weeks &gt; 26)\n\n\nget_correlation(births_post26, \n                weeks ~ weight)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.578"
  },
  {
    "objectID": "slides/week4-day1.html#section-6",
    "href": "slides/week4-day1.html#section-6",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Anscombe Correlations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFour datasets, very different graphical presentations\n\nsame mean and standard deviation in both \\(x\\) and \\(y\\)\nsame correlation\nsame regression line\n\n\n\n\n\n\nFor which of these relationships is correlation a reasonable summary measure?"
  },
  {
    "objectID": "slides/week4-day1.html#section-7",
    "href": "slides/week4-day1.html#section-7",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "The Importance of Language\n\n\n\nThe word “correlation” has both a precise mathematical definition and a more general definition for typical usage in English.\nThese uses are obviously related and generally in sync.\nThere are times when these two uses can be conflated and/or misconstrued."
  },
  {
    "objectID": "slides/week4-day1.html#section-8",
    "href": "slides/week4-day1.html#section-8",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Linear Regression\n\n\n\nModels are ubiquitous in Statistics!\nWe often assume that the value of our response variable is some function of our explanatory variable, plus some random noise.\n\n\n\nIn this case, we assume the relationship between \\(x\\) and \\(y\\) takes the form of a linear function.\n\n\\(response = intercept + slope \\cdot explanatory + noise\\)"
  },
  {
    "objectID": "slides/week4-day1.html#section-9",
    "href": "slides/week4-day1.html#section-9",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Estimated / Fitted Regression Model\n\n\\[\n  \\widehat{y} = b_0 + b_1 \\cdot x\n\\]\n\n\nWhy does this equation have a hat on y?"
  },
  {
    "objectID": "slides/week4-day1.html#section-10",
    "href": "slides/week4-day1.html#section-10",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Coefficient Estimates\n\n\nweeks_lm &lt;- lm(weight ~ weeks, data = births_post26)\n  \nget_regression_table(weeks_lm)\n\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept   -5.34      0.565     -9.45       0   -6.45    -4.23 \n2 weeks        0.325     0.015     22.2        0    0.296    0.354"
  },
  {
    "objectID": "slides/week4-day1.html#section-11",
    "href": "slides/week4-day1.html#section-11",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Our focus (for now…)"
  },
  {
    "objectID": "slides/week4-day1.html#section-12",
    "href": "slides/week4-day1.html#section-12",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Estimated regression equation\n\n\\[\\widehat{y} = b_0 + b_1 \\cdot x\\]\n\n\n\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept   -5.34      0.565     -9.45       0   -6.45    -4.23 \n2 weeks        0.325     0.015     22.2        0    0.296    0.354\n\n\n\n\n\n\nWrite out the estimated regression equation!"
  },
  {
    "objectID": "slides/week4-day1.html#section-13",
    "href": "slides/week4-day1.html#section-13",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "How do you interpret the intercept value of -5.341?\n\n\n\n\nHow do you interpret the slope value of 0.325?"
  },
  {
    "objectID": "slides/week4-day1.html#section-14",
    "href": "slides/week4-day1.html#section-14",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Obtaining Residuals\n\n\n\\(\\widehat{weight} = -5.341+0.325 \\cdot weeks\\)\n\n\n\nWhat would the residual be for a pregnancy that lasted 39 weeks and whose baby weighed 7.63 pounds?"
  },
  {
    "objectID": "slides/week4-day1.html#section-15",
    "href": "slides/week4-day1.html#section-15",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "A different explanatory variable\n\n\nweight_gain_lm &lt;- lm(weight ~ gained, data = births_post26)\n  \nget_regression_table(weight_gain_lm)\n\n# A tibble: 2 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    6.74      0.102     65.8        0    6.54     6.94 \n2 gained       0.015     0.003      4.79       0    0.009    0.021\n\n\n\n\nWrite out this estimated regression equation!"
  },
  {
    "objectID": "slides/week4-day1.html#section-16",
    "href": "slides/week4-day1.html#section-16",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "How would you choose which model is better?!\n\n\n\nbirths_post26 %&gt;% \n  drop_na(weight, gained, weeks) %&gt;% \n  summarize(cor_weeks = cor(weight, weeks), \n            R_sq_weeks = cor_weeks^2, \n            cor_gained = cor(weight, gained),\n            R_sq_gained = cor_gained^2)\n\n# A tibble: 1 × 4\n  cor_weeks R_sq_weeks cor_gained R_sq_gained\n      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     0.572      0.327      0.153      0.0234"
  },
  {
    "objectID": "slides/week4-day1.html#section-17",
    "href": "slides/week4-day1.html#section-17",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Categorical Explanatory Variables\n\n\n\nFinding distinct levels:\n\n\ndistinct(births_post26, habit)\n\n# A tibble: 2 × 1\n  habit    \n  &lt;fct&gt;    \n1 nonsmoker\n2 smoker"
  },
  {
    "objectID": "slides/week4-day1.html#section-18",
    "href": "slides/week4-day1.html#section-18",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Indicator Variables\n\n\n\\[\n  \\hat{y} = b_0 + b_1 \\cdot x\n\\]\n\n\n\n\n\\(x\\) is a categorical variable with levels:\n\n\"nonsmoker\"\n\"smoker\"\n\n\n\n\nNeed:\n\n“baseline” mean\n“offsets”\n\n\n\n\n\n\n\\(1_{smoker}(x) = 1\\) if the mother was a \"smoker\"\n\\(1_{smoker}(x) = 0\\) if the mother was a \"nonsmoker\""
  },
  {
    "objectID": "slides/week4-day1.html#section-19",
    "href": "slides/week4-day1.html#section-19",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "A different equation\n\n\nhabit_lm &lt;- lm(weight ~ habit, data = births_post26)\n  \nget_regression_table(habit_lm)\n\n# A tibble: 2 × 7\n  term          estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         7.23     0.047    155.     0        7.14     7.32 \n2 habit: smoker    -0.4      0.13      -3.07   0.002   -0.656   -0.145\n\n\n\n\nWhat is the estimated mean birth weight for nonsmoking mothers?"
  },
  {
    "objectID": "slides/week5-day2.html#section",
    "href": "slides/week5-day2.html#section",
    "title": "Work Day: Coding a Multiple Linear Regression",
    "section": "",
    "text": "Step 1\n\n\n\nTwo Numerical Variables\n\nVisualize the model with both variables – color gradient\nVisualize two simple linear regression models – one for each variable\n\n\n\n\nOne Categorical & One Numerical Variable\n\nVisualize the model using geom_smooth(method = \"lm\")\nVisualize the model using geom_parallel_lines()"
  },
  {
    "objectID": "slides/week5-day2.html#section-1",
    "href": "slides/week5-day2.html#section-1",
    "title": "Work Day: Coding a Multiple Linear Regression",
    "section": "",
    "text": "Step 2\n\n\n\nTwo Numerical Variables\n\n\nIf there appears to be a relationship with the colors – include both variables!\nIf the colors are equally dispersed throughout the plot – choose the one variable that has the stronger relationship!\n\n\n\n\n\nOne Categorical & One Numerical Variable\n\n\nCompare the plots! Which looks like a better representation of the relationship?"
  },
  {
    "objectID": "slides/week5-day2.html#section-2",
    "href": "slides/week5-day2.html#section-2",
    "title": "Work Day: Coding a Multiple Linear Regression",
    "section": "",
    "text": "Step 3 – Fit the regression model with lm()\n\n\n\nTwo Numerical Variables\n\n\nAre both variables included? Use a + to separate them!\n\n\n\n\n\nOne Categorical & One Numerical Variable\n\n\nAre the slopes different? Use a * to separate the variables!\nAre the slopes similar? Use a + to separate the variables!"
  },
  {
    "objectID": "slides/week5-day2.html#section-3",
    "href": "slides/week5-day2.html#section-3",
    "title": "Work Day: Coding a Multiple Linear Regression",
    "section": "",
    "text": "Step 4: Get the coefficients with get_regression_table()"
  },
  {
    "objectID": "slides/week3-day1-alt.html#in-r",
    "href": "slides/week3-day1-alt.html#in-r",
    "title": "Incorporating Categorical Variables",
    "section": "In R…",
    "text": "In R…\ncategorical variables can have either character or factor data types\n\n\nfactor – structured & fixed number of levels / options\n\ncan be ordered or unordered\n\n\n\n\ncharacter – unstructured & variable number of levels\n\nis inherently unordered"
  },
  {
    "objectID": "slides/week3-day1-alt.html#incorporating-categorical-variables-into-data-visualizations",
    "href": "slides/week3-day1-alt.html#incorporating-categorical-variables-into-data-visualizations",
    "title": "Incorporating Categorical Variables",
    "section": "Incorporating Categorical Variables into Data Visualizations",
    "text": "Incorporating Categorical Variables into Data Visualizations\n\n\nAs a variable on the x- or y-axis\nAs a color / fill\nAs a facet"
  },
  {
    "objectID": "slides/week3-day1-alt.html#salamander-size",
    "href": "slides/week3-day1-alt.html#salamander-size",
    "title": "Incorporating Categorical Variables",
    "section": "Salamander Size",
    "text": "Salamander Size\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_2_mm)) + \n  geom_histogram(binwidth = 14) + \n  labs(x = \"Snout to Tail Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would this histogram look if there was no variation in salamander length?\n\nWhat are possible causes for the variation in salamander length?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#faceted-histograms",
    "href": "slides/week3-day1-alt.html#faceted-histograms",
    "title": "Incorporating Categorical Variables",
    "section": "Faceted Histograms",
    "text": "Faceted Histograms\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm)) + \n  geom_histogram(binwidth = 14) + \n  facet_wrap(~ section, scales = \"free\") +\n  labs(x = \"Snout to Tail Length (mm)\")\n\n\nWhat do you think scales = \"free\" does?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#side-by-side-boxplots",
    "href": "slides/week3-day1-alt.html#side-by-side-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                     y = species)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\") \n\n\n\n\n\n\n\n\n\n\nggplot(data = salamander, \n       mapping = aes(y = length_1_mm, \n                     x = species)\n         ) + \n  geom_boxplot() + \n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Salamander Species\")\n\n\n\n\n\n\n\n\n\n\nWhich orientation do you prefer? Vertical or horizontal?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#colors-in-boxplots",
    "href": "slides/week3-day1-alt.html#colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Colors in Boxplots",
    "text": "Colors in Boxplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = unittype)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\", \n       color = \"Channel Type\")\n\n\n\nWhy are there only two boxplots for the Olympic torrent salamander?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-colors-in-boxplots",
    "href": "slides/week3-day1-alt.html#facets-colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = section)\n         ) + \n  geom_boxplot() + \n  facet_wrap(~ unittype) + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\", \n       color = \"Section in Mack Creek\")"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-colors-in-boxplots-output",
    "href": "slides/week3-day1-alt.html#facets-colors-in-boxplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-color-in-scatterplots",
    "href": "slides/week3-day1-alt.html#facets-color-in-scatterplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = weight_g, \n                       color = section)\n         ) + \n  geom_point() + \n  facet_wrap(~species, scales = \"free\") +\n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Year\", \n       color = \"Salamander Species\")"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-color-in-scatterplots-output",
    "href": "slides/week3-day1-alt.html#facets-color-in-scatterplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots"
  },
  {
    "objectID": "slides/week3-day1-alt.html#your-turn",
    "href": "slides/week3-day1-alt.html#your-turn",
    "title": "Incorporating Categorical Variables",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhat are the aesthetics included in this plot?"
  },
  {
    "objectID": "slides/week1-day1.html#section",
    "href": "slides/week1-day1.html#section",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "About Me…"
  },
  {
    "objectID": "slides/week1-day1.html#section-1",
    "href": "slides/week1-day1.html#section-1",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What is Statistics?\n\n\n\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data.\n\n\nStatistics is the study of how best to collect, analyze, and draw conclusions from data.\n\n\nIntroduction to Modern Statistics"
  },
  {
    "objectID": "slides/week1-day1.html#section-2",
    "href": "slides/week1-day1.html#section-2",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What Statistics Is To Me\n\n \n\n\n\n\nThe data science cycle – Wickham & Grolemund"
  },
  {
    "objectID": "slides/week1-day1.html#section-3",
    "href": "slides/week1-day1.html#section-3",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What you can expect in STAT 313\n\nThis course will teach you the fundamentals of linear models—simple linear regression, multiple linear regression, and analysis of variance—and experimental design. You will extend the concepts covered in your Stat I course, to:\n\n\nwork with data in a reproducible way (using R)\nvisualize and summarize a variety of datasets (in R)\ncritically evaluate the use of Statistics\nperform statistical analyses to answer research questions (using R)"
  },
  {
    "objectID": "slides/week1-day1.html#section-4",
    "href": "slides/week1-day1.html#section-4",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Coding 🙀\n\n\nCoding is a huge part of how doing statistics in the wild looks.\n\n\n\nEveryone is coming from a different background\nDifferent aspects of the course will be difficult to different people\nYou will be given coding resources each week\nUse your peers to support your learning"
  },
  {
    "objectID": "slides/week1-day1.html#section-5",
    "href": "slides/week1-day1.html#section-5",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Course Components\n\n\n\n\nBefore Class\n\n\nReading Guides\nConcept Quizzes\nR Tutorials\n\n\n\n\n\nDuring Class\n\n\nGroup Discussion\nHands-on Activities\nLab Assignments\n\n\n\n\n\nOutside of Class\n\n\nStatistical Critiques\nMidterm Project\nFinal Project"
  },
  {
    "objectID": "slides/week1-day1.html#section-6",
    "href": "slides/week1-day1.html#section-6",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Specifications Based Grading\n\n\n\n\nEveryone is capable of earning an A!"
  },
  {
    "objectID": "slides/week1-day1.html#section-7",
    "href": "slides/week1-day1.html#section-7",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "How Smart are You?\n\n\n\n(2 minutes)\n\nWrite two criteria would you use to rank yourself compared to everyone else in this class\n\n\n\n\n(3 minutes)\n\nTalk with the person on your right about the criteria you proposed\n\n\n\n\n(5 minutes)\n\nShare out\nPerson with most vowels in name should be prepared to share!"
  },
  {
    "objectID": "slides/week1-day1.html#section-8",
    "href": "slides/week1-day1.html#section-8",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Cooperative Learning\n\nis a structured form of small-group learning\n\n\n\n\nRoles relate to how the work should be done\n\nRoles are not about breaking up the work intellectually\n\nRoles allow each person to contribute to the group in significant ways\n\nEach person’s participation is necessary to complete the task"
  },
  {
    "objectID": "slides/week1-day1.html#section-9",
    "href": "slides/week1-day1.html#section-9",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Group Norms\n\n\n\n\nZero tolerance for: racism, sexism, homophobia, transphobia, ageism, ableism\nRespect one another\nIntent and impact both matter\n\n\n\n\n\nNon-judgmental\nTake space, make space\nEmbrace discomfort\nMake decisions by consensus"
  },
  {
    "objectID": "slides/week1-day1.html#section-10",
    "href": "slides/week1-day1.html#section-10",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "How We Learn Together\n\n\n\n\n\nNo one is done until everyone is done\nYou have the right to ask anyone in your group for help\nYou have the duty to help anyone in your group who asks for help\n\n\n\n\n\n\n\nHelping someone means explaining your thinking not giving answers or doing the work for them\nProvide a justification when you make a statement\nThink and work together – don’t divide up the work"
  },
  {
    "objectID": "slides/week1-day1.html#section-11",
    "href": "slides/week1-day1.html#section-11",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Break"
  },
  {
    "objectID": "slides/data-viz-principles.html#section",
    "href": "slides/data-viz-principles.html#section",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "theme_classic() in ggplot()"
  },
  {
    "objectID": "slides/data-viz-principles.html#section-1",
    "href": "slides/data-viz-principles.html#section-1",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "Gestalt Principles\n\n\n\nGestalt psychology is a theory of perception that believes humans are inclined to understand objects as an entire structure rather than the sum of its parts.\n\n\n\nGestalt Principles for Data Visualization\n\n\nThe Figure and Ground Principle describes the capacity to perceive the relationship between form and surrounding space to create meaning. A sense of wholeness or unity depends on how you perceive the relationship between an object and the area in which it is contained. The ‘figure’ is the focus element, while the ‘ground’ is the figure’s background."
  },
  {
    "objectID": "slides/data-viz-principles.html#section-2",
    "href": "slides/data-viz-principles.html#section-2",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "theme_bw() in ggplot()\n\n\n\nList of all ggplot() themes"
  },
  {
    "objectID": "slides/week6-day2.html#section",
    "href": "slides/week6-day2.html#section",
    "title": "Machine Learning",
    "section": "",
    "text": "Data is Power"
  },
  {
    "objectID": "slides/week6-day2.html#section-1",
    "href": "slides/week6-day2.html#section-1",
    "title": "Machine Learning",
    "section": "",
    "text": "Intersectional Feminism\n\n\nFocuses on the distribution of power — who has it and who doesn’t\n\n\n\n\n\n\n\n\n\nIntersectionality is a concept out of Black feminism, conceptually derived by the Combahee River Collective and linguistically created by Kimberlee Crenshaw."
  },
  {
    "objectID": "slides/week6-day2.html#section-2",
    "href": "slides/week6-day2.html#section-2",
    "title": "Machine Learning",
    "section": "",
    "text": "“Data is the new oil.”\n\n\nThe Economist, Intel CEO, Reliance Industrices CEO, UAE Minister of Artifical Intelligence, Google execs, etc., mainly elite white men\n\n\n\n“Data is the same old oppression.”\n\n\nBIWOC, white women, Indigenous people, immigrant communities, LGBTQ+ individuals, + more"
  },
  {
    "objectID": "slides/week6-day2.html#section-3",
    "href": "slides/week6-day2.html#section-3",
    "title": "Machine Learning",
    "section": "",
    "text": "Joy Buolamwini found that she had to put on a white mask for the facial detection program to “see” her face."
  },
  {
    "objectID": "slides/week6-day2.html#proposition-25",
    "href": "slides/week6-day2.html#proposition-25",
    "title": "Machine Learning",
    "section": "Proposition 25",
    "text": "Proposition 25"
  },
  {
    "objectID": "slides/week6-day1.html#section",
    "href": "slides/week6-day1.html#section",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Lots of available predictor variables\n\n\nevals:\n\n\n\n\n\nID\nprof_ID\nscore\nage\nbty_avg\ngender\nethnicity\nlanguage\nrank\npic_outfit\npic_color\ncls_did_eval\ncls_students\ncls_level\n\n\n\n\n241\n46\n3.7\n58\n4.667\nfemale\nnot minority\nenglish\ntenured\nformal\nblack&white\n26\n34\nupper\n\n\n413\n83\n4.0\n47\n6.667\nfemale\nnot minority\nenglish\nteaching\nnot formal\nblack&white\n15\n17\nlower\n\n\n295\n56\n4.7\n32\n3.833\nmale\nnot minority\nenglish\ntenure track\nformal\nblack&white\n72\n103\nupper"
  },
  {
    "objectID": "slides/week6-day1.html#section-1",
    "href": "slides/week6-day1.html#section-1",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Interested in prediction not explanation\n\n\n\n\nYou want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). You don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\).\nModernDive"
  },
  {
    "objectID": "slides/week6-day1.html#section-2",
    "href": "slides/week6-day1.html#section-2",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "How do you use model selection?\n\n\n\n\n\nStepwise Selection\n\nForward Selection\nBackward Selection\n\n\n\n\n\n\nResampling Methods\n\nCross Validation\nTesting / Training Datasets\n\n\n\n\n\n\n\n\nWith any of these methods, you get to choose how you decide if one model is better than another model."
  },
  {
    "objectID": "slides/week6-day1.html#section-3",
    "href": "slides/week6-day1.html#section-3",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "\\(R^2\\) – Coefficient of Determination\n\n\n\n\n\n\nWright, Sewall. 1921. Correlation and causation. Journal of Agricultural Research 20: 557-585.\n\n\n\n\n\n\nIn statistics, the coefficient of determination, denoted \\(R^2\\) or \\(r^2\\) and pronounced “R squared,” is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-4",
    "href": "slides/week6-day1.html#section-4",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "\\(R^2 = 1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\)\n\n\n\\(\\text{var}(\\text{residuals})\\) is the variance of the residuals “leftover” from the regression model\n\\(\\text{var}(y)\\) is the inherent variability of the response variable\n\n\n\nSuppose we have a simple linear regression with an \\(R^2\\) of 0.85. How would you interpret this quantity?"
  },
  {
    "objectID": "slides/week6-day1.html#section-5",
    "href": "slides/week6-day1.html#section-5",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "But! \\(R^2\\) always increases as you increase the number of explanatory variables.\n\n\n\n\nThe variance of the residuals will always decrease when you include additional explanatory variables.\n\n\n\nSimple Linear Regression\n\\(0.85 = 1 - \\frac{0.75}{5}\\)\n\n\n\nOne Additional Variable\n\\(0.86 = 1 - \\frac{0.7}{5}\\)"
  },
  {
    "objectID": "slides/week6-day1.html#section-6",
    "href": "slides/week6-day1.html#section-6",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Adjusted \\(R^2\\)\n\n\n\n\n\n\n\n\n\nThe use of an adjusted \\(R^2\\) is an attempt to account for the phenomenon of the \\(R^2\\) automatically increasing when extra explanatory variables are added to the model.\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-7",
    "href": "slides/week6-day1.html#section-7",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "\\(R^2_{adj} = 1 - \\frac{\\text{var(residuals)}}{\\text{var}(y)} \\times \\frac{(n - 1)}{(n - k - 1)}\\)\n\n\n\\(n\\) is the sample size\n\\(k\\) is the number of coefficients needed to be calculated\n\n\n\nSuppose you have a two categorical variables included in your regression, one with 7 levels and one with 4 levels.\n\nWhat value will you use for \\(k\\) in the calculation of \\(n - k - 1\\)?"
  },
  {
    "objectID": "slides/week6-day1.html#section-8",
    "href": "slides/week6-day1.html#section-8",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "p-values\n\n\n\n\n\n\n\n\n\nIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis.\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-9",
    "href": "slides/week6-day1.html#section-9",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "AIC – An Information Criterion\n\n\n\n\n\n\n\n\n\nThe Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models.\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-10",
    "href": "slides/week6-day1.html#section-10",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "How do you use AIC to choose a “best” model?\n\n\n\n\n\n\n\n\n\nmodel\nAIC\nDelta AIC\n\n\n\n\nFull Model\n4724.970\n0.000000\n\n\nAll Variables Except Year\n4727.242\n2.272501\n\n\nAll Variables Except Flipper Length\n4757.214\n32.244605\n\n\nAll Variables Except Species\n4793.681\n68.710933\n\n\n\n\n\n\n\n\n\n\n\n\nIf you’ve ever assessed whether \\(\\Delta\\) AIC \\(&gt; 2\\) you have done something that is mathematically close to \\(p &gt; 0.05\\)."
  },
  {
    "objectID": "slides/week6-day1.html#section-11",
    "href": "slides/week6-day1.html#section-11",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Model Selection Activity!"
  },
  {
    "objectID": "slides/week6-day1.html#section-12",
    "href": "slides/week6-day1.html#section-12",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "What should you use instead?\n\n\nIn fact, many statisticians discourage the use of stepwise regression alone for model selection and advocate, instead, for a more thoughtful approach that carefully considers the research focus and features of the data.\nIntroduction to Modern Statistics"
  },
  {
    "objectID": "project/midterm-proposal.html",
    "href": "project/midterm-proposal.html",
    "title": "Midterm Project Proposal",
    "section": "",
    "text": "This week you will get started on your midterm project by selecting what dataset you wish to analyze and writing an introduction about the dataset you chose."
  },
  {
    "objectID": "project/midterm-proposal.html#option-1-use-your-own-data",
    "href": "project/midterm-proposal.html#option-1-use-your-own-data",
    "title": "Midterm Project Proposal",
    "section": "1.1 Option 1 – Use your own data",
    "text": "1.1 Option 1 – Use your own data\nIf you have a dataset from another course or your own research which you believe can be appropriately modeled with a linear regression, you can propose to use these data.\n\nDeliverable\nFor the Midterm Project Proposal assignment on Canvas, you are required to state at the beginning of the document, that you have chosen to use your own dataset. You are also required to submit your dataset as either an Excel file (.csv or .xlsx)."
  },
  {
    "objectID": "project/midterm-proposal.html#option-2-use-a-public-dataset",
    "href": "project/midterm-proposal.html#option-2-use-a-public-dataset",
    "title": "Midterm Project Proposal",
    "section": "1.2 Option 2 – Use a public dataset",
    "text": "1.2 Option 2 – Use a public dataset\nI’ve compiled a list of datasets from a variety of contexts, all which are relatively tidy and ready for analysis.\nFrom the lterdatasampler package:\n\nand_vertebrates: Size data for Cutthroat trout and salamanders in different sections of forest (from Lab 3).\nntl_icecover: Data on duration of ice cover of lakes in the Madison, WI area (from Lab 4).\nhbr_maples: Data on the growth of Sugar Maple (Acer saccharum) seedlings in response to calcium addition.\npie_crab: Data on Fiddler crab body size in salt marshes from Florida to Massachusetts.\n\nFrom the openintro package:\n\nncbirths: Data from North Carolina births in 2004 (from Week 4).\npossum: Data representing possums in Australia and New Guinea.\n\nFrom the moderndive package:\n\nevals: Data from end of semester student evaluations from University of Texas at Austin (discussed in ModernDive textbook).\n\nFrom the gapminder package:\n\ngapminder: Data on life expectancy, GDP per capita, and population by country (discussed in ModernDive textbook).\n\n\nDeliverable\nFor the Midterm Project Proposal assignment on Canvas, you are required to state at the beginning of the document, the name of the dataset you have chosen to use."
  },
  {
    "objectID": "project/midterm-proposal.html#if-you-chose-to-use-your-own-data",
    "href": "project/midterm-proposal.html#if-you-chose-to-use-your-own-data",
    "title": "Midterm Project Proposal",
    "section": "2.1 If you chose to use your own data",
    "text": "2.1 If you chose to use your own data\nStep 1: Describe the context of your dataset in your own words! How were the data collected? Was there a study these data came from?\nI don’t know anything about your data, so I expect for this description to be extensive. If the data you are using are from a class, you are responsible for obtaining the context of the data–you cannot simply say “These were data used in BIO 253.”\nStep 2: Choose your variables\nDescribe at least three variables you are interested in. Choose one variable for the response and at least two explanatory variables."
  },
  {
    "objectID": "project/midterm-proposal.html#if-you-chose-a-dataset-from-the-list-above",
    "href": "project/midterm-proposal.html#if-you-chose-a-dataset-from-the-list-above",
    "title": "Midterm Project Proposal",
    "section": "2.2 If you chose a dataset from the list above",
    "text": "2.2 If you chose a dataset from the list above\nStep 1: Describe the context of your dataset in your own words! How were the data collected? Was there a study these data came from?\n\n\n\n\n\n\nTip\n\n\n\nTo obtain information on the dataset, click on the link provided in its name!\n\n\nStep 2: Choose your variables\nDescribe at least three variables you are interested in. Choose one variable for the response and at least two explanatory variables."
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "",
    "text": "We will be using a “forward” selection process for deciding on the “best” ANOVA model. Meaning, we will start with a simple model and keep adding complexity until it seems like the complexity isn’t “worth it.”"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#plot-the-model",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#plot-the-model",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Plot the model",
    "text": "Plot the model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are we looking for?\nWe are looking to see if the mean of the response variable (IMDB rating) differs between the explanatory variable groups (genre or era). This is akin to looking for evidence that the slope of the regression line is not 0!\nFor these data we are looking to see if at least one of the genres has a different mean IMDB rating or if at least one of the eras has a different mean movie rating."
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#fit-the-models",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#fit-the-models",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Fit the model(s)",
    "text": "Fit the model(s)\n\naov(rating ~ era, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.669   0.575\n2 Residuals    52 119.     2.28    NA      NA    \n\naov(rating ~ genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 genre         1   4.68   4.68      2.13   0.150\n2 Residuals    54 119.     2.20     NA     NA"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#perform-a-hypothesis-test",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#perform-a-hypothesis-test",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nThe era line of the ANOVA table is testing if the mean movie rating is different for at least one era. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every era\n\\(H_A\\): The mean movie rating is different for at least one era\nWith a p-value of 0.575 (from an F-statistic of 0.669 with 3 and 52 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating differs for at least one era.\nThe genre line of the ANOVA table is testing if the mean movie rating is different for at least genre. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every genre\n\\(H_A\\): The mean movie rating is different for at least one genre\nWith a p-value of 0.015 (from an F-statistic of 2.127 with 1 and 54 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating differs for at least one genre.\n\n\n\n\n\n\nCategorical variables with two levels\n\n\n\nTechnically, if your categorical variable has only two levels, your alternative hypothesis is testing if there is a difference between the levels. But! This is not the case if your variable has more than two levels!"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#plot-the-model-1",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#plot-the-model-1",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Plot the model",
    "text": "Plot the model"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#fit-the-model",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#fit-the-model",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Fit the Model",
    "text": "Fit the Model\n\n\n\n\n\n\nTip\n\n\n\nNotice I’m using a + for an interaction model!\n\n\n\naov(rating ~ era + genre, data = movies) %&gt;%\n  tidy()\n\n# A tibble: 3 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.680   0.568\n2 genre         1   4.19   4.19     1.86    0.178\n3 Residuals    51 115.     2.25    NA      NA"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#perform-a-hypothesis-test-1",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#perform-a-hypothesis-test-1",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nThe era and genre lines of the two-way ANOVA table perform similar tests to the separate one-way ANOVA models. However, by including both variables in the same model, each of these tests are now conditional on the other variable(s) in the model.\nThe era line is testing if the mean movie rating is different for at least one era, conditional on the genre of the movie. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every era, given the genre of the movie\n\\(H_A\\): The mean movie rating is different for at least one era, given the genre of the movie\nWith a p-value of 0.568 (from an F-statistic of 0.680 with 3 and 51 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating differs for at least one era.\nThe genre line is testing if the mean movie rating is different for at least one era, conditional on the era of the movie. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every genre, given the era of the movie\n\\(H_A\\): The mean movie rating is different for at least one genre, given the era of the movie\nWith a p-value of 0.078 (from an F-statistic of 1.86 with 1 and 51 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating differs for at least one genre."
  },
  {
    "objectID": "project/final-project-grading-rubric.html",
    "href": "project/final-project-grading-rubric.html",
    "title": "Final Project Rubric",
    "section": "",
    "text": "Introduction\nMethods\nFindings\nScope of Inference\n\n\n\n\nExcellent\nProvides robust description of data, collection methods, context.\n\nWell articulated research question(s) correctly phrased for a one-way ANOVA.\nOutlines variables considered for analysis, including all relevant regarding their associated units / levels.\nCreates data visualizations requested, axis labels allow visualization to be interpreted independent of additional information.\nProvides robust description of relationships seen in visualization.\nEvaluates conditions of one-way ANOVA model with few if any errors that do not bring into question understanding of the underlying concepts.\nCorrectly fits one-way ANOVA models and obtains ANOVA table.\nProvides well written conclusion regarding the hypotheses tested in the both one-way ANOVAs, with few if any errors that do not bring into question understanding of the underlying concepts.\nCorrectly determines if an additive two-way ANOVA model should be fit.\n(If applicable) Provides well written conclusion regarding the hypotheses tested in the two-way ANOVA, with few if any errors that do not bring into question understanding of the underlying concepts.\nProvides well articulated description of how the findings from the statistical model(s) connect to what was seen in visualizations, reaches a conclusion for the research question stated in the Introduction.\nClearly articulates sampling methodology used in study, connects methodology to whom the analysis can be inferred onto.\nClearly articulates the design of the study, connects study design to what relationships can be inferred between the variables.\nInterpretations are all described in the context of the data, not in general terms.\n\n\nSatisfactory\nProvides satisfactory description of data, but description lacks some details regarding the collection methods, context.\nResearch question(s) articulated but contain instances of incorrect language for a one-way ANOVA.\nOutlines variables considered for analysis, omitting some details regarding their associated units / levels.\nCreates data visualizations requested, some additional information is needed to supplement axis labels.\nProvides satisfactor description of relationships seen in visualization,where some details have been omitted.\nEvaluates conditions of one-way ANOVA model with some errors that bring into question understanding of underlying concepts.\nCorrectly fits one-way ANOVA models and obtains ANOVA table.\nProvides well written conclusion regarding the hypotheses tested in the both one-way ANOVAs, with some errors that bring into question understanding of underlying concepts.\nCorrectly determines if an additive two-way ANOVA model should be fit.\n(If applicable) Provides well written conclusion regarding the hypotheses tested in the two-way ANOVA, with some errors that bring into question understanding of underlying concepts.\nDescription of connection between model findings and visualizations are mostly clear, but connections to the research question stated in the Introduction are at times unclear.\nDescribes sampling methodology used in study and states whom the analysis can be inferred onto, but connections between methods and inference are at times unclear.\nStatement regarding what relationships can be inferred between the variables is mostly clear, with connections to the design of the study.\nInterpretations are all described in the context of the study.\n\n\nProgressing\nProvides poor description of data, where description lacks multiple details regarding the collection methods, context.\nResearch question(s) poorly articulated, with little to no general connection to a one-way ANOVA.\nOutlines variables considered for analysis, with very few details regarding their associated units / levels.\nCreates data visualizations requested, axis labels require additional information to understand the context.\nProvides poor description of relationships seen in visualization.\nEvaluates conditions of one-way ANOVA model with many errors that bring into question understanding of underlying concepts.\nDoes not correctly fit one-way ANOVA models and obtains ANOVA table.\nProvides written conclusion regarding the hypotheses tested in the both one-way ANOVAs, with many errors that bring into question understanding of underlying concepts.\nDoes not correctly determine if an additive two-way ANOVA model should be fit.\n(If applicable) Provides written conclusion regarding the hypotheses tested in the both two-way ANOVA, with many errors that bring into question understanding of underlying concepts.\nDescription of connection between model findings and visualization is unclear with little to no connection to the research question stated in the Introduction.\nStatement regarding who the analysis can be inferred to is mostly unclear, with few if any connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with few if any connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study.\n\n\nNo Credit\nProvides little to no description of data.\nResearch question(s) not articulated.\nOutlines variables considered for analysis, with no details regarding their associated units / levels.\nCreates data visualizations requested, axis labels are not changed.\nProvides little to no description of relationships seen in visualization.\nEvaluation of model conditions bring into question any understanding of the underlying concepts.\nDoes not fit one-way ANOVA models and obtains ANOVA table.\nInterpretations of one-way ANOVA contain multiple errors that demonstrate little to no understanding of the underlying concepts.\nDoes not correctly determine if an additive two-way ANOVA model should be fit.\nLittle to no description of connection between models fit and visualization, no connection to the research question stated in the Introduction.\nStatement regarding who the analysis can be inferred to is mostly unclear, with no connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with no connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study."
  },
  {
    "objectID": "project/final-project-grading-rubric.html#overall-project-grade",
    "href": "project/final-project-grading-rubric.html#overall-project-grade",
    "title": "Final Project Rubric",
    "section": "Overall Project Grade",
    "text": "Overall Project Grade\n\n\n\n\n\n\n\n\n\nExcellent Project\nSatisfactory Project\nProgressing Project\nNo Credit\n\n\n\n\nAt most one section is marked “Satisfactory” the remainder are marked “Excellent”\n\n\nPresentation is completed\nAt most one section is marked “Progressing” the remainder are marked “Satisfactory” or “Excellent”\n\nPresentation is completed\nAt most two sections are marked “Progressing” the remainder are marked “Satisfactory”\n\n\nPresentation is not completed\nAt most one section is marked “Satisfactory” or “Excellent” the remainder are marked “Incomplete”\n\nPresentation is not completed"
  },
  {
    "objectID": "project/midterm_project_template.html#methods",
    "href": "project/midterm_project_template.html#methods",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Methods",
    "text": "Methods\n\nVariables\n\n\nData Visualizations\n\n\n\n\n\n\n\n\nProposed Statistical Model"
  },
  {
    "objectID": "project/midterm_project_template.html#findings",
    "href": "project/midterm_project_template.html#findings",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Findings",
    "text": "Findings\n\nMultiple Linear Regression Model\n\n\nModel Interpretations\n\n\nModel Conclusions"
  },
  {
    "objectID": "project/midterm_project_template.html#scope-of-inference",
    "href": "project/midterm_project_template.html#scope-of-inference",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Scope of Inference",
    "text": "Scope of Inference"
  },
  {
    "objectID": "project/midterm-project-grading-rubric.html",
    "href": "project/midterm-project-grading-rubric.html",
    "title": "Midterm Project Rubric",
    "section": "",
    "text": "Writing\nIntroduction\nMethods\nFindings\nScope of Inference\n\n\n\n\nExcellent\nReport is clear and can be read independent of other information on the project\nProvides robust description of data, collection methods, context.\n\nWell articulated research question(s) correctly phrased for multiple linear regression analysis.\nOutlines variables considered for analysis, including all relevant regarding their associated units / levels.\nCreates data visualizations requested, axis labels allow visualization to be interpreted independent of additional information.\nProvides robust description of relationships seen in visualization, clearly connects what is seen in the visualizations to the statistical model chosen for analysis.\nCorrectly fits and obtains coefficients for model stated in Methods section.\nProvides well written regression equation(s) which clearly indicate the context of the regression.\nInterprets the coefficients associated with the regression model (intercept(s) and slope(s)) with few if any errors that do not bring into question understanding of the underlying concepts.\nProvides well articulated description of how regression model connects to what was seen in visualization, reaches a conclusion for the research question stated in the Introduction.\nClearly states who the analysis can be inferred to, connecting to sampling methodology used.\nClearly states what relationships can be inferred between the variables, connecting to the design of the study.\nInterpretations are all described in the context of the study.\n\n\nSatisfactory\nReport is mostly clear, but additional information on the project is needed occasionally\nProvides satisfactory description of data with small elements omitted.\nResearch question(s) articulated but contain instances of incorrect language for a multiple linear regression analysis.\nOutlines variables considered for analysis, omitting some details regarding their associated units / levels.\nCreates data visualizations requested, some additional information is needed to supplement axis labels.\nProvides description of relationships seen in visualization, but connections to the statistical model chosen for analysis are at times unclear.\nCorrectly fits and obtains coefficients for model stated in Methods section.\nProvides regression equation(s) with some context, but meaning is sometimes unclear.\nInterprets the coefficients associated with the regression model (intercept(s) and slope(s)), with some errors that bring into question understanding of underlying concepts.\nDescription of connection between regression model and visualization are mostly clear, but connections to the research question stated in the Introduction are at times unclear.\nStatement regarding who the analysis can be inferred to is mostly clear, with connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly clear, with connections to the design of the study.\nInterpretations are all described in the context of the study.\n\n\nProgressing\nMostly clear responses to prompts in the project but not written as an independent report\nProvides poor description of data with multiple elements omitted.\nResearch question(s) poorly articulated, with little to no general connection to a multiple linear regression analysis.\nOutlines variables considered for analysis, with very few details regarding their associated units / levels.\nCreates data visualizations requested, axis labels require additional information to understand the context.\nProvides poor description of relationships seen in visualization,connections to the statistical model chosen for analysis are quite unclear.\nDoes not fit model stated in Methods section but obtains coefficients.\nProvides regression equation(s) with little to no context, meaning is often unclear.\nInterprets the coefficients associated with the regression model (intercept(s) and slope(s)), with many errors that bring into question understanding of underlying concepts.\nDescription of connection between regression model and visualization is unclear with little to no connection to the research question stated in the Introduction.\nStatement regarding who the analysis can be inferred to is mostly unclear, with few if any connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with few if any connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study.\n\n\nNo Credit\nNo, minimal, or unclear responses to prompts in project\nProvides little to no description of data.\nResearch question(s) not articulated.\nOutlines variables considered for analysis, with no details regarding their associated units / levels.\nCreates data visualizations requested, axis labels are not changed.\nProvides little to no description of relationships seen in visualization,with no connections to the statistical model chosen for analysis.\nDoes not fit model stated in Methods section and does not obtain coefficients.\nProvides regression equation(s) with no context, meaning is almost entirely unclear.\nInterpretations of coefficients associated with the regression model (intercept(s) and slope(s)) bring into question any understanding of underlying concepts.\nLittle to no description of connection between regression model and visualization,no connection to the research question stated in the Introduction.\nStatement regarding who the analysis can be inferred to is mostly unclear, with no connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with no connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study."
  },
  {
    "objectID": "project/midterm-project-grading-rubric.html#overall-project-grade",
    "href": "project/midterm-project-grading-rubric.html#overall-project-grade",
    "title": "Midterm Project Rubric",
    "section": "Overall Project Grade",
    "text": "Overall Project Grade\n\n\n\n\n\n\n\n\n\nExcellent Project\nSatisfactory Project\nProgressing Project\nNo Credit\n\n\n\n\nAt most one section is marked “Satisfactory” the remainder are marked “Excellent”\nAt most one section is marked “Progressing” the remainder are marked “Satisfactory” or “Excellent”\nAt most two sections are marked “Progressing” the remainder are marked “Satisfactory”\nAt most one section is marked “Satisfactory” or “Excellent” the remainder are marked “Incomplete”"
  },
  {
    "objectID": "project/midterm-project-directions.html",
    "href": "project/midterm-project-directions.html",
    "title": "Midterm Project Guidelines",
    "section": "",
    "text": "Your Task\nFor this project, you are expected to use a multiple linear regression to investigate the relationship between the variables you outlined in your Midterm Project Proposal. Your project is required to have at least two explanatory variables, one of which must be numerical. You are expected to justify why you chose each explanatory variable in the Introduction section of your report.\n\n\nIntroduction\n\nIn 4-6 sentences describe how the data were collected, the context of the data (e.g., Are they from a study or a publication?), and the background of the research problem (e.g., What question do these data address? Why were the data collected?)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may need to look up the publication(s) the data are associated with to obtain information on how they were collected.\n\n\n\nState the question(s) of interest you will address with your statistical analysis. The more specific you define the question of interest here, the easier the rest of the analysis and report will be. The research questions should start with, “What is the relationship between…” and should be as specific as possible. Your Findings section should directly address the question(s) you pose here.\n\n\n\nMethods\nThis section should lay out the steps, decisions, and logic leading to the statistical model you will use to answer the research question of interest.\n\nDescribe the response and explanatory variables, how they were measured and their associated units. For categorical variables, describe the levels of the categorical variable.\nProduce data visualizations exploring the relationship(s) you are interested in investigating, contrasting the need for a second explanatory variable.\n\nFor a multiple linear regression with one numerical variable and one categorical variable, you should produce two visualizations: one with different slopes and one with parallel slopes.\nFor a multiple linear regression with two numerical variables, you should produce three visualizations: one with both explanatory variables included, and two with each explanatory variable on its own.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nEvery visualization should have nicely formatted axis labels!\n\n\n\nDescribe what you see in the visualizations, making direct references to the plots!\nOutline the appropriate statistical model you will use to answer the question(s) of interest that you stated previously. Be specific about why the method being used are appropriate for the investigation at hand making direct reference to the visualizations.\n\n\n\n\n\n\n\nTip\n\n\n\nThe statistical model you fit in the next section depends on what you see in your visualization. Click here for a flowchart of how you should select the statistical model that is best for your situation.\n\n\n\n\nFindings\nIn this section you will write up your findings for each research question of interest.\n\nFit the statistical model stated at the end of the Methods section\nObtain the coefficients for the model\n\n\n\n\n\n\n\nTip\n\n\n\nUse the get_regression_table() function from the moderndive package to provide nicely formatted output from your regression model.\n\n\n\nWrite out the estimated regression equation for your statistical model.\n\nIf your regression contains categorical variables, you can either write your equation out with indicator variables or write out a different equation for each group.\n\nInterpret in the context of the data the coefficients from the regression equation.\nBased on your visualizations and the regression model, what is your conclusion for the questions of interest?\n\n\n\n\n\n\n\nCaution\n\n\n\nThere should be no mention of p-values in your conclusion!\n\n\n\n\nScope of Inference\nWrite a 3-4 sentence statement on what can be inferred from the design of the study and the results of your statistical analysis. Specifically, answer these two questions and comment on their implications:\n\nBased on the sampling method used, what larger population can you infer the results or your analysis onto?\nBased on the design of the study, what type of statements can be made about the relationship between the explanatory and response variables. Specifically, can cause-and-effect statements be made?",
    "crumbs": [
      "Projects",
      "Midterm Project Description"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#student-hours",
    "href": "course-support.html#student-hours",
    "title": "Course support",
    "section": "Student Hours",
    "text": "Student Hours\n\n\n\nDay\nTime\n\n\n\n\nTuesdays\n10:10 pm – 11:00 am (in-person)\n\n\nWednesdays\n2:10 pm - 3:00 pm (individual appointments)\n\n\nThursdays\n10:10 am – 11:00 am (in-person)\n\n\n\nA lot of questions are most effectively answered in-person, and student hours are the best place to come ask me questions! I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself.\n\nPersonal Meetings\nIf you would like to talk with me individually, I’ve reserved time on Wednesdays from 2:10pm to 3:00pm for individual appointments. You can make appointments through the following link: https://calendly.com/allisontheobold\nI do request that you make appointments at least 1-hour ahead of time, so I don’t miss our meeting!\nIf you need to meet, but none of the student hours work for you please let me know! It is possible we can communicate asynchronously through Discord or email, but I am happy to schedule a meeting during another time if necessary.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#discord",
    "href": "course-support.html#discord",
    "title": "Course support",
    "section": "Discord",
    "text": "Discord\nWe will use Discord to manage questions and responses regarding course content. There are channels for the different components of each week (e.g., Week 1 Lab Assignment). Please do not send an email about homework questions or questions about the course material. It is incredibly helpful for others in the course to see the questions you have and the responses to those questions. I will try to answer any questions posted to Discord within 3-4 hours (unless it is posted at midnight). If you think you can answer another student’s question, please respond!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should be posted to Discord), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Allison Theobold at atheobol@calpoly.edu.\nI do my best to reply to emails promptly and helpfully. However, I receive a lot of emails. To help both you and me, here are some specific expectations about emails:\n\nPlease tell me what course and section (by time or number) you are in!\nIf you email me between 9am and 4pm on Monday through Friday, I’ll try my best to reply to you on the same day.\nIf you email me outside of those hours, I will do my best to respond to you by the next working day. For my own mental health, I do not work on weekends. Thus, if you send an email after 4pm on Friday or during the weekend, you will not receive a response until Monday morning.\nIf your question is much easier to discuss face-to-face, I may ask you to meet with me in my office or on Discord (at a time that works for both of us) rather than answering directly in an email.\nInclude any relevant photos / screenshots / error messages / PDFs / links with your email.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#well-being-access-and-accommodations",
    "href": "course-support.html#well-being-access-and-accommodations",
    "title": "Course support",
    "section": "Well-being, Access, and Accommodations",
    "text": "Well-being, Access, and Accommodations\n\nWhat if I have accommodations or feel that accommodations would be beneficial to my learning?\nI enthusiastically support the mission of Disability Resource Center to make education accessible to all. I design all my courses with accessibility at the forefront of my thinking, but if you have any suggestions for ways I can make things more accessible, please let me know. Come talk to me if you need accommodation for your disabilities. I honor self-diagnosis: let’s talk to each other about how we can make the course as accessible as possible. See also the standard syllabus statements, which include more information about formal processes.\n\n\nI’m having difficulty paying for food and rent, what can I do?\nIf you have difficulty affording groceries or accessing sufficient food to eat every day, or if you lack a safe and stable place to live, and you believe this may affect your performance in the course, I urge you to contact the Dean of Students for support. Furthermore, please notify me if you are comfortable in doing so. This will enable me to advocate for you and to connect you with other campus resources.\n\n\nMy mental health is impairing my ability to engage in my classes, what should I do?\nNational surveys of college students have consistently found that stress, sleep problems, anxiety, depression, interpersonal concerns, death of a significant other and alcohol use are among the top ten health impediments to academic performance. If you are experiencing any mental health issues, I and Cal Poly are here to help you. Cal Poly’s Counseling Services (805-756-2511) is a free and confidential resource for assistance, support and advocacy.\n\n\nSomeone is threatening me, what can I do?\nI will listen and believe you if someone is threatening you. I will help you get the help you need. I commit to changing campus culture that responds poorly to dating violence and stalking.\n\n\nWhat if I can’t arrange for childcare?\nIf you are responsible for childcare on short notice, you are welcome to bring children to class with you. If you are a lactating parent, you many take breaks to feed your infant or express milk as needed. If I can support yo in navigating parenting, coursework, and other obligations in any way, please let me know.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "critique/critique-2.html#assignment-details",
    "href": "critique/critique-2.html#assignment-details",
    "title": "Statistical Critique 2: Statistical Argumentation",
    "section": "Assignment Details",
    "text": "Assignment Details\nIn your second statistical critique, you will focus on critiquing another key aspect of any statistical argument—statistical significance. No doubt you have seen \\(p\\)-values in a previous statistical course and / or disciplinary course, and this week you’re adding to that knowledge. For this critique you read about common misinterpretations and misuses of \\(p\\)-values, and critique the statistical argumentation used in the article you selected."
  },
  {
    "objectID": "critique/critique-2.html#footnotes",
    "href": "critique/critique-2.html#footnotes",
    "title": "Statistical Critique 2: Statistical Argumentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is my professional organization.↩︎"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-12",
    "href": "slides/week1-day1-alt.html#section-12",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "NBA player of the week\n\n\n\n\n\n\n\n\nAge\nDate\nDraft Year\nHeight\nPlayer\nPosition\n\n\n\n\n28\nJan 7, 2013\n2003\n6-8\nCarmelo Anthony\nF\n\n\n23\nJan 13, 2008\n2003\n6-11\nChris Bosh\nPF\n\n\n27\nJan 25, 2004\n1995\n6-11\nKevin Garnett\nPF\n\n\n34\nJan 13, 1991\n1977\n6-7\nBernard King\nSF\n\n\n21\nMar 1, 1998\n1997\n6-11\nTim Duncan\nFC\n\n\n31\nFeb 18, 1996\n1987\n6-9\nArmen Gilliam\nPF"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-22",
    "href": "slides/week1-day1-alt.html#section-22",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Data Types in R\n\n\n\nglimpse(births_small)\n\nRows: 1,000\nColumns: 10\n$ fage           &lt;int&gt; 34, 36, 37, NA, 32, 32, 37, 29, 30, 29, 30, 34, 28, 28,…\n$ mage           &lt;dbl&gt; 34, 31, 36, 16, 31, 26, 36, 24, 32, 26, 34, 27, 22, 31,…\n$ weeks          &lt;dbl&gt; 37, 41, 37, 38, 36, 39, 36, 40, 39, 39, 42, 40, 40, 39,…\n$ premie         &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"pr…\n$ gained         &lt;dbl&gt; 28, 41, 28, 29, 48, 45, 20, 65, 25, 22, 40, 30, 31, NA,…\n$ weight         &lt;dbl&gt; 6.96, 8.86, 7.51, 6.19, 6.75, 6.69, 6.13, 6.74, 8.94, 9…\n$ lowbirthweight &lt;chr&gt; \"not low\", \"not low\", \"not low\", \"not low\", \"not low\", …\n$ sex            &lt;fct&gt; male, female, female, male, female, female, female, mal…\n$ habit          &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"no…\n$ whitemom       &lt;chr&gt; \"white\", \"white\", \"not white\", \"white\", \"white\", \"white…\n\n\n\n\n\nWhat do you think dbl means?\nHow is that different from int?\n\n\n\nWhat does chr mean?\nHow might it differ from fct?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-23",
    "href": "slides/week1-day1-alt.html#section-23",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Types of Studies\n\n\n\nExperiment\n\nrandomization\nreplication\ncontrolling\nblocking\n\n\n\n\nObservational Study\n\ncollect data in a way that does not directly interfere with how the data arise"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-25",
    "href": "slides/week1-day1-alt.html#section-25",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Causal Inference\n\n\nassociation \\(\\neq\\) causation\n\n\nWhat do you need to say that the explanatory variable causes a change in the response variable?"
  },
  {
    "objectID": "weeks/week-2.html#concept-quiz-due-thursday-by-the-beginning-of-class",
    "href": "weeks/week-2.html#concept-quiz-due-thursday-by-the-beginning-of-class",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.4 Concept Quiz – Due Thursday by the Beginning of Class",
    "text": "1.4 Concept Quiz – Due Thursday by the Beginning of Class\n\n\n\n\n\n\nNote\n\n\n\nThe two concept quizzes from each chapter have been combined into one concept quiz on Canvas.\n\n\n\nWhat aesthetics are being used in the following plot?\n\nHint: Think about what goes inside of the aes() function and what does not.\n\n\n\n\n\n\n\n\n\n\nx axis\ny axis\ncolor\nfacets\npoints\nlines\n\n\nWhat geometric objects are being used in the displayed visualization?\n\nHint: Think about what geoms you would use to make this plot!\n\npoints\nlines / smoothers\ncolors\nfacets\n\n\nWhat aspects of the distribution of departure delays can you see in the histogram that you could not see in the boxplot?\n\n\n\n\n\n\n\n\n\n\n\n\nshape of distribution\nmedian\noutliers\nmode"
  },
  {
    "objectID": "slides/week2-day1.html#moving-into-lab-2",
    "href": "slides/week2-day1.html#moving-into-lab-2",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Moving into Lab 2",
    "text": "Moving into Lab 2"
  },
  {
    "objectID": "slides/week2-day1.html#find-your-team",
    "href": "slides/week2-day1.html#find-your-team",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Find Your Team",
    "text": "Find Your Team"
  },
  {
    "objectID": "weeks/week-3.html#reading-guide-due-tuesday-by-midnight",
    "href": "weeks/week-3.html#reading-guide-due-tuesday-by-midnight",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.1 Reading Guide – Due Tuesday by Midnight",
    "text": "1.1 Reading Guide – Due Tuesday by Midnight\nDownload the Word Document\n\n\n\n\n\n\nAnswers\n\n\n\nPlease include your answers as a different color! You can pick whatever color you like, but please use a color other than black."
  },
  {
    "objectID": "weeks/week-3.html#concept-quiz-due-tuesday-by-midnight",
    "href": "weeks/week-3.html#concept-quiz-due-tuesday-by-midnight",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.4 Concept Quiz – Due Tuesday by Midnight",
    "text": "1.4 Concept Quiz – Due Tuesday by Midnight\n1. Suppose you’ve run the following code to make side-by-side boxplots of life expectancy:\n\nggplot(data = gapminder_small, \n       mapping = aes(x = country, y = lifeExp)) +\n  geom_boxplot() +\n  labs(x = \"\", \n       y = \"\", \n       title = \"Life Expectancy (in Years) by Country\",\n       fill = \"Country\"\n       )\n\nYour resulting plot looks like this:\n\n\n\n\n\nWhat should you do?\n\nMove country to the y-axis.\nRemove some countries so the names don’t overlap.\nNothing, it looks great!\n\n2. If you didn’t want to stack your boxplots side-by-side, how else could you separate the groups?\n\n\n\n\n\n\nTip\n\n\n\nThis does not go inside the aes() function!\n\n\n3. What functions are necessary to calculate summary statistics for different groups of a categorical variable?\n\nsummarize()\nmutate()\narrange()\ngroup_by()\nfilter()\n\n4. A categorical variable could be correctly stored as what data types?\n\ninteger\ndouble\ncharacter\nfactor\n\n5. In Lab 2, you worked with the nycflights dataset, which contained information on flights departing from NYC airports. If we wanted to know the average departure delay for every airline in the dataset (e.g., Delta, Allegiant, United), what steps would we need to use? Match the code to its corresponding step.\n\n\nFirst Step\nSecond Step\nThird Step\n\n\n\nnycflights\ngroup_by(airline)\nfilter(dest == \"SFO\")\nsummarize(mean_dd = mean(dep_delay))\n\n\n6. What function did you learn that is used for converting a numerical variable to a categorical variable (a process called discretizing)?\n\nmutate()\nfilter()\nif_else()\ngroup_by()\n\n7. The arrange() function sorts or reorder a data frame’s rows according to the values of the specified variable. By default, what ordering does it use for the rows?\n\nascending (smallest to largest)\ndescending (largest to smallest)"
  },
  {
    "objectID": "weeks/week-3.html#r-tutorial-due-thursday-by-midnight",
    "href": "weeks/week-3.html#r-tutorial-due-thursday-by-midnight",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.5 R Tutorial – Due Thursday by Midnight",
    "text": "1.5 R Tutorial – Due Thursday by Midnight\nRequired Tutorial: Derive Information with dplyr\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit a screenshot of the completion page for each tutorial to the Canvas assignment portal!"
  },
  {
    "objectID": "slides/week2-day1.html#todays-layout",
    "href": "slides/week2-day1.html#todays-layout",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Today’s Layout",
    "text": "Today’s Layout\n\nReview visualizations for numerical variables\nPractice producing these visualizations\nDiscuss the pros / cons of each visualization\n\n\n\nOpen today’s interactive document, so you can be ready to code along with me!\n\nhttps://bit.ly/week-2-code-along"
  },
  {
    "objectID": "slides/week2-day1.html#recreate-my-plot",
    "href": "slides/week2-day1.html#recreate-my-plot",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Recreate My Plot",
    "text": "Recreate My Plot\nOpen today’s interactive document to code along with me!\nhttps://bit.ly/week-2-code-along"
  },
  {
    "objectID": "slides/week2-day1.html#section-2",
    "href": "slides/week2-day1.html#section-2",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Recreate my plot!\n\n\nhttps://bit.ly/week-2-code-along"
  },
  {
    "objectID": "slides/week2-day1.html#section-8",
    "href": "slides/week2-day1.html#section-8",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Bivariate (Two Variables) Visualizations – For Numerical Data\n\n\n\n\nScatterplots\nFaceted Histograms (Week 3)\nSide-by-Side Boxplots (Week 3)\nStacked Density Plots (Week 3)"
  },
  {
    "objectID": "slides/week2-day1.html#section-9",
    "href": "slides/week2-day1.html#section-9",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Recreate my plot!\n\n\nhttps://bit.ly/week-2-code-along"
  },
  {
    "objectID": "slides/week2-day1.html#section-10",
    "href": "slides/week2-day1.html#section-10",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are strengths of a scatterplot?\n\n\nPlots the raw data!\nInspect form and strength of a relationship\nIdentify unusual values\n\n\n\nWhat are weaknesses of a scatterplot?\n\n\nCan get “busy” when there are lots of observations (points)"
  },
  {
    "objectID": "slides/week2-day1.html#section-11",
    "href": "slides/week2-day1.html#section-11",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Multivariate (3+ Variables) Plots\n\n\nThere are two main methods for adding a third (or fourth) variable into a data visualization:\n\n\n\nColors\n\ncreates colors for every level of a categorical variable\ncreates a gradient for different values of a quantitative variable\n\n\n\n\nFacets\n\ncreates subplots for every level of a categorial variable\nlabels each sub-plot with the value of the variable"
  },
  {
    "objectID": "slides/week2-day1.html#section-12",
    "href": "slides/week2-day1.html#section-12",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Recreate my plot!"
  },
  {
    "objectID": "slides/week2-day1.html#section-13",
    "href": "slides/week2-day1.html#section-13",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Measures of Spread\n\n\n\n\n\nNot Resistant\n\nVariance\nRange\n\n\n\n\nResistant\n\nInner Quartile Range (IQR)\n\n\n \n\n\n\n\n\n\n60-second question\n\n\nWhy are the variance and range not resistant?"
  },
  {
    "objectID": "slides/week2-day1.html#section-14",
    "href": "slides/week2-day1.html#section-14",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Some Rules to Play By\n\n\n\n\n\nLook at and understand your raw data before aggregating\n\n\n\n\n\n\nBoxplots (and such) don’t count as visualizing the raw data\n\n\n\n\n\n\nWe should only average things we are convinced are measuring the same thing"
  },
  {
    "objectID": "slides/week2-day1.html#lab-2",
    "href": "slides/week2-day1.html#lab-2",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Lab 2",
    "text": "Lab 2"
  },
  {
    "objectID": "slides/week2-day1.html#my-plan",
    "href": "slides/week2-day1.html#my-plan",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "My Plan",
    "text": "My Plan\n\n\n\nWhat you can expect from me:\n\nPost coursework by Friday\nPost slides from last quarter\nExtend Lab 2 revision deadline by 1-week\n\n\n\n\n\n\nWhat I expect from you:\n\nComplete coursework\nReview / work through slides as if you were in class\n\n\n\n\n\n\nWhat I expect from your group:\n\nComplete the lab assignment as a group"
  },
  {
    "objectID": "slides/week2-day1.html#why-not-use-facets-with-numerical-variables",
    "href": "slides/week2-day1.html#why-not-use-facets-with-numerical-variables",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Why not use facets with numerical variables???",
    "text": "Why not use facets with numerical variables???"
  },
  {
    "objectID": "activity/week-2-code-along.html#adding-colors-to-scatterplots",
    "href": "activity/week-2-code-along.html#adding-colors-to-scatterplots",
    "title": "Week 2 Lecture - Practicing Your Visualizations",
    "section": "Adding Colors to Scatterplots",
    "text": "Adding Colors to Scatterplots\nWhat happens when you use a categorical variable to color the points instead?"
  },
  {
    "objectID": "slides/week2-day1.html",
    "href": "slides/week2-day1.html",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "15-minutes\n\n\n\nReview Lab 1 comments\nAsk questions\nStart revisions"
  },
  {
    "objectID": "slides/week2-day1.html#why-not-use-facets-with-a-numerical-variable",
    "href": "slides/week2-day1.html#why-not-use-facets-with-a-numerical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Why not use facets with a numerical variable???",
    "text": "Why not use facets with a numerical variable???"
  },
  {
    "objectID": "slides/week2-day1.html#departure-delays",
    "href": "slides/week2-day1.html#departure-delays",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Departure Delays",
    "text": "Departure Delays\n\nInspect the nycflights dataset\nVisualize departure delays\nPlay with binwidth\nFilter data to include only certain flights\nCalculate summary statistics\nCompare summary statistics with a visualization"
  }
]